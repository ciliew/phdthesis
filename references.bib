@misc{2020,
  title = {The {{State}} of {{AI Ethics Report}} ({{Oct}} 2020)},
  year = {2020},
  month = oct,
  journal = {Montreal AI Ethics Institute},
  abstract = {Full report in PDF formDownload The State of AI Ethics Report (October 2020) captures the most relevant developments in AI Ethics since July of 2020. (see our June report here) Our goal is to save you\ldots},
  howpublished = {https://montrealethics.ai/oct2020/},
  langid = {american},
  keywords = {grey literature,reference},
  file = {/Users/cife/Zotero/storage/FMWLRI7Y/oct2020.html}
}

@misc{2020a,
  title = {Mini Batching with {{Bayesian GPLVM}} - {{Tutorials}}},
  year = {2020},
  month = nov,
  journal = {Pyro Discussion Forum},
  abstract = {Trying to define mini-batch logic for Bayesian GPLVM training but unsuccessful so far following the suggestions in this older thread: Pyro Bayesian GPLVM SVI with minibatching  So the suggestion in this thread is to use:  X\_minibatch = pyro.sample(\ldots, dist.Normal(x\_loc[minibatch\_indices], x\_scale[minibatch\_indices]))  y\_minibatch = y[minibatch\_indices]  self.base\_model.set\_data(X\_minibatch, y\_minibatch)  (\ldots in each epoch before calling svi.step())  However, this causes the learnt X representation ...},
  howpublished = {https://forum.pyro.ai/t/mini-batching-with-bayesian-gplvm/2375},
  langid = {english},
  file = {/Users/cife/Zotero/storage/HKAC7RV5/12.html}
}

@misc{2022,
  title = {Geometric {{Kernels}}},
  year = {2022},
  month = apr,
  abstract = {Geometric kernels on manifolds, meshes and graphs},
  copyright = {Apache-2.0},
  howpublished = {GPflow}
}

@misc{262588213843476,
  title = {Dtu-Run.Sh},
  author = {262588213843476},
  journal = {Gist},
  abstract = {GitHub Gist: instantly share code, notes, and snippets.},
  howpublished = {https://gist.github.com/rasmusbergpalm/58e506615c8fed930763d663c274e721},
  langid = {english},
  file = {/Users/cife/Zotero/storage/ETCYBSUY/58e506615c8fed930763d663c274e721.html}
}

@inproceedings{adam2020,
  title = {Doubly {{Sparse Variational Gaussian Processes}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande, Nicolas and Hensman, James},
  year = {2020},
  month = jun,
  pages = {2874--2884},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint.The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting some sparsity in the precision matrix.In this work, we propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameterisation.Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Adam_2020_Doubly_Sparse_Variational_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/WUD6QJIN/Adam et al. - 2020 - Doubly Sparse Variational Gaussian Processes.pdf}
}

@article{adam2021,
  title = {Dual {{Parameterization}} of {{Sparse Variational Gaussian Processes}}},
  author = {Adam, Vincent and Chang, Paul E. and Khan, Mohammad Emtiyaz and Solin, Arno},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.03412 [cs, stat]},
  eprint = {2111.03412},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Sparse variational Gaussian process (SVGP) methods are a common choice for non-conjugate Gaussian process inference because of their computational benefits. In this paper, we improve their computational efficiency by using a dual parameterization where each data example is assigned dual parameters, similarly to site parameters used in expectation propagation. Our dual parameterization speeds-up inference using natural gradient descent, and provides a tighter evidence lower bound for hyperparameter learning. The approach has the same memory cost as the current SVGP methods, but it is faster and more accurate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Adam_2021_Dual_Parameterization_of_Sparse_Variational_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/NBNEWT92/2111.html}
}

@article{adams2021,
  title = {Gaussian {{Process Manifold Learning}}},
  author = {Adams, Larin Cole},
  year = {2021},
  pages = {145},
  langid = {english},
  file = {/Users/cife/Zotero/storage/EDN86JUD/Adams - Gaussian Process Manifold Learning.pdf}
}

@article{ahmed2019,
  title = {{{GrandPrix}}: Scaling up the {{Bayesian GPLVM}} for Single-Cell Data},
  shorttitle = {{{GrandPrix}}},
  author = {Ahmed, Sumon and Rattray, M. and Boukouvalas, A.},
  year = {2019},
  journal = {Bioinform.},
  doi = {10.1093/bioinformatics/bty533},
  abstract = {The Gaussian Process Latent Variable Model has the capability of producing meaningful biological insights about cell ordering as well as cell fate regulation and is extended to higher-dimensional latent spaces that can be used to simultaneously infer pseudotime and other structure such as branching. Motivation: The Gaussian Process Latent Variable Model (GPLVM) is a popular approach for dimensionality reduction of single-cell data and has been used for pseudotime estimation with capture time information. However, current implementations are computationally intensive and will not scale up to modern droplet-based single-cell datasets which routinely profile many tens of thousands of cells. Results: We provide an efficient implementation which allows scaling up this approach to modern single-cell datasets. We also generalize the application of pseudotime inference to cases where there are other sources of variation such as branching dynamics. We apply our method on microarray, nCounter, RNA-seq, qPCR and droplet-based datasets from different organisms. The model converges an order of magnitude faster compared to existing methods whilst achieving similar levels of estimation accuracy. Further, we demonstrate the flexibility of our approach by extending the model to higher-dimensional latent spaces that can be used to simultaneously infer pseudotime and other structure such as branching. Thus, the model has the capability of producing meaningful biological insights about cell ordering as well as cell fate regulation. Availability and implementation: Software available at github.com/ManchesterBioinference/GrandPrix. Supplementary information: Supplementary data are available at Bioinformatics online.},
  file = {/Users/cife/Dropbox/Zotero/Ahmed_2019_GrandPrix.pdf}
}

@article{akhlevniuk2017,
  title = {Introduction to Pullback Metric},
  author = {A Khlevniuk and V Tymchyshyn},
  year = {2017},
  doi = {10.13140/RG.2.2.18073.31841},
  abstract = {The purpose of this paper is to present the topic of pullback (induced) metric in an accessible way and to elaborate on a few neat examples of it.},
  langid = {english},
  keywords = {geometry,read-soon},
  file = {/Users/cife/Dropbox/Zotero/A_Khlevniuk_2017_Introduction_to_pullback_metric.pdf}
}

@article{alet2021,
  title = {Noether {{Networks}}: {{Meta-Learning Useful Conserved Quantities}}},
  shorttitle = {Noether {{Networks}}},
  author = {Alet, Ferran and Doblar, Dylan and Zhou, Allan and Tenenbaum, Joshua and Kawaguchi, Kenji and Finn, Chelsea},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03321 [cs]},
  eprint = {2112.03321},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Progress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Alet_2021_Noether_Networks.pdf;/Users/cife/Zotero/storage/KSKYAX68/2112.html}
}

@misc{altosaar2020,
  title = {Tutorial - {{What}} Is a Variational Autoencoder?},
  author = {Altosaar, Jaan},
  year = {2020},
  journal = {Jaan Altosaar},
  abstract = {Understanding Variational Autoencoders (VAEs) from two perspectives: deep learning and graphical models.},
  howpublished = {https://jaan.io/what-is-variational-autoencoder-vae-tutorial/},
  keywords = {read-someday,tutorial},
  file = {/Users/cife/Zotero/storage/UNJXVJN5/what-is-variational-autoencoder-vae-tutorial.html}
}

@misc{alvarez-melis2020,
  title = {Measuring Dataset Similarity Using Optimal Transport},
  author = {{Alvarez-Melis}, David and Fusi, Nicolo},
  year = {2020},
  month = sep,
  journal = {Microsoft Research},
  abstract = {Is FashionMNIST, a dataset of images of clothing items labeled by category, more similar to MNIST or to USPS, both of which are classification datasets of handwritten digits? This is a pretty hard question to answer, but the solution could have an impact on various aspects of machine learning. For example, it could change how [\ldots ]},
  langid = {american},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/3KTA6S7M/measuring-dataset-similarity-using-optimal-transport.html}
}

@article{alvarez-melis2020a,
  title = {Geometric {{Dataset Distances}} via {{Optimal Transport}}},
  author = {{Alvarez-Melis}, David and Fusi, Nicolo},
  year = {2020},
  month = sep,
  abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e. g., require training a model on each [\ldots ]},
  langid = {american},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/7YZ6IPZ2/geometric-dataset-distances-via-optimal-transport.html}
}

@article{alvarez2012,
  title = {Kernels for {{Vector-Valued Functions}}: {{A Review}}},
  shorttitle = {Kernels for {{Vector-Valued Functions}}},
  author = {{\'A}lvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = jun,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {4},
  number = {3},
  pages = {195--266},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000036},
  abstract = {Kernels for Vector-Valued Functions: A Review},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Álvarez_2012_Kernels_for_Vector-Valued_Functions.pdf;/Users/cife/Zotero/storage/GM3HZSQB/MAL-036.html}
}

@article{alvarez2016,
  title = {Bayesian Inference for a Covariance Matrix},
  author = {Alvarez, Ignacio and Niemi, Jarad and Simpson, Matt},
  year = {2016},
  month = jul,
  journal = {arXiv:1408.4050 [stat]},
  eprint = {1408.4050},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix. Here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix. Inverse Wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in Bayesian statistical software. However inverse Wishart distribution presents some undesirable properties from a modeling point of view. It can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations. Some alternatives distributions has been proposed. The scaled inverse Wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. Secondly, it is possible to fit separate priors for individual correlations and standard deviations. This strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Alvarez_2016_Bayesian_inference_for_a_covariance_matrix.pdf;/Users/cife/Zotero/storage/JUBJHCEZ/1408.html}
}

@article{amid2018,
  title = {A More Globally Accurate Dimensionality Reduction Method Using Triplets},
  author = {Amid, Ehsan and Warmuth, Manfred K.},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.00854 [cs]},
  eprint = {1803.00854},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We first show that the commonly used dimensionality reduction (DR) methods such as t-SNE and LargeVis poorly capture the global structure of the data in the low dimensional embedding. We show this via a number of tests for the DR methods that can be easily applied by any practitioner to the dataset at hand. Surprisingly enough, t-SNE performs the best w.r.t. the commonly used measures that reward the local neighborhood accuracy such as precision-recall while having the worst performance in our tests for global structure. We then contrast the performance of these two DR method against our new method called TriMap. The main idea behind TriMap is to capture higher orders of structure with triplet information (instead of pairwise information used by t-SNE and LargeVis), and to minimize a robust loss function for satisfying the chosen triplets. We provide compelling experimental evidence on large natural datasets for the clear advantage of the TriMap DR results. As LargeVis, TriMap scales linearly with the number of data points.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,sb,tsne},
  file = {/Users/cife/Dropbox/Zotero/Amid_2018_A_more_globally_accurate_dimensionality_reduction_method_using_triplets.pdf;/Users/cife/Zotero/storage/IDZIAMKZ/1803.html}
}

@article{amid2019,
  title = {{{TriMap}}: {{Large-scale Dimensionality Reduction Using Triplets}}},
  shorttitle = {{{TriMap}}},
  author = {Amid, Ehsan and Warmuth, Manfred K.},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.00204 [cs, stat]},
  eprint = {1910.00204},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce ``TriMap''; a dimensionality reduction technique based on triplet constraints that preserves the global accuracy of the data better than the other commonly used methods such as t-SNE, LargeVis, and UMAP. To quantify the global accuracy, we introduce a score which roughly reflects the relative placement of the clusters rather than the individual points. We empirically show the excellent performance of TriMap on a large variety of datasets in terms of the quality of the embedding as well as the runtime. On our performance benchmarks, TriMap easily scales to millions of points without depleting the memory and clearly outperforms t-SNE, LargeVis, and UMAP in terms of runtime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,sb,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Amid_2019_TriMap.pdf;/Users/cife/Zotero/storage/H9BBCMNJ/1910.html}
}

@article{anderson1946,
  title = {The {{Non-Central Wishart Distribution}} and {{Certain Problems}} of {{Multivariate Statistics}}},
  author = {Anderson, T. W.},
  year = {1946},
  doi = {10.1214/AOMS/1177730882},
  abstract = {Semantic Scholar extracted view of "The Non-Central Wishart Distribution and Certain Problems of Multivariate Statistics" by T. W. Anderson},
  file = {/Users/cife/Dropbox/Zotero/Anderson_1946_The_Non-Central_Wishart_Distribution_and_Certain_Problems_of_Multivariate.pdf}
}

@article{anderson1946a,
  title = {The {{Non-Central Wishart Distribution}} and {{Certain Problems}} of {{Multivariate Statistics}}},
  author = {Anderson, T. W.},
  year = {1946},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {17},
  number = {4},
  pages = {409--431},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177730882},
  abstract = {The non-central Wishart distribution is the joint distribution of the sums of squares and cross-products of the deviations from the sample means when the observations arise from a set of normal multivariate populations with constant covariance matrix but expected values that vary from observation to observation. The characteristic function for this distribution is obtained from the distribution of the observations (Theorem 1). By using the characteristic functions it is shown that the convolution of several non-central Wishart distributions is another non-central Wishart distribution (Theorem 2). A simple integral representation of the distribution in the general case is given (Theorem 3). The integrand is a function of the roots of a determinantal equation involving the matrix of sums of squares and cross-products of deviations of observations and the matrix of sums of squares and cross-products of deviations of corresponding expected values. The knowledge of the non-central Wishart distribution is applied to two general problems of multivariate normal statistics. The moments of the generalized variance, which is the determinant of sums of squares and cross-products multiplied by a constant, are given for the cases of the expected values of the variates lying on a line (Theorem 4) and lying on a plane (Theorem 5). The likelihood ratio criterion for testing linear hypotheses can be expressed as the ratio of two determinants or as a symmetric function of the roots of a determinantal equation. In either case there is involved a matrix having a Wishart distribution and another matrix independently distributed such that the sum of these two matrices has a non-central Wishart distribution. When the null hypothesis is not true the moments of this criterion are given in the non-central planar case (Theorem 6).},
  file = {/Users/cife/Dropbox/Zotero/Anderson_1946_The_Non-Central_Wishart_Distribution_and_Certain_Problems_of_Multivariate3.pdf;/Users/cife/Zotero/storage/V446ZSPI/1177730882.html}
}

@misc{anonymous2019,
  type = {Text},
  title = {Ethics Guidelines for Trustworthy {{AI}}},
  author = {Anonymous},
  year = {2019},
  month = apr,
  journal = {Shaping Europe's digital future - European Commission},
  abstract = {On 8 April 2019, the High-Level Expert Group on AI presented Ethics Guidelines for Trustworthy Artificial Intelligence. This followed the publication of the guidelines' first draft in December 2018 on which more than 500 comments were received through an open consultation.},
  howpublished = {https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai},
  langid = {english},
  keywords = {grey literature,reference},
  file = {/Users/cife/Zotero/storage/URJZ4H55/ethics-guidelines-trustworthy-ai.html}
}

@article{anscombe1973,
  title = {Graphs in Statistical Analysis},
  author = {Anscombe, Francis J},
  year = {1973},
  journal = {The American Statistician},
  volume = {27},
  number = {1},
  pages = {17--21},
  publisher = {{Taylor \& Francis}}
}

@article{arnold2015,
  title = {A {{Definition}} of {{Systems Thinking}}: {{A Systems Approach}}},
  shorttitle = {A {{Definition}} of {{Systems Thinking}}},
  author = {Arnold, Ross D. and Wade, Jon P.},
  year = {2015},
  month = jan,
  journal = {Procedia Computer Science},
  series = {2015 {{Conference}} on {{Systems Engineering Research}}},
  volume = {44},
  pages = {669--678},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.03.050},
  abstract = {This paper proposes a definition of systems thinking for use in a wide variety of disciplines, with particular emphasis on the development and assessment of systems thinking educational efforts. The definition was derived from a review of the systems thinking literature combined with the application of systems thinking to itself. Many different definitions of systems thinking can be found throughout the systems community, but key components of a singular definition can be distilled from the literature. This researcher considered these components both individually and holistically, then proposed a new definition of systems thinking that integrates these components as a system. The definition was tested for fidelity against a System Test and against three widely accepted system archetypes. Systems thinking is widely believed to be critical in handling the complexity facing the world in the coming decades; however, it still resides in the educational margins. In order for this important skill to receive mainstream educational attention, a complete definition is required. Such a definition has not yet been established. This research is an attempt to rectify this deficiency by providing such a definition.},
  langid = {english},
  keywords = {grey literature,reference,system dynamics,systems approach,systems thinking,systems thinking definition,what is systems thinking},
  file = {/Users/cife/Dropbox/Zotero/Arnold_2015_A_Definition_of_Systems_Thinking.pdf;/Users/cife/Zotero/storage/4PRYBUZC/S1877050915002860.html;/Users/cife/Zotero/storage/QXQM7GAU/S1877050915002860.html}
}

@article{arora2018,
  ids = {arora2018a},
  title = {An {{Analysis}} of the T-{{SNE Algorithm}} for {{Data Visualization}}},
  author = {Arora, Sanjeev and Hu, Wei and Kothari, Pravesh K.},
  year = {2018},
  month = jun,
  journal = {arXiv:1803.01768 [cs]},
  eprint = {1803.01768},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A first line of attack in exploratory data analysis is data visualization, i.e., generating a 2-dimensional representation of data that makes clusters of similar points visually identifiable. Standard Johnson-Lindenstrauss dimensionality reduction does not produce data visualizations. The t-SNE heuristic of van der Maaten and Hinton, which is based on non-convex optimization, has become the de facto standard for visualization in a wide range of applications. This work gives a formal framework for the problem of data visualization - finding a 2-dimensional embedding of clusterable data that correctly separates individual clusters to make them visually identifiable. We then give a rigorous analysis of the performance of t-SNE under a natural, deterministic condition on the "ground-truth" clusters (similar to conditions assumed in earlier analyses of clustering) in the underlying data. These are the first provable guarantees on t-SNE for constructing good data visualizations. We show that our deterministic condition is satisfied by considerably general probabilistic generative models for clusterable data such as mixtures of well-separated log-concave distributions. Finally, we give theoretical evidence that t-SNE provably succeeds in partially recovering cluster structure even when the above deterministic condition is not met.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,sb,tsne},
  file = {/Users/cife/Dropbox/phdnotes/reading/arora2018-zotero.md;/Users/cife/Dropbox/Zotero/Arora_2018_An_Analysis_of_the_t-SNE_Algorithm_for_Data_Visualization.pdf;/Users/cife/Zotero/storage/SCCKEZ3Z/1803.html}
}

@inproceedings{artemev2021,
  title = {Tighter {{Bounds}} on the {{Log Marginal Likelihood}} of {{Gaussian Process Regression Using Conjugate Gradients}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Artemev, Artem and Burt, David R. and van der Wilk, Mark},
  year = {2021},
  month = jul,
  pages = {362--372},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose a lower bound on the log marginal likelihood of Gaussian process regression models that can be computed without matrix factorisation of the full kernel matrix. We show that approximate maximum likelihood learning of model parameters by maximising our lower bound retains many benefits of the sparse variational approach while reducing the bias introduced into hyperparameter learning. The basis of our bound is a more careful analysis of the log-determinant term appearing in the log marginal likelihood, as well as using the method of conjugate gradients to derive tight lower bounds on the term involving a quadratic form. Our approach is a step forward in unifying methods relying on lower bound maximisation (e.g. variational methods) and iterative approaches based on conjugate gradients for training Gaussian processes. In experiments, we show improved predictive performance with our model for a comparable amount of training time compared to other conjugate gradient based approaches.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Artemev_2021_Tighter_Bounds_on_the_Log_Marginal_Likelihood_of_Gaussian_Process_Regression.pdf;/Users/cife/Zotero/storage/DADU5C5L/Artemev et al. - 2021 - Tighter Bounds on the Log Marginal Likelihood of G.pdf}
}

@article{arvanitidis2016,
  title = {A {{Locally Adaptive Normal Distribution}}},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  year = {2016},
  pages = {9},
  abstract = {The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the ``manifold'' setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in RD. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep.},
  langid = {english},
  keywords = {geometry,read},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2016_A_Locally_Adaptive_Normal_Distribution.pdf;/Users/cife/Dropbox/Zotero/Arvanitidis_2016_A_Locally_Adaptive_Normal_Distribution2.pdf}
}

@inproceedings{arvanitidis2017,
  title = {Maximum {{Likelihood Estimation}} of {{Riemannian Metrics}} from {{Euclidean Data}}},
  booktitle = {Geometric {{Science}} of {{Information}}},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  editor = {Nielsen, Frank and Barbaresco, Fr{\'e}d{\'e}ric},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {38--46},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-68445-1_5},
  abstract = {Euclidean data often exhibit a nonlinear behavior, which may be modeled by assuming the data is distributed near a nonlinear submanifold in the data space. One approach to find such a manifold is to estimate a Riemannian metric that locally models the given data. Data distributions with respect to this metric will then tend to follow the nonlinear structure of the data. In practice, the learned metric rely on parameters that are hand-tuned for a given task. We propose to estimate such parameters by maximizing the data likelihood under the assumed distribution. This is complicated by two issues: (1) a change of parameters imply a change of measure such that different likelihoods are incomparable; (2) some choice of parameters renders the numerical calculation of distances and geodesics unstable such that likelihoods cannot be evaluated. As a practical solution, we propose to (1) re-normalize likelihoods with respect to the usual Lebesgue measure of the data space, and (2) to bound the likelihood when its exact value is unattainable. We provide practical algorithms for these ideas and illustrate their use on synthetic data, images of digits and faces, as well as signals extracted from EEG scalp measurements.},
  isbn = {978-3-319-68445-1},
  langid = {english},
  keywords = {Manifold learning,Metric learning,read-someday,Statistics on manifolds},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2017_Maximum_Likelihood_Estimation_of_Riemannian_Metrics_from_Euclidean_Data.pdf}
}

@article{arvanitidis2018,
  title = {Latent {{Space Oddity}}: On the {{Curvature}} of {{Deep Generative Models}}},
  shorttitle = {Latent {{Space Oddity}}},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  year = {2018},
  month = jan,
  journal = {arXiv:1710.11379 [stat]},
  eprint = {1710.11379},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear "generator" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.},
  archiveprefix = {arXiv},
  keywords = {geometry,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2018_Latent_Space_Oddity.pdf;/Users/cife/Zotero/storage/KZBHZ6VW/1710.html}
}

@phdthesis{arvanitidis2019,
  title = {Geometrical {{Aspects}} of {{Manifold Learning}}},
  author = {Arvanitidis, Georgios},
  year = {2019},
  langid = {english},
  school = {Technical University of Denmark},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2019_Geometrical_Aspects_of_Manifold_Learning.pdf}
}

@article{arvanitidis2019a,
  title = {Fast and {{Robust Shortest Paths}} on {{Manifolds Learned}} from {{Data}}},
  author = {Arvanitidis, Georgios and Hauberg, S{\o}ren and Hennig, Philipp and Schober, Michael},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.07229 [cs, stat]},
  eprint = {1901.07229},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a fast, simple and robust algorithm for computing shortest paths and distances on Riemannian manifolds learned from data. This amounts to solving a system of ordinary differential equations (ODEs) subject to boundary conditions. Here standard solvers perform poorly because they require well-behaved Jacobians of the ODE, and usually, manifolds learned from data imply unstable and ill-conditioned Jacobians. Instead, we propose a fixed-point iteration scheme for solving the ODE that avoids Jacobians. This enhances the stability of the solver, while reduces the computational cost. In experiments involving both Riemannian metric learning and deep generative models we demonstrate significant improvements in speed and stability over both general-purpose state-of-the-art solvers as well as over specialized solvers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2019_Fast_and_Robust_Shortest_Paths_on_Manifolds_Learned_from_Data.pdf;/Users/cife/Zotero/storage/9S9FX2LX/1901.html}
}

@article{arvanitidis2020,
  title = {Geometrically {{Enriched Latent Spaces}}},
  author = {Arvanitidis, Georgios and Hauberg, S{\o}ren and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.00565 [cs, stat]},
  eprint = {2008.00565},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A common assumption in generative models is that the generator immerses the latent space into a Euclidean ambient space. Instead, we consider the ambient space to be a Riemannian manifold, which allows for encoding domain knowledge through the associated Riemannian metric. Shortest paths can then be defined accordingly in the latent space to both follow the learned manifold and respect the ambient geometry. Through careful design of the ambient metric we can ensure that shortest paths are well-behaved even for deterministic generators that otherwise would exhibit a misleading bias. Experimentally we show that our approach improves interpretability of learned representations both using stochastic and deterministic generators.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Zotero/storage/ZBCXQ6UH/Arvanitidis et al. - 2020 - Geometrically Enriched Latent Spaces.pdf}
}

@article{arvanitidis2021,
  title = {A Prior-Based Approximate Latent {{Riemannian}} Metric},
  author = {Arvanitidis, Georgios and Georgiev, Bogdan and Sch{\"o}lkopf, Bernhard},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.05290 [cs, stat]},
  eprint = {2103.05290},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Stochastic generative models enable us to capture the geometric structure of a data manifold lying in a high dimensional space through a Riemannian metric in the latent space. However, its practical use is rather limited mainly due to inevitable complexity. In this work we propose a surrogate conformal Riemannian metric in the latent space of a generative model that is simple, efficient and robust. This metric is based on a learnable prior that we propose to learn using a basic energy-based model. We theoretically analyze the behavior of the proposed metric and show that it is sensible to use in practice. We demonstrate experimentally the efficiency and robustness, as well as the behavior of the new approximate metric. Also, we show the applicability of the proposed methodology for data analysis in the life sciences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Arvanitidis_2021_A_prior-based_approximate_latent_Riemannian_metric.pdf;/Users/cife/Zotero/storage/I5WLR2CH/2103.html}
}

@article{bachoc2017,
  title = {Some Properties of Nested {{Kriging}} Predictors},
  author = {Bachoc, Fran{\c c}ois and Durrande, Nicolas and Rulli{\`e}re, Didier and Chevalier, Cl{\'e}ment},
  year = {2017},
  month = jul,
  abstract = {Kriging is a widely employed technique, in particular for computer experiments, in machine learning or in geostatistics. An important challenge for Kriging is the computational burden when the data set is large. We focus on a class of methods aiming at decreasing this computational cost, consisting in aggregating Kriging predictors based on smaller data subsets. We prove that aggregations based solely on the conditional variances provided by the different Kriging predictors can yield an inconsistent final Kriging prediction. In contrasts, we study theoretically the recent proposal by [Rulli\{\textbackslash `e\}re et al., 2017] and obtain additional attractive properties for it. We prove that this predictor is consistent, we show that it can be interpreted as an exact conditional distribution for a modified process and we provide error bounds for it.}
}

@article{bachoc2019,
  title = {Maximum Likelihood Estimation for {{Gaussian}} Processes under Inequality Constraints},
  author = {Bachoc, Fran{\c c}ois and Lagnoux, Agn{\`e}s and {L{\'o}pez-Lopera}, Andr{\'e}s F.},
  year = {2019},
  month = jul,
  journal = {arXiv:1804.03378 [math, stat]},
  eprint = {1804.03378},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We consider covariance parameter estimation for a Gaussian process under inequality constraints (boundedness, monotonicity or convexity) in fixed-domain asymptotics. We address the estimation of the variance parameter and the estimation of the microergodic parameter of the Mat\'ern and Wendland covariance functions. First, we show that the (unconstrained) maximum likelihood estimator has the same asymptotic distribution, unconditionally and conditionally to the fact that the Gaussian process satisfies the inequality constraints. Then, we study the recently suggested constrained maximum likelihood estimator. We show that it has the same asymptotic distribution as the (unconstrained) maximum likelihood estimator. In addition, we show in simulations that the constrained maximum likelihood estimator is generally more accurate on finite samples. Finally, we provide extensions to prediction and to noisy observations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Bachoc_2019_Maximum_likelihood_estimation_for_Gaussian_processes_under_inequality.pdf}
}

@article{baekdal2018,
  title = {Pharmacokinetics, {{Safety}}, and {{Tolerability}} of {{Oral Semaglutide}} in {{Subjects With Hepatic Impairment}}},
  author = {B{\ae}kdal, Tine A. and Thomsen, Mette and Kup{\v c}ov{\'a}, Viera and Hansen, Cilie W. and Anderson, Thomas W.},
  year = {2018},
  journal = {The Journal of Clinical Pharmacology},
  volume = {58},
  number = {10},
  pages = {1314--1323},
  issn = {1552-4604},
  doi = {10.1002/jcph.1131},
  abstract = {Semaglutide is a human glucagon-like peptide-1 analog that has been co-formulated with the absorption enhancer, sodium N-(8-[2-hydroxybenzoyl] amino) caprylate, for oral administration. This trial (NCT02016911) investigated whether hepatic impairment affects the pharmacokinetics, safety, and tolerability of oral semaglutide. Subjects were classified into groups: normal hepatic function (n = 24), and mild (n = 12), moderate (n = 12), or severe (n = 8) hepatic impairment according to Child-Pugh criteria, and received once-daily oral semaglutide (5 mg for 5 days followed by 10 mg for 5 days). Semaglutide plasma concentrations were measured during dosing and for up to 21 days post-last dose. Area under the semaglutide plasma concentration\textendash time curve from 0\textendash 24 hours after the 10th dose (primary end point) and maximum semaglutide concentration after the 10th dose appeared similar across hepatic function groups. Similarly, there was no apparent effect of hepatic impairment on time to maximum semaglutide concentration (median range 1.0\textendash 1.5 hours) or half-life (geometric mean range 142\textendash 156 hours). No safety concerns were identified in subjects with hepatic impairment receiving semaglutide. Reported adverse events were in line with those observed for other glucagon-like peptide-1 receptor agonists. There was no apparent effect of hepatic impairment on the pharmacokinetics, safety, and tolerability of oral semaglutide. The results of this trial suggest that dose adjustment of oral semaglutide is not warranted in subjects with hepatic impairment.},
  langid = {english},
  keywords = {GLP-1 receptor agonists,hepatic impairment,novo,pharmacokinetics,semaglutide},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcph.1131},
  file = {/Users/cife/Dropbox/Zotero/Bækdal_2018_Pharmacokinetics,_Safety,_and_Tolerability_of_Oral_Semaglutide_in_Subjects_With.pdf;/Users/cife/Zotero/storage/R4GBFNSS/jcph.html}
}

@article{baekdal2018a,
  title = {A Randomized Study Investigating the Effect of Omeprazole on the Pharmacokinetics of Oral Semaglutide},
  author = {B{\ae}kdal, Tine A. and Breitschaft, Astrid and Navarria, Andrea and Hansen, Cilie W.},
  year = {2018},
  month = aug,
  journal = {Expert Opinion on Drug Metabolism \& Toxicology},
  volume = {14},
  number = {8},
  pages = {869--877},
  issn = {1742-5255, 1744-7607},
  doi = {10.1080/17425255.2018.1488965},
  abstract = {Background: Since the first oral glucagon-like peptide-1 analog comprises semaglutide co-formulated with an absorption enhancer, sodium N-(8-[2-hydroxybenzoyl] amino) caprylate, which induces a transient, localized increase in gastric pH, we have investigated whether a proton pump inhibitor affects the pharmacokinetics of oral semaglutide. Research design and methods: A single-center, randomized, open-label, parallel-group trial investigated pharmacokinetic interactions of oral semaglutide with omeprazole (40 mg once-daily) in 54 healthy subjects. Primary endpoints were area under the plasma concentration-time curve over 24 h for semaglutide (AUC0-24h,semaglutide,Day10) and maximum concentration of semaglutide (Cmax,semaglutide,Day10) at day 10. Results: Exposure of semaglutide appeared to be slightly increased, although not statistically significantly, with oral semaglutide plus omeprazole versus oral semaglutide alone (AUC0-24h,semaglutide,Day10 [estimated treatment ratio 1.13; 90\%CI 0.88, 1.45] and Cmax,semaglutide,Day10 [estimated treatment ratio 1.16; 90\%CI 0.90, 1.49]). Gastric pH was higher with oral semaglutide and omeprazole versus oral semaglutide alone. Adverse events were mild or moderate and, most commonly, gastrointestinal disorders. Conclusions: There was a slight non-statistically significant increase in semaglutide exposure when oral semaglutide was administered with omeprazole, but this is not considered clinically relevant and no dose adjustment is likely to be required.},
  langid = {english},
  keywords = {novo},
  file = {/Users/cife/Zotero/storage/IXP8MNUK/Bækdal et al. - 2018 - A randomized study investigating the effect of ome.pdf}
}

@article{baekdal2019,
  title = {Effect of {{Oral Semaglutide}} on the {{Pharmacokinetics}} of {{Lisinopril}}, {{Warfarin}}, {{Digoxin}}, and {{Metformin}} in {{Healthy Subjects}}},
  author = {B{\ae}kdal, Tine A. and Borregaard, Jeanett and Hansen, Cilie W. and Thomsen, Mette and Anderson, Thomas W.},
  year = {2019},
  month = sep,
  journal = {Clinical Pharmacokinetics},
  volume = {58},
  number = {9},
  pages = {1193--1203},
  issn = {1179-1926},
  doi = {10.1007/s40262-019-00756-2},
  abstract = {Oral semaglutide is a tablet co-formulation of the human glucagon-like peptide-1 (GLP-1) analog semaglutide with the absorption enhancer sodium N-(8-[2-hydroxybenzoyl]~amino)~caprylate (SNAC). The absorption of coadministered oral drugs may be altered due to enhancement by SNAC, potential gastric emptying delay by semaglutide, or other mechanisms. Two one-sequence crossover trials investigated the effect of oral semaglutide on the pharmacokinetics of lisinopril, warfarin, digoxin, and metformin.},
  langid = {english},
  keywords = {novo},
  file = {/Users/cife/Dropbox/Zotero/Bækdal_2019_Effect_of_Oral_Semaglutide_on_the_Pharmacokinetics_of_Lisinopril,_Warfarin,.pdf}
}

@article{baez,
  title = {Information {{Geometry}} ({{Part}} 1)},
  author = {Baez, John},
  pages = {108},
  langid = {english},
  file = {/Users/cife/Zotero/storage/CK9G8264/Baez - Information Geometry (Part 1).pdf}
}

@article{baharlouei2020,
  title = {R\textbackslash 'enyi {{Fair Inference}}},
  author = {Baharlouei, Sina and Nouiehed, Maher and Beirami, Ahmad and Razaviyayn, Meisam},
  year = {2020},
  month = jan,
  journal = {arXiv:1906.12005 [cs, stat]},
  eprint = {1906.12005},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence between the variables, or they lack computational convergence guarantees. In this paper, we use Re\textasciiacute nyi correlation as a measure of fairness of machine learning models and develop a general training framework to impose fairness. In particular, we propose a min-max formulation which balances the accuracy and fairness when solved to optimality. For the case of discrete sensitive attributes, we suggest an iterative algorithm with theoretical convergence guarantee for solving the proposed min-max problem. Our algorithm and analysis are then specialized to fair classification and fair clustering problems. To demonstrate the performance of the proposed Re\textasciiacute nyi fair inference framework in practice, we compare it with wellknown existing methods on several benchmark datasets. Experiments indicate that the proposed method has favorable empirical performance against state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,fairness,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Baharlouei_2020_R-'enyi_Fair_Inference.pdf}
}

@article{bankestad2020,
  title = {The {{Elliptical Processes}}: A {{New Family}} of {{Flexible Stochastic Processes}}},
  shorttitle = {The {{Elliptical Processes}}},
  author = {B{\aa}nkestad, Maria and Sj{\"o}lund, Jens and Taghia, Jalil and Sch{\"o}n, Thomas},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.07201 [cs, stat]},
  eprint = {2003.07201},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the elliptical processes-a new family of stochastic processes that subsumes the Gaussian process and the Student-t process. This generalization retains computational tractability while substantially increasing the range of tail behaviors that can be modeled. We base the elliptical processes on a representation of elliptical distributions as mixtures of Gaussian distributions and derive closed-form expressions for the marginal and conditional distributions. We perform an in-depth study of a particular elliptical process, where the mixture distribution is piecewise constant, and show some of its advantages over the Gaussian process through a number of experiments on robust regression. Looking forward, we believe there are several settings, e.g. when the likelihood is not Gaussian or when accurate tail modeling is critical, where the elliptical processes could become the stochastic processes of choice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Bånkestad_2020_The_Elliptical_Processes.pdf;/Users/cife/Zotero/storage/2LI86QKY/2003.html}
}

@article{barakat1970,
  title = {Mean and Variance of the Arc Length of a {{Gaussian}} Process on a Finite Interval\textdagger},
  author = {Barakat, Richard and Baumann, Elizabeth},
  year = {1970},
  month = sep,
  journal = {International Journal of Control},
  volume = {12},
  number = {3},
  pages = {377--383},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207177008931855},
  abstract = {The first and second moments of the distribution function of the arc length of a Gaussian process on a finite interval are obtained in terms of the covariance function of the derivative process. A closed expression (in terms of a modified Bessel function) was obtained for the first moment; however, the second moment had to be evaluated numerically. Numerical calculations were carried out for three typical covariance functions.},
  keywords = {read-soon},
  annotation = {\_eprint: https://doi.org/10.1080/00207177008931855},
  file = {/Users/cife/Dropbox/Zotero/BARAKAT_1970_Mean_and_variance_of_the_arc_length_of_a_Gaussian_process_on_a_finite_interval†.pdf;/Users/cife/Zotero/storage/AKT6DT22/00207177008931855.html}
}

@misc{barr,
  title = {The {{Einstein Summation Notation}}},
  author = {Barr, Alan H.},
  publisher = {{California Institute of Technology}},
  file = {/Users/cife/Zotero/storage/C2ZVNZBJ/Einstein-Summation-Notation.pdf}
}

@book{bartels1995,
  title = {An Introduction to Splines for Use in Computer Graphics and Geometric Modeling},
  author = {Bartels, Richard H and Beatty, John C and Barsky, Brian A},
  year = {1995},
  publisher = {{Morgan Kaufmann}}
}

@article{bartels2022,
  title = {Adaptive {{Cholesky Gaussian Processes}}},
  author = {Bartels, Simon and {Stensbo-Smidt}, Kristoffer and {Moreno-Mu{\~n}oz}, Pablo and Boomsma, Wouter and Frellsen, Jes and Hauberg, S{\o}ren},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.10769 [cs]},
  eprint = {2202.10769},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a method to fit exact Gaussian process models to large datasets by considering only a subset of the data. Our approach is novel in that the size of the subset is selected on the fly during exact inference with little computational overhead. From an empirical observation that the log-marginal likelihood often exhibits a linear trend once a sufficient subset of a dataset has been observed, we conclude that many large datasets contain redundant information that only slightly affects the posterior. Based on this, we provide probabilistic bounds on the full model evidence that can identify such subsets. Remarkably, these bounds are largely composed of terms that appear in intermediate steps of the standard Cholesky decomposition, allowing us to modify the algorithm to adaptively stop the decomposition once enough data have been observed. Empirically, we show that our method can be directly plugged into well-known inference schemes to fit exact Gaussian process models to large datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bartels_2022_Adaptive_Cholesky_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/UAPX5CRA/2202.html}
}

@article{bartels2022a,
  title = {Adaptive {{Cholesky Gaussian Processes}}},
  author = {Bartels, Simon and {Stensbo-Smidt}, Kristoffer and {Moreno-Mu{\~n}oz}, Pablo and Boomsma, Wouter and Frellsen, Jes and Hauberg, S{\o}ren},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.10769 [cs]},
  eprint = {2202.10769},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a method to fit exact Gaussian process models to large datasets by considering only a subset of the data. Our approach is novel in that the size of the subset is selected on the fly during exact inference with little computational overhead. From an empirical observation that the log-marginal likelihood often exhibits a linear trend once a sufficient subset of a dataset has been observed, we conclude that many large datasets contain redundant information that only slightly affects the posterior. Based on this, we provide probabilistic bounds on the full model evidence that can identify such subsets. Remarkably, these bounds are largely composed of terms that appear in intermediate steps of the standard Cholesky decomposition, allowing us to modify the algorithm to adaptively stop the decomposition once enough data have been observed. Empirically, we show that our method can be directly plugged into well-known inference schemes to fit exact Gaussian process models to large datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bartels_2022_Adaptive_Cholesky_Gaussian_Processes2.pdf;/Users/cife/Zotero/storage/W22ZKTPU/2202.html}
}

@incollection{basak2022,
  title = {Numerical Issues in Maximum Likelihood Parameter Estimation for {{Gaussian}} Process Interpolation},
  author = {Basak, Subhasish and Petit, S{\'e}bastien and Bect, Julien and Vazquez, Emmanuel},
  year = {2022},
  volume = {13164},
  eprint = {2101.09747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {116--131},
  doi = {10.1007/978-3-030-95470-3_9},
  abstract = {This article investigates the origin of numerical issues in maximum likelihood parameter estimation for Gaussian process (GP) interpolation and investigates simple but effective strategies for improving commonly used open-source software implementations. This work targets a basic problem but a host of studies, particularly in the literature of Bayesian optimization, rely on off-the-shelf GP implementations. For the conclusions of these studies to be reliable and reproducible, robust GP implementations are critical.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Basak_2022_Numerical_issues_in_maximum_likelihood_parameter_estimation_for_Gaussian.pdf;/Users/cife/Zotero/storage/IIFU9EXM/2101.html}
}

@incollection{bauer2016,
  title = {Understanding {{Probabilistic Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Bauer, Matthias and {van der Wilk}, Mark and Rasmussen, Carl Edward},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {1533--1541},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Bauer_2016_Understanding_Probabilistic_Sparse_Gaussian_Process_Approximations.pdf;/Users/cife/Zotero/storage/QVRJ4PAQ/Supplement_camera_ready.pdf;/Users/cife/Zotero/storage/K5TJ79U2/6477-understanding-probabilistic-sparse-gaussian-process-approximations.html}
}

@article{bauer2017,
  ids = {bauer2017a},
  title = {Understanding {{Probabilistic Sparse Gaussian Process Approximations}}},
  author = {Bauer, Matthias and {van der Wilk}, Mark and Rasmussen, Carl Edward},
  year = {2017},
  month = may,
  journal = {arXiv:1606.04820 [stat]},
  eprint = {1606.04820},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bauer_2017_Understanding_Probabilistic_Sparse_Gaussian_Process_Approximations.pdf;/Users/cife/Zotero/storage/AR54U8QZ/1606.html}
}

@book{bayes1763,
  title = {An Essay towards Solv a {{Problem}} in the {{Doctrine}} of {{Chances}}},
  author = {Bayes, Thomas},
  year = {1763},
  volume = {53},
  publisher = {{Philosophical Transactions of the Royal Society of London}}
}

@article{beal,
  title = {Variational {{Algorithms}} for {{Approximate Bayesian Inference}}},
  author = {Beal, Matthew James},
  pages = {281},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Beal_Variational_Algorithms_for_Approximate_Bayesian_Inference.pdf}
}

@book{beal2003,
  title = {Variational Algorithms for Approximate {{Bayesian}} Inference},
  author = {Beal, Matthew James},
  year = {2003},
  publisher = {{University of London, University College London (United Kingdom)}}
}

@misc{beik-mohammadi2022,
  title = {Reactive {{Motion Generation}} on {{Learned Riemannian Manifolds}}},
  author = {{Beik-Mohammadi}, Hadi and Hauberg, S{\o}ren and Arvanitidis, Georgios and Neumann, Gerhard and Rozo, Leonel},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07761},
  eprint = {2203.07761},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is especially intended to recover full-pose end-effector states and joint space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacleaware ambient metric. The motion generated using these geodesics may naturally result in multiple-solution tasks that have not been explicitly demonstrated previously. We extensively tested our approach in task space and joint space scenarios using a 7-DoF robotic manipulator. We demonstrate that our method is capable of learning and generating motion skills based on complicated motion patterns demonstrated by a human operator. Additionally, we assess several obstacle avoidance strategies and generate trajectories in multiple-mode settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/cife/Zotero/storage/UL2LXZZX/Beik-Mohammadi et al. - 2022 - Reactive Motion Generation on Learned Riemannian M.pdf}
}

@article{belkin2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = sep,
  journal = {arXiv:1812.11118 [cs, stat]},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Belkin_2019_Reconciling_modern_machine_learning_practice_and_the_bias-variance_trade-off.pdf;/Users/cife/Zotero/storage/RL4DG6KE/1812.html}
}

@article{bengio2004,
  title = {Out-of-{{Sample Extensions}} for {{LLE}}, {{Isomap}}, {{MDS}}, {{Eigenmaps}}, and {{Spectral Clustering}}},
  author = {Bengio, Yoshua and Paiement, Jean-fran{\c c}cois and Vincent, Pascal and Delalleau, Olivier and Roux, Nicolas L and Ouimet, Marie},
  year = {2004},
  pages = {8},
  abstract = {Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data.},
  langid = {english},
  keywords = {read-soon,sb},
  file = {/Users/cife/Dropbox/Zotero/Bengio_2004_Out-of-Sample_Extensions_for_LLE,_Isomap,_MDS,_Eigenmaps,_and_Spectral.pdf}
}

@article{bengio2013,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  keywords = {Abstracts,AI,Algorithms,artificial intelligence,Artificial Intelligence,autoencoder,autoencoders,Boltzmann machine,data representation,data structures,Deep learning,density estimation,Feature extraction,feature learning,geometrical connections,Humans,Learning systems,Machine learning,machine learning algorithms,manifold learning,Manifolds,neural nets,Neural networks,Neural Networks (Computer),probabilistic models,probability,read,read-soon,representation learning,sb,Speech recognition,unsupervised feature learning,unsupervised learning},
  file = {/Users/cife/Dropbox/Zotero/Bengio_2013_Representation_Learning.pdf;/Users/cife/Zotero/storage/4KLLY45P/6472238.html}
}

@article{bengio2019,
  title = {The {{Consciousness Prior}}},
  author = {Bengio, Yoshua},
  year = {2019},
  month = dec,
  journal = {arXiv:1709.08568 [cs, stat]},
  eprint = {1709.08568},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bengio_2019_The_Consciousness_Prior.pdf;/Users/cife/Zotero/storage/JTVFQFD9/1709.html}
}

@article{bergsson,
  title = {Visualizing {{Riemannian}} Data with {{Rie-SNE}}},
  author = {Bergsson, Andri and Hauberg, S{\o}ren},
  pages = {7},
  abstract = {Faithful visualizations of data residing on manifolds must take the underlying geometry into account when producing a flat planar view of the data. In this paper, we extend the classic stochastic neighbor embedding (SNE) algorithm to data on general Riemannian manifolds. We replace standard Gaussian assumptions with Riemannian diffusion counterparts and propose an efficient approximation that only requires access to calculations of Riemannian distances and volumes. We demonstrate that the approach also allows for mapping data from one manifold to another, e.g. from a high-dimensional sphere to a low-dimensional one.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/ML5XCQEC/Bergsson and Hauberg - Visualizing Riemannian data with Rie-SNE.pdf}
}

@article{bewsher2017,
  title = {Distribution of {{Gaussian Process Arc Lengths}}},
  author = {Bewsher, Justin D. and Tosi, Alessandra and Osborne, Michael A. and Roberts, Stephen J.},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.08031 [stat]},
  eprint = {1703.08031},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We present the first treatment of the arc length of the Gaussian Process (gp) with more than a single output dimension. Gps are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary gp with multiple output dimensions. A new method is used to derive the mean of a one-dimensional gp over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued gp in Rn by moment matching the distribution. Numerical simulations confirm our theoretical derivations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read-very-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bewsher_2017_Distribution_of_Gaussian_Process_Arc_Lengths.pdf}
}

@article{bingham2019,
  title = {Pyro: Deep Universal Probabilistic Programming},
  shorttitle = {Pyro},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  year = {2019},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {20},
  number = {1},
  pages = {973--978},
  issn = {1532-4435},
  abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
  keywords = {approximate Bayesian inference,code,deep learning,generative models,graphical models,probabilistic programming,read},
  file = {/Users/cife/Dropbox/Zotero/Bingham_2019_Pyro.pdf}
}

@article{binns2018,
  title = {Fairness in {{Machine Learning}}: {{Lessons}} from {{Political Philosophy}}},
  shorttitle = {Fairness in {{Machine Learning}}},
  author = {Binns, Reuben},
  year = {2018},
  month = jan,
  journal = {arXiv:1712.03586 [cs]},
  eprint = {1712.03586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Binns_2018_Fairness_in_Machine_Learning.pdf;/Users/cife/Zotero/storage/ZR873DSP/1712.html}
}

@inproceedings{bishop1997,
  title = {Magnification Factors for the {{GTM}} Algorithm},
  booktitle = {Fifth {{International Conference}} on {{Artificial Neural Networks}}},
  author = {Bishop, C.M.},
  year = {1997},
  volume = {1997},
  pages = {64--69},
  publisher = {{IEE}},
  address = {{Cambridge, UK}},
  doi = {10.1049/cp:19970703},
  abstract = {The Generative Topographic Mapping (GTM) algorithm of Bishop, Svens\textasciiacute en, and Williams (1998) has been introduced as a principled alternative to the SelfOrganizing Map (SOM). As well as avoiding a number of deficiencies in the SOM, the GTM algorithm has the key property that the smoothness properties of the model are decoupled from the reference vectors, and are described by a continuous mapping from a lower-dimensional latent space into the data space. Magnification factors, which are approximated by the difference between code-book vectors in SOMs, can therefore be evaluated for the GTM model as continuous functions of the latent variables using the techniques of differential geometry. They play an important role in data visualization by highlighting the boundaries between data clusters, and are illustrated here for both a toy data set, and a problem involving the identification of crab species from morphological data.},
  isbn = {978-0-85296-690-7},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Bishop_1997_Magnification_factors_for_the_GTM_algorithm.pdf}
}

@book{bishop2006,
  ids = {bishopPatternRecognitionMachine2006a},
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception,reference},
  file = {/Users/cife/Dropbox/Zotero/Bishop_2006_Pattern_recognition_and_machine_learning.pdf}
}

@article{bishop2018,
  title = {An {{Introduction}} to {{Wishart Matrix Moments}}},
  author = {Bishop, Adrian N. and Del Moral, Pierre and Niclas, Angele},
  year = {2018},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {11},
  number = {2},
  eprint = {1710.10864},
  eprinttype = {arxiv},
  pages = {97--218},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000072},
  abstract = {These lecture notes provide a comprehensive, self-contained introduction to the analysis of Wishart matrix moments. This study may act as an introduction to some particular aspects of random matrix theory, or as a self-contained exposition of Wishart matrix moments. Random matrix theory plays a central role in statistical physics, computational mathematics and engineering sciences, including data assimilation, signal processing, combinatorial optimization, compressed sensing, econometrics and mathematical finance, among numerous others. The mathematical foundations of the theory of random matrices lies at the intersection of combinatorics, non-commutative algebra, geometry, multivariate functional and spectral analysis, and of course statistics and probability theory. As a result, most of the classical topics in random matrix theory are technical, and mathematically difficult to penetrate for non-experts and regular users and practitioners. The technical aim of these notes is to review and extend some important results in random matrix theory in the specific context of real random Wishart matrices. This special class of Gaussian-type sample covariance matrix plays an important role in multivariate analysis and in statistical theory. We derive non-asymptotic formulae for the full matrix moments of real valued Wishart random matrices. As a corollary, we derive and extend a number of spectral and trace-type results for the case of non-isotropic Wishart random matrices. We also derive the full matrix moment analogues of some classic spectral and trace-type moment results. For example, we derive semi-circle and Marchencko-Pastur-type laws in the non-isotropic and full matrix cases. Laplace matrix transforms and matrix moment estimates are also studied, along with new spectral and trace concentration-type inequalities.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/cife/Dropbox/Zotero/Bishop_2018_An_Introduction_to_Wishart_Matrix_Moments.pdf;/Users/cife/Zotero/storage/JCZER3UP/1710.html}
}

@inproceedings{bitzer2010,
  title = {Kick-Starting {{GPLVM}} Optimization via a Connection to Metric {{MDS}}},
  booktitle = {{{NIPS}} 2010 {{Workshop}} on {{Challenges}} of {{Data Visualization}}},
  author = {Bitzer, Sebastian and Williams, Christopher K. I.},
  year = {2010},
  month = dec,
  doi = {http://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/abstracts.html},
  abstract = {Author: Bitzer, Sebastian et al.; Genre: Conference Paper; Published in Print: 2010-12; Open Access; Title: Kick-starting GPLVM optimization via a connection to metric MDS},
  keywords = {read-someday,sb},
  file = {/Users/cife/Dropbox/Zotero/Bitzer_2010_Kick-starting_GPLVM_optimization_via_a_connection_to_metric_MDS.pdf;/Users/cife/Zotero/storage/348RWY8X/ViewItemOverviewPage.html}
}

@article{blei2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Blei_2017_Variational_Inference.pdf}
}

@incollection{blote2016,
  title = {Introduction to {{General Relativity}}},
  author = {Blote, Henk W J},
  year = {2016},
  pages = {78},
  address = {{Lorentz Institute for Theoretical Physics}},
  langid = {english},
  keywords = {geometry,reference},
  file = {/Users/cife/Zotero/storage/9IUL3CYN/Blote - Introduction to General Relativity.pdf}
}

@article{bonilla2018,
  title = {Generic {{Inference}} in {{Latent Gaussian Process Models}}},
  author = {Bonilla, Edwin V. and Krauth, Karl and Dezfouli, Amir},
  year = {2018},
  month = nov,
  journal = {arXiv:1609.00577 [stat]},
  eprint = {1609.00577},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using samples from univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our approach quantitatively and qualitatively with experiments on small datasets, medium-scale datasets and large datasets, showing its competitiveness under different likelihood models and sparsity levels. On the large-scale experiments involving prediction of airline delays and classification of handwritten digits, we show that our method is on par with the state-of-the-art hard-coded approaches for scalable GP regression and classification.},
  archiveprefix = {arXiv},
  keywords = {read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bonilla_2018_Generic_Inference_in_Latent_Gaussian_Process_Models.pdf;/Users/cife/Zotero/storage/FJE6G7G9/1609.html}
}

@article{borovitskiy2020,
  title = {Mat\'ern {{Gaussian Processes}} on {{Riemannian Manifolds}}},
  author = {Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/borovitskiy2020.md;/Users/cife/Dropbox/Zotero/Borovitskiy_2020_Matérn_Gaussian_Processes_on_Riemannian_Manifolds.pdf;/Users/cife/Zotero/storage/98VA5CUD/92bf5e6240737e0326ea59846a83e076-Abstract.html;/Users/cife/Zotero/storage/ME2AWIIA/92bf5e6240737e0326ea59846a83e076-Abstract.html}
}

@inproceedings{brand2003,
  title = {Charting a {{Manifold}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brand, Matthew},
  year = {2003},
  volume = {15},
  publisher = {{MIT Press}},
  file = {/Users/cife/Dropbox/Zotero/Brand_2003_Charting_a_Manifold.pdf}
}

@article{bronstein2017,
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  shorttitle = {Geometric Deep Learning},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  eprint = {1611.08097},
  eprinttype = {arxiv},
  pages = {18--42},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Bronstein_2017_Geometric_deep_learning.pdf}
}

@article{bronstein2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  journal = {arXiv:2104.13478 [cs, stat]},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Bronstein_2021_Geometric_Deep_Learning.pdf;/Users/cife/Zotero/storage/I9GBDNVR/2104.html}
}

@article{bui2015a,
  title = {Stochastic {{Variational Inference}} for {{Gaussian Process Latent Variable Models}} Using {{Back Constraints}}},
  author = {Bui, Thang D and Turner, Richard E},
  year = {2015},
  journal = {Black Box Learning and Inference NIPS workshop},
  pages = {5},
  abstract = {Gaussian process latent variable models (GPLVMs) are a probabilistic approach to modelling data that employs Gaussian process mapping from latent variables to observations. This paper revisits a recently proposed variational inference technique for GPLVMs and methodologically analyses the optimality and different parameterisations of the variational approximation. We investigate a structured variational distribution, that maintains information about the dependencies between hidden dimensions, and propose a mini-batch based stochastic training procedure, enabling more scalable training algorithm. This is achieved by using variational recognition models (also known as back constraints) to parameterise the variational approximation. We demonstrate the validity of our approach on a set of unsupervised learning tasks for texture images and handwritten digits.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/DM42FQS9/Bui and Turner - Stochastic Variational Inference for Gaussian Proc.pdf}
}

@article{bui2016,
  title = {A {{Unifying Framework}} for {{Gaussian Process Pseudo-Point Approximations}} Using {{Power Expectation Propagation}}},
  author = {Bui, Thang D and Yan, Josiah and Turner, Richard E},
  year = {2016},
  pages = {72},
  abstract = {Gaussian processes (GPs) are flexible distributions over functions that enable highlevel assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at `inference time' rather than at `modelling time', resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.},
  langid = {english},
  keywords = {read-someday,read-soon,scalable-gp},
  file = {/Users/cife/Dropbox/Zotero/Bui_2016_A_Unifying_Framework_for_Gaussian_Process_Pseudo-Point_Approximations_using.pdf}
}

@article{bui2016a,
  title = {Stochastic {{Variational Inference}} for {{Gaussian Process Latent Variable Models}} Using {{Back Constraints}}},
  author = {Bui, Thang D and Turner, Richard E},
  year = {2016},
  pages = {5},
  abstract = {Gaussian process latent variable models (GPLVMs) are a probabilistic approach to modelling data that employs Gaussian process mapping from latent variables to observations. This paper revisits a recently proposed variational inference technique for GPLVMs and methodologically analyses the optimality and different parameterisations of the variational approximation. We investigate a structured variational distribution, that maintains information about the dependencies between hidden dimensions, and propose a mini-batch based stochastic training procedure, enabling more scalable training algorithm. This is achieved by using variational recognition models (also known as back constraints) to parameterise the variational approximation. We demonstrate the validity of our approach on a set of unsupervised learning tasks for texture images and handwritten digits.},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Zotero/storage/LYZIJNFB/Bui and Turner - Stochastic Variational Inference for Gaussian Proc.pdf}
}

@article{calandra2016,
  title = {Manifold {{Gaussian Processes}} for {{Regression}}},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  year = {2016},
  month = apr,
  journal = {arXiv:1402.5876 [cs, stat]},
  eprint = {1402.5876},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Calandra_2016_Manifold_Gaussian_Processes_for_Regression.pdf;/Users/cife/Zotero/storage/DR5WZL2J/1402.html}
}

@article{calandra2016a,
  title = {Manifold {{Gaussian Processes}} for {{Regression}}},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  year = {2016},
  month = apr,
  journal = {arXiv:1402.5876 [cs, stat]},
  eprint = {1402.5876},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Calandra_2016_Manifold_Gaussian_Processes_for_Regression3.pdf;/Users/cife/Zotero/storage/SPNYY6FB/1402.html}
}

@inproceedings{calandra2016b,
  title = {Manifold {{Gaussian Processes}} for Regression},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  year = {2016},
  month = jul,
  pages = {3338--3345},
  publisher = {{IEEE}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1109/IJCNN.2016.7727626},
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and nondifferentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  isbn = {978-1-5090-0620-5},
  langid = {english},
  file = {/Users/cife/Zotero/storage/A3V35WTJ/Calandra et al. - 2016 - Manifold Gaussian Processes for regression.pdf}
}

@article{campbell2014,
  title = {Learning a Manifold of Fonts},
  author = {Campbell, Neill D. F. and Kautz, Jan},
  year = {2014},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {4},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/2601097.2601212},
  langid = {english},
  keywords = {application,read},
  file = {/Users/cife/Dropbox/Zotero/Campbell_2014_Learning_a_manifold_of_fonts.pdf;/Users/cife/Dropbox/Zotero/Campbell_2014_Learning_a_manifold_of_fonts2.pdf}
}

@article{cao2017,
  title = {Automatic {{Selection}} of T-{{SNE Perplexity}}},
  author = {Cao, Yanshuai and Wang, Luyu},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.03229 [cs, stat]},
  eprint = {1708.03229},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely used dimensionality reduction methods for data visualization, but it has a perplexity hyperparameter that requires manual selection. In practice, proper tuning of t-SNE perplexity requires users to understand the inner working of the method as well as to have hands-on experience. We propose a model selection objective for t-SNE perplexity that requires negligible extra computation beyond that of the t-SNE itself. We empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets. The similarities of our approach to Bayesian information criteria (BIC) and minimum description length (MDL) are also analyzed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Cao_2017_Automatic_Selection_of_t-SNE_Perplexity.pdf;/Users/cife/Zotero/storage/RFC74DB9/1708.html}
}

@misc{carapetis2017,
  title = {Answer to "{{Why}} Does {{Brownian}} Motion Have Drift on {{Riemannian Manifolds}}?"},
  shorttitle = {Answer to "{{Why}} Does {{Brownian}} Motion Have Drift on {{Riemannian Manifolds}}?},
  author = {Carapetis, Anthony},
  year = {2017},
  month = jul,
  journal = {Mathematics Stack Exchange},
  keywords = {Amswer to Why does Brownian motion have drift on Riemannian Manifolds?},
  file = {/Users/cife/Zotero/storage/Y3KF3YPP/why-does-brownian-motion-have-drift-on-riemannian-manifolds.html;/Users/cife/Zotero/storage/YK3LWV63/why-does-brownian-motion-have-drift-on-riemannian-manifolds.html}
}

@article{carlsson2009,
  title = {Topology and Data},
  author = {Carlsson, Gunnar},
  year = {2009},
  month = jan,
  journal = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {255--308},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-09-01249-X},
  langid = {english},
  keywords = {read,sb},
  file = {/Users/cife/Dropbox/phdnotes/reading/carlsson2009.md;/Users/cife/Dropbox/Zotero/Carlsson_2009_Topology_and_data.pdf}
}

@article{carlsson2020,
  title = {Persistent {{Homology}} and {{Applied Homotopy Theory}}},
  author = {Carlsson, Gunnar},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.00738 [math]},
  eprint = {2004.00738},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {This paper is a survey of persistent homology, primarily as it is used in topological data analysis. It includes the theory of persistence modules, as well as stability theorems for persistence barcodes, generalized persistence, vectorization of persistence barcodes, as well as some applications.},
  archiveprefix = {arXiv},
  keywords = {55N31 primary; 62R40; 68T09 secondary,Mathematics - Algebraic Topology,read},
  file = {/Users/cife/Dropbox/Zotero/Carlsson_2020_Persistent_Homology_and_Applied_Homotopy_Theory.pdf;/Users/cife/Zotero/storage/66MILJ87/2004.html}
}

@book{carmo1992,
  title = {Riemannian Geometry},
  author = {do Carmo, Manfredo Perdig{\textbackslash}tilde\{a\}o},
  year = {1992},
  series = {Mathematics. {{Theory}} \& Applications},
  publisher = {{Birkh\"auser}},
  address = {{Boston}},
  isbn = {978-0-8176-3490-2 978-3-7643-3490-1},
  langid = {english},
  lccn = {QA649 .C2913 1992},
  keywords = {Geometry; Riemannian},
  file = {/Users/cife/Zotero/storage/UTN5CK3Y/Riemannian Geometry [do Carmo].pdf}
}

@inproceedings{casale2018,
  title = {Gaussian {{Process Prior Variational Autoencoders}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Casale, Francesco Paolo and Dalca, Adrian and Saglietti, Luca and Listgarten, Jennifer and Fusi, Nicolo},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {10369--10380},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Casale_2018_Gaussian_Process_Prior_Variational_Autoencoders.pdf;/Users/cife/Zotero/storage/3D6SBPTQ/8238-gaussian-process-prior-variational-autoencoders.html}
}

@inproceedings{caselles-dupre2019,
  title = {Symmetry-{{Based Disentangled Representation Learning}} Requires {{Interaction}} with {{Environments}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {{Caselles-Dupr{\'e}}, Hugo and Garcia Ortiz, Michael and Filliat, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {4608--4617},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-soon,sb},
  file = {/Users/cife/Dropbox/Zotero/Caselles-Dupré_2019_Symmetry-Based_Disentangled_Representation_Learning_requires_Interaction_with.pdf;/Users/cife/Zotero/storage/SYIELPYE/8709-symmetry-based-disentangled-representation-learning-requires-interaction-with-environments.html}
}

@article{chawla2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  eprint = {1106.1813},
  eprinttype = {arxiv},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Chawla_2002_SMOTE.pdf;/Users/cife/Zotero/storage/GGUEW8CC/1106.html}
}

@misc{chazal2021,
  title = {An Introduction to {{Topological Data Analysis}}: Fundamental and Practical Aspects for Data Scientists},
  shorttitle = {An Introduction to {{Topological Data Analysis}}},
  author = {Chazal, Fr{\'e}d{\'e}ric and Michel, Bertrand},
  year = {2021},
  month = feb,
  number = {arXiv:1710.04019},
  eprint = {1710.04019},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.04019},
  abstract = {Topological Data Analysis is a recent and fast growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of \textbackslash tda\textbackslash{} for non experts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Chazal_2021_An_introduction_to_Topological_Data_Analysis.pdf;/Users/cife/Zotero/storage/JNH3U6YP/1710.html}
}

@article{chen2018,
  title = {How Priors of Initial Hyperparameters Affect {{Gaussian}} Process Regression Models},
  author = {Chen, Zexun and Wang, Bo},
  year = {2018},
  month = jan,
  journal = {Neurocomputing},
  volume = {275},
  eprint = {1605.07906},
  eprinttype = {arxiv},
  pages = {1702--1710},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.10.028},
  abstract = {The hyperparameters in Gaussian process regression (GPR) model with a specified kernel are often estimated from the data via the maximum marginal likelihood. Due to the non-convexity of marginal likelihood with respect to the hyperparameters, the optimization may not converge to the global maxima. A common approach to tackle this issue is to use multiple starting points randomly selected from a specific prior distribution. As a result the choice of prior distribution may play a vital role in the predictability of this approach. However, there exists little research in the literature to study the impact of the prior distributions on the hyperparameter estimation and the performance of GPR. In this paper, we provide the first empirical study on this problem using simulated and real data experiments. We consider different types of priors for the initial values of hyperparameters for some commonly used kernels and investigate the influence of the priors on the predictability of GPR models. The results reveal that, once a kernel is chosen, different priors for the initial hyperparameters have no significant impact on the performance of GPR prediction, despite that the estimates of the hyperparameters are very different to the true values in some cases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read,sb,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Chen_2018_How_priors_of_initial_hyperparameters_affect_Gaussian_process_regression_models.pdf}
}

@article{chen2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  journal = {arXiv:1806.07366 [cs, stat]},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Chen_2019_Neural_Ordinary_Differential_Equations.pdf;/Users/cife/Dropbox/Zotero/Chen_2019_Neural_Ordinary_Differential_Equations2.pdf}
}

@article{chen2020,
  title = {Learning {{Flat Latent Manifolds}} with {{VAEs}}},
  author = {Chen, Nutan and Klushyn, Alexej and Ferroni, Francesco and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.04881 [cs, stat]},
  eprint = {2002.04881},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Measuring the similarity between data points often requires domain knowledge. This can in parts be compensated by relying on unsupervised methods such as latent-variable models, where similarity/distance is estimated in a more compact latent space. Prevalent is the use of the Euclidean metric, which has the drawback of ignoring information about similarity of data stored in the decoder, as captured by the framework of Riemannian geometry. Alternatives---such as approximating the geodesic---are often computationally inefficient, rendering the methods impractical. We propose an extension to the framework of variational auto-encoders allows learning flat latent manifolds, where the Euclidean metric is a proxy for the similarity between data points. This is achieved by defining the latent space as a Riemannian manifold and by regularising the metric tensor to be a scaled identity matrix. Additionally, we replace the compact prior typically used in variational auto-encoders with a recently presented, more expressive hierarchical one---and formulate the learning problem as a constrained optimisation problem. We evaluate our method on a range of data-sets, including a video-tracking benchmark, where the performance of our unsupervised approach nears that of state-of-the-art supervised approaches, while retaining the computational efficiency of straight-line-based approaches.},
  archiveprefix = {arXiv},
  keywords = {bad,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Chen_2020_Learning_Flat_Latent_Manifolds_with_VAEs.pdf;/Users/cife/Zotero/storage/HRPE35IP/2002.html}
}

@inproceedings{chen2020a,
  title = {Stochastic {{Gradient Descent}} in {{Correlated Settings}}: {{A Study}} on {{Gaussian Processes}}},
  shorttitle = {Stochastic {{Gradient Descent}} in {{Correlated Settings}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Hao and Zheng, Lili and AL Kontar, Raed and Raskutti, Garvesh},
  year = {2020},
  volume = {33},
  pages = {2722--2733},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/cife/Dropbox/Zotero/Chen_2020_Stochastic_Gradient_Descent_in_Correlated_Settings.pdf}
}

@article{chen2021,
  title = {Gaussian {{Process Inference Using Mini-batch Stochastic Gradient Descent}}: {{Convergence Guarantees}} and {{Empirical Benefits}}},
  shorttitle = {Gaussian {{Process Inference Using Mini-batch Stochastic Gradient Descent}}},
  author = {Chen, Hao and Zheng, Lili and Kontar, Raed Al and Raskutti, Garvesh},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.10461 [cs, stat]},
  eprint = {2111.10461},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Stochastic gradient descent (SGD) and its variants have established themselves as the go-to algorithms for large-scale machine learning problems with independent samples due to their generalization performance and intrinsic computational advantage. However, the fact that the stochastic gradient is a biased estimator of the full gradient with correlated samples has led to the lack of theoretical understanding of how SGD behaves under correlated settings and hindered its use in such cases. In this paper, we focus on hyperparameter estimation for the Gaussian process (GP) and take a step forward towards breaking the barrier by proving minibatch SGD converges to a critical point of the full log-likelihood loss function, and recovers model hyperparameters with rate \$O(\textbackslash frac\{1\}\{K\})\$ for \$K\$ iterations, up to a statistical error term depending on the minibatch size. Our theoretical guarantees hold provided that the kernel functions exhibit exponential or polynomial eigendecay which is satisfied by a wide range of kernels commonly used in GPs. Numerical studies on both simulated and real datasets demonstrate that minibatch SGD has better generalization over state-of-the-art GP methods while reducing the computational burden and opening a new, previously unexplored, data size regime for GPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Chen_2021_Gaussian_Process_Inference_Using_Mini-batch_Stochastic_Gradient_Descent.pdf;/Users/cife/Zotero/storage/SIY2Q8EG/2111.html}
}

@incollection{chennuruvankadara2018,
  title = {Measures of Distortion for Machine Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Chennuru Vankadara, Leena and {von Luxburg}, Ulrike},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {4886--4895},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read,sb},
  file = {/Users/cife/Dropbox/Zotero/Chennuru_Vankadara_2018_Measures_of_distortion_for_machine_learning.pdf;/Users/cife/Zotero/storage/LT3XJ7ZN/7737-measures-of-distortion-for-machine-learning.html}
}

@article{cohen-steiner2007,
  title = {Stability of {{Persistence Diagrams}}},
  author = {{Cohen-Steiner}, David and Edelsbrunner, Herbert and Harer, John},
  year = {2007},
  month = jan,
  journal = {Discrete \& Computational Geometry},
  volume = {37},
  number = {1},
  pages = {103--120},
  issn = {1432-0444},
  doi = {10.1007/s00454-006-1276-5},
  abstract = {The persistence diagram of a real-valued function on a topological space is a multiset of points in the extended plane. We prove that under mild assumptions on the function, the persistence diagram is stable:  small changes in the function imply only small changes in the diagram. We apply this result to estimating the homology of sets in a metric space and to comparing and classifying geometric shapes.},
  langid = {english},
  keywords = {read,sb},
  file = {/Users/cife/Dropbox/Zotero/Cohen-Steiner_2007_Stability_of_Persistence_Diagrams.pdf}
}

@article{cohen2016,
  title = {Group {{Equivariant Convolutional Networks}}},
  author = {Cohen, Taco S. and Welling, Max},
  year = {2016},
  month = jun,
  journal = {arXiv:1602.07576 [cs, stat]},
  eprint = {1602.07576},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Cohen_2016_Group_Equivariant_Convolutional_Networks.pdf;/Users/cife/Zotero/storage/392W2WWA/1602.html}
}

@article{cohen2018,
  title = {Spherical {{CNNs}}},
  author = {Cohen, Taco S. and Geiger, Mario and Koehler, Jonas and Welling, Max},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.10130 [cs, stat]},
  eprint = {1801.10130},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Cohen_2018_Spherical_CNNs.pdf}
}

@book{cormen1990,
  title = {Introduction to {{Algorithms}}},
  author = {Cormen, T. and Leiserson, C. E. and Rivest, R. L.},
  year = {1990},
  publisher = {{Cambridge, MA}}
}

@inproceedings{cummings2019,
  title = {On the {{Compatibility}} of {{Privacy}} and {{Fairness}}},
  booktitle = {Adjunct {{Publication}} of the 27th {{Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}  - {{UMAP}}'19 {{Adjunct}}},
  author = {Cummings, Rachel and Gupta, Varun and Kimpara, Dhamma and Morgenstern, Jamie},
  year = {2019},
  pages = {309--315},
  publisher = {{ACM Press}},
  address = {{Larnaca, Cyprus}},
  doi = {10.1145/3314183.3323847},
  abstract = {In this work, we investigate whether privacy and fairness can be simultaneously achieved by a single classifier in several different models. Some of the earliest work on fairness in algorithm design defined fairness as a guarantee of similar outputs for ``similar'' input data, a notion with tight technical connections to differential privacy. We study whether tensions exist between differential privacy and statistical notions of fairness, namely Equality of False Positives and Equality of False Negatives (EFP/EFN). We show that even under full distributional access, there are cases where the constraint of differential privacy precludes exact EFP/EFN. We then turn to ask whether one can learn a differentially private classifier which approximately satisfies EFP/EFN, and show the existence of a PAC learner which is private and approximately fair with high probability. We conclude by giving an efficient algorithm for classification that maintains utility and satisfies both privacy and approximate fairness with high probability.},
  isbn = {978-1-4503-6711-0},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/FRXDT29W/Cummings et al. - 2019 - On the Compatibility of Privacy and Fairness.pdf}
}

@article{dai2015,
  title = {Spike and {{Slab Gaussian Process Latent Variable Models}}},
  author = {Dai, Zhenwen and Hensman, James and Lawrence, Neil},
  year = {2015},
  month = may,
  journal = {arXiv:1505.02434 [cs, stat]},
  eprint = {1505.02434},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Gaussian process latent variable model (GP-LVM) is a popular approach to non-linear probabilistic dimensionality reduction. One design choice for the model is the number of latent variables. We present a spike and slab prior for the GP-LVM and propose an efficient variational inference procedure that gives a lower bound of the log marginal likelihood. The new model provides a more principled approach for selecting latent dimensions than the standard way of thresholding the length-scale parameters. The effectiveness of our approach is demonstrated through experiments on real and simulated data. Further, we extend multi-view Gaussian processes that rely on sharing latent dimensions (known as manifold relevance determination) with spike and slab priors. This allows a more principled approach for selecting a subset of the latent space for each view of data. The extended model outperforms the previous state-of-the-art when applied to a cross-modal multimedia retrieval task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Dai_2015_Spike_and_Slab_Gaussian_Process_Latent_Variable_Models.pdf;/Users/cife/Zotero/storage/JIQSPQU6/1505.html}
}

@article{dai2016,
  title = {Variational {{Auto-encoded Deep Gaussian Processes}}},
  author = {Dai, Zhenwen and Damianou, Andreas and Gonz{\'a}lez, Javier and Lawrence, Neil},
  year = {2016},
  month = feb,
  journal = {arXiv:1511.06455 [cs, stat]},
  eprint = {1511.06455},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Dai_2016_Variational_Auto-encoded_Deep_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/D5ZCF2P4/1511.html}
}

@article{damianou,
  title = {Tutorial on {{Gaussian Processes}} and the {{Gaussian Process Latent Variable Model}}   (\& Discussion on the {{GPLVM}} Tech. Report by {{Prof}}. {{N}}. {{Lawrence}}, '06)},
  author = {Damianou, Andreas},
  pages = {35},
  langid = {english},
  file = {/Users/cife/Zotero/storage/VSJFYPW9/Damianou - Tutorial on Gaussian Processes and the Gaussian Pr.pdf}
}

@article{damianou2013,
  title = {Deep {{Gaussian Processes}}},
  author = {Damianou, Andreas and Lawrence, Neil},
  year = {2013},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief net- work based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent vari- able model (GP-LVM). We perform inference in the model by approximate variational marginal- ization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically ap- plied to relatively large data sets using stochas- tic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model se- lection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Damianou_2013_Deep_Gaussian_Processes.pdf}
}

@article{damianou2014,
  ids = {damianou2014b},
  title = {Variational {{Inference}} for {{Uncertainty}} on the {{Inputs}} of {{Gaussian Process Models}}},
  author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.2287 [cs, stat]},
  eprint = {1409.2287},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.},
  archiveprefix = {arXiv},
  keywords = {60G15 (Primary); 58E30,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.1.2,G.3,I.2.6,I.5.4,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Damianou_2014_Variational_Inference_for_Uncertainty_on_the_Inputs_of_Gaussian_Process_Models.pdf;/Users/cife/Dropbox/Zotero/Damianou_2014_Variational_Inference_for_Uncertainty_on_the_Inputs_of_Gaussian_Process_Models2.pdf;/Users/cife/Zotero/storage/P5M7M6UI/1409.html;/Users/cife/Zotero/storage/QC4MMUTF/1409.html}
}

@phdthesis{damianou2015,
  title = {Deep {{Gaussian Processes}}  and {{Variational Propagation}} of {{Uncertainty}}},
  author = {Damianou, Andreas},
  year = {2015},
  langid = {english},
  school = {University of Sheffield},
  keywords = {read},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/damianou2015.md;/Users/cife/Dropbox/Zotero/Damianou_2015_Deep_Gaussian_Processes_and_Variational_Propagation_of_Uncertainty.pdf}
}

@article{damianou2016,
  title = {Variational {{Inference}} for {{Latent Variables}} and {{Uncertain Inputs}} in {{Gaussian Processes}}},
  author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {42},
  pages = {1--62},
  issn = {1533-7928},
  abstract = {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.},
  file = {/Users/cife/Dropbox/Zotero/Damianou_2016_Variational_Inference_for_Latent_Variables_and_Uncertain_Inputs_in_Gaussian.pdf}
}

@article{daskalakis2021,
  title = {How {{Good}} Are {{Low-Rank Approximations}} in {{Gaussian Process Regression}}?},
  author = {Daskalakis, Constantinos and Dellaportas, Petros and Panos, Aristeidis},
  year = {2021},
  month = dec,
  journal = {arXiv:2004.01584 [cs, stat]},
  eprint = {2004.01584},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We provide guarantees for approximate Gaussian Process (GP) regression resulting from two common low-rank kernel approximations: based on random Fourier features, and based on truncating the kernel's Mercer expansion. In particular, we bound the Kullback-Leibler divergence between an exact GP and one resulting from one of the afore-described low-rank approximations to its kernel, as well as between their corresponding predictive densities, and we also bound the error between predictive mean vectors and between predictive covariance matrices computed using the exact versus using the approximate GP. We provide experiments on both simulated data and standard benchmarks to evaluate the effectiveness of our theoretical bounds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Daskalakis_2021_How_Good_are_Low-Rank_Approximations_in_Gaussian_Process_Regression.pdf;/Users/cife/Zotero/storage/WF53SSCD/2004.html}
}

@article{delfinado1995,
  title = {An Incremental Algorithm for {{Betti}} Numbers of Simplicial Complexes on the 3-Sphere},
  author = {Delfinado, Cecil Jose A. and Edelsbrunner, Herbert},
  year = {1995},
  journal = {Computer Aided Geometric Design},
  volume = {12},
  pages = {771--784}
}

@article{detlefsen2019,
  title = {Explicit {{Disentanglement}} of {{Appearance}} and {{Perspective}} in {{Generative Models}}},
  author = {Detlefsen, Nicki Skafte and Hauberg, S{\o}ren},
  year = {2019},
  month = nov,
  journal = {arXiv:1906.11881 [cs, stat]},
  eprint = {1906.11881},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Detlefsen_2019_Explicit_Disentanglement_of_Appearance_and_Perspective_in_Generative_Models.pdf;/Users/cife/Zotero/storage/U7AB3UIL/1906.html}
}

@article{detlefsen2021,
  title = {What Is a Meaningful Representation of Protein Sequences?},
  author = {Detlefsen, Nicki Skafte and Hauberg, S{\o}ren and Boomsma, Wouter},
  year = {2021},
  month = oct,
  journal = {arXiv:2012.02679 [cs, q-bio]},
  eprint = {2012.02679},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods},
  file = {/Users/cife/Dropbox/Zotero/Detlefsen_2021_What_is_a_meaningful_representation_of_protein_sequences.pdf;/Users/cife/Zotero/storage/NMP3US6A/Detlefsen et al. - 2021 - What is a meaningful representation of protein seq.pdf;/Users/cife/Zotero/storage/9LC5ETRN/2012.html}
}

@article{dey2016,
  title = {Machine {{Learning Algorithms}}: {{A Review}}},
  author = {Dey, Ayon},
  year = {2016},
  volume = {7},
  pages = {6},
  abstract = {In this paper, various machine learning algorithms have been discussed. These algorithms are used for various purposes like data mining, image processing, predictive analytics, etc. to name a few. The main advantage of using machine learning is that, once an algorithm learns what to do with data, it can do its work automatically.},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Dey_2016_Machine_Learning_Algorithms.pdf}
}

@article{dieleman2016,
  title = {Exploiting {{Cyclic Symmetry}} in {{Convolutional Neural Networks}}},
  author = {Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  year = {2016},
  month = may,
  journal = {arXiv:1602.02660 [cs]},
  eprint = {1602.02660},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/cife/Dropbox/Zotero/Dieleman_2016_Exploiting_Cyclic_Symmetry_in_Convolutional_Neural_Networks.pdf;/Users/cife/Zotero/storage/YQ2NKSYT/1602.html}
}

@article{dijkstra1959,
  title = {A Note on Two Problems in Connexion with Graphs},
  author = {Dijkstra, Edsger W},
  year = {1959},
  journal = {Numerische mathematik},
  volume = {1},
  number = {1},
  pages = {269--271},
  publisher = {{Springer}}
}

@book{doren2011,
  title = {How to {{Read}} a {{Book}}: {{The Classic Guide}} to {{Intelligent Reading}}},
  shorttitle = {How to {{Read}} a {{Book}}},
  author = {Doren, Charles Van and Adler, Mortimer J.},
  year = {2011},
  month = may,
  edition = {Rev Ed edition},
  publisher = {{Touchstone}},
  abstract = {With half a million copies in print, How to Read a Book is the best and most successful guide to reading comprehension for the general reader, completely rewritten and updated with new material. A CNN Book of the Week: ``Explains not just why we should read books, but how we should read them. It's masterfully done.'' \textendash Farheed ZakariaOriginally published in 1940, this book is a rare phenomenon, a living classic that introduces and elucidates the various levels of reading and how to achieve them\textemdash from elementary reading, through systematic skimming and inspectional reading, to speed reading. Readers will learn when and how to ``judge a book by its cover,'' and also how to X-ray it, read critically, and extract the author's message from the text. Also included is instruction in the different techniques that work best for reading particular genres, such as practical books, imaginative literature, plays, poetry, history, science and mathematics, philosophy and social science works. Finally, the authors offer a recommended reading list and supply reading tests you can use measure your own progress in reading skills, comprehension, and speed.},
  langid = {english},
  keywords = {book}
}

@book{doumont2009,
  title = {Trees, Maps, and Theorems: {{Effective Communication}} for {{Rational Minds}}},
  author = {Doumont, Jean-luc},
  year = {2009},
  publisher = {{Principi\ae, 2009}},
  isbn = {90-813677-0-6 978-90-813677-0-7},
  keywords = {read,read-someday},
  file = {/Users/cife/Zotero/storage/YH38L9Q6/book.html}
}

@book{duhigg2012,
  title = {The {{Power}} of {{Habit}}: {{Why We Do What We Do}}, and {{How}} to {{Change}}},
  shorttitle = {The {{Power}} of {{Habit}}},
  author = {Duhigg, Charles},
  year = {2012},
  month = apr,
  edition = {19 edition},
  publisher = {{Cornerstone Digital}},
  address = {{London}},
  abstract = {There's never been a better time to set new habits. This book will change your life.\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_Why do we do develop habits? And how can we change them?We can always change. In The Power of Habit, award-winning New York Times business reporter Charles Duhigg translates cutting-edge behavioural science into practical self-improvement action, distilling advanced neuroscience into fascinating narratives of transformation.Why can some people and companies change overnight, and some stay stuck in their old ruts? The answer lies deep in the human brain, and The Power of Habits reveals the secret pressure points that can change a life. From Olympic swimmer Michael Phelps to Martin Luther King Jr., from the CEO of Starbucks to the locker rooms of the NFL, Duhigg explores the incredible results of keystone habits, and how they can make all the difference between billions and millions, failure and success \textendash{} or even life and death.  The Power of Habit makes an exhilarating case: the key to almost any door in life is instilling the right habit. From exercise to weight loss, childrearing to productivity, market disruption to social revolution, and above all success, the right habits can change everything.  Habits aren't destiny. They're science, one which can transform our businesses, our communities, and our lives.\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_'Plenty of business books that try to tap into the scientific world manage to distil complicated research into readable prose. But few take the next step and become essential manuals for business and living. The Power of Habit is an exception.' ANDREW HILL, FINANCIAL TIMES},
  isbn = {978-1-84794-624-9},
  langid = {english},
  keywords = {book}
}

@article{dunson2019,
  title = {Diffusion Based {{Gaussian}} Process Regression via Heat Kernel Reconstruction},
  author = {Dunson, D. and Wu, Hau-Tieng and Wu, Nan},
  year = {2019},
  journal = {undefined},
  abstract = {We propose an algorithm for Gaussian Process regression on an unknown embedded manifold in a Euclidean space by using the heat kernel of the manifold as the covariance kernel in the Gaussian Process. It is not straightforward to define a valid covariance kernel for the Gaussian process regression when the predictors are on a manifold. Although the heat kernel provides a natural and canonical choice theoretically, it is analytically intractable to directly evaluate, and current Bayesian methods rely on computationally demanding Monte Carlo approximations. We instead develop a computationally efficient and accurate approximation to the heat kernel relying on the diffusion property of the graph Laplacian, leading to a diffusion-based Gaussian process (DB-GP) modeling this http URL the manifold setting, we provide a series of spectral convergence results quantifying how the eigenvectors and eigenvalues of the graph Laplacian converge to the eigenfunctions and eigenvalues of the Laplace-Beltrami operator in the \$L\^\textbackslash infty\$ sense.The convergence rate is also provided. Based on these results, convergence of the proposed heat kernel approximation algorithm, as well as the convergence rate, to the exact heat kernel is this http URL our knowledge, this is the first work providing a numerical heat kernel reconstruction from the point cloud with theoretical guarantees. We also discuss the performance when there are measurement errors in the predictors, so that they do not fall exactly on the embedded manifold. Simulations illustrate performance gains for the proposed approach over traditional approaches.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/I5H596K7/514428dfeb9d76475cc1e2ec57dac4e1a0f6a116.html}
}

@misc{dunton2022,
  title = {Fast {{Gaussian Process Posterior Mean Prediction}} via {{Local Cross Validation}} and {{Precomputation}}},
  author = {Dunton, Alec M. and Priest, Benjamin W. and Muyskens, Amanda},
  year = {2022},
  month = may,
  number = {arXiv:2205.10879},
  eprint = {2205.10879},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {Gaussian processes (GPs) are Bayesian non-parametric models useful in a myriad of applications. Despite their popularity, the cost of GP predictions (quadratic storage and cubic complexity with respect to the number of training points) remains a hurdle in applying GPs to large data. We present a fast posterior mean prediction algorithm called FastMuyGPs to address this shortcoming. FastMuyGPs is based upon the MuyGPs hyperparameter estimation algorithm and utilizes a combination of leave-one-out cross-validation, batching, nearest neighbors sparsification, and precomputation to provide scalable, fast GP prediction. We demonstrate several benchmarks wherein FastMuyGPs prediction attains superior accuracy and competitive or superior runtime to both deep neural networks and state-of-the-art scalable GP algorithms.},
  archiveprefix = {arXiv},
  keywords = {60G15,Computer Science - Machine Learning,G.3,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Dunton_2022_Fast_Gaussian_Process_Posterior_Mean_Prediction_via_Local_Cross_Validation_and.pdf;/Users/cife/Zotero/storage/7SRILS8G/2205.html}
}

@inproceedings{dutordoir2020,
  title = {Sparse {{Gaussian Processes}} with {{Spherical Harmonic Features}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dutordoir, Vincent and Durrande, Nicolas and Hensman, James},
  year = {2020},
  month = nov,
  pages = {2793--2802},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We introduce a new class of inter-domain variational Gaussian processes (GP) where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. Our inference scheme is comparable to variational Fourier features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. Our experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy. We also demonstrate competitive performance on classification with non-conjugate likelihoods.},
  langid = {english},
  keywords = {read,sasgp},
  file = {/Users/cife/Dropbox/Zotero/Dutordoir_2020_Sparse_Gaussian_Processes_with_Spherical_Harmonic_Features.pdf;/Users/cife/Zotero/storage/ZI44BLZD/Dutordoir et al. - 2020 - Sparse Gaussian Processes with Spherical Harmonic .pdf}
}

@inproceedings{dutordoir2021,
  title = {Deep {{Neural Networks}} as {{Point Estimates}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dutordoir, Vincent and Hensman, James and {van der Wilk}, Mark and Ek, Carl Henrik and Ghahramani, Zoubin and Durrande, Nicolas},
  year = {2021},
  volume = {34},
  pages = {9443--9455},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Neural networks and Gaussian processes are complementary in their strengths and weaknesses. Having a better understanding of their relationship comes with the promise to make each method benefit from the strengths of the other. In this work, we establish an equivalence between the forward passes of neural networks and (deep) sparse Gaussian process models. The theory we develop is based on interpreting activation functions as interdomain inducing features through a rigorous analysis of the interplay between activation functions and kernels. This results in models that can either be seen as neural networks with improved uncertainty prediction or deep Gaussian processes with increased prediction accuracy. These claims are supported by experimental results on regression and classification datasets.},
  file = {/Users/cife/Dropbox/Zotero/Dutordoir_2021_Deep_Neural_Networks_as_Point_Estimates_for_Deep_Gaussian_Processes.pdf}
}

@misc{dutordoir2022,
  title = {Neural {{Diffusion Processes}}},
  author = {Dutordoir, Vincent and Saul, Alan and Ghahramani, Zoubin and Simpson, Fergus},
  year = {2022},
  month = jun,
  number = {arXiv:2206.03992},
  eprint = {2206.03992},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Gaussian processes provide an elegant framework for specifying prior and posterior distributions over functions. They are, however, also computationally expensive, and limited by the expressivity of their covariance function. We propose Neural Diffusion Processes (NDPs), a novel approach based upon diffusion models, that learn to sample from distributions over functions. Using a novel attention block, we can incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs are able to capture functional distributions that are close to the true Bayesian posterior of a Gaussian process. This enables a variety of downstream tasks, including hyperparameter marginalisation and Bayesian optimisation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Dutordoir_2022_Neural_Diffusion_Processes.pdf;/Users/cife/Zotero/storage/TG3CU7MK/2206.html}
}

@inproceedings{duvenaud2013,
  title = {Structure {{Discovery}} in {{Nonparametric Regression}} through {{Compositional Kernel Search}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},
  year = {2013},
  month = may,
  pages = {1166--1174},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Duvenaud_2013_Structure_Discovery_in_Nonparametric_Regression_through_Compositional_Kernel.pdf}
}

@phdthesis{duvenaud2014,
  title = {Automatic {{Model Construction}}  with {{Gaussian Processes}}},
  author = {Duvenaud, David Kristjanson},
  year = {2014},
  langid = {english},
  school = {University of Cambridge},
  keywords = {kernel,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Duvenaud_2014_Automatic_Model_Construction_with_Gaussian_Processes.pdf}
}

@article{dwork2011,
  title = {Fairness {{Through Awareness}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Rich},
  year = {2011},
  month = nov,
  journal = {arXiv:1104.3913 [cs]},
  eprint = {1104.3913},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computers and Society,fairness,read},
  file = {/Users/cife/Dropbox/Zotero/Dwork_2011_Fairness_Through_Awareness.pdf;/Users/cife/Zotero/storage/7UBCQIY2/1104.html}
}

@misc{dwork2020,
  title = {Individual {{Fairness}} in {{Pipelines}}},
  author = {Dwork, Cynthia and Ilvento, Christina and Jagadeesan, Meena},
  year = {2020},
  month = apr,
  number = {arXiv:2004.05167},
  eprint = {2004.05167},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.05167},
  abstract = {It is well understood that a system built from individually fair components may not itself be individually fair. In this work, we investigate individual fairness under pipeline composition. Pipelines differ from ordinary sequential or repeated composition in that individuals may drop out at any stage, and classification in subsequent stages may depend on the remaining "cohort" of individuals. As an example, a company might hire a team for a new project and at a later point promote the highest performer on the team. Unlike other repeated classification settings, where the degree of unfairness degrades gracefully over multiple fair steps, the degree of unfairness in pipelines can be arbitrary, even in a pipeline with just two stages. Guided by a panoply of real-world examples, we provide a rigorous framework for evaluating different types of fairness guarantees for pipelines. We show that na\textbackslash "\{i\}ve auditing is unable to uncover systematic unfairness and that, in order to ensure fairness, some form of dependence must exist between the design of algorithms at different stages in the pipeline. Finally, we provide constructions that permit flexibility at later stages, meaning that there is no need to lock in the entire pipeline at the time that the early stage is constructed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Dwork_2020_Individual_Fairness_in_Pipelines.pdf;/Users/cife/Zotero/storage/7G7IHMSU/2004.html}
}

@book{eaton2007,
  title = {Chapter 8: {{The Wishart Distribution}}},
  shorttitle = {Chapter 8},
  author = {Eaton, Morris L.},
  year = {2007},
  journal = {Multivariate Statistics},
  pages = {302--333},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/lnms/1196285114},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  keywords = {reference},
  file = {/Users/cife/Dropbox/Zotero/Eaton_2007_Chapter_8.pdf;/Users/cife/Zotero/storage/7DZCDCC6/1196285114.html}
}

@inproceedings{edelsbrunner2000,
  title = {Topological Persistence and Simplification},
  booktitle = {Proceedings 41st Annual Symposium on Foundations of Computer Science},
  author = {Edelsbrunner, Herbert and Letscher, David and Zomorodian, Afra},
  year = {2000},
  pages = {454--463}
}

@article{edwards2017,
  title = {Towards a {{Neural Statistician}}},
  author = {Edwards, Harrison and Storkey, Amos},
  year = {2017},
  month = mar,
  journal = {arXiv:1606.02185 [cs, stat]},
  eprint = {1606.02185},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@phdthesis{ek2009,
  ids = {ek},
  title = {Shared {{Gaussian Process Latent Variables Models}}},
  author = {Ek, Carl Henrik},
  year = {2009},
  abstract = {A fundamental task is machine learning is modeling the relationship between dif- ferent observation spaces. Dimensionality reduction is the task reducing the num- ber of dimensions in a parameterization of a data-set. In this thesis we are inter- ested in the cross-road between these two tasks: shared dimensionality reduction. Shared dimensionality reduction aims to represent multiple observation spaces within the same model. Previously suggested models have been limited to the scenarios where the observations have been generated from the same manifold. In this paper we present a Gaussian process Latent Variable Model (GP-LVM) [33] for shared dimensionality reduction without making assumptions about the relationship between the observations. Further we suggest an extension to Canon- ical Correlation Analysis (CCA) called Non Consolidating Component Analy- sis (NCCA). The proposed algorithm extends classical CCA to represent the full variance of the data opposed to only the correlated. We compare the suggested GP-LVM model to existing models and show results on real-world problems ex- emplifying the advantages of our approach.},
  langid = {english},
  school = {Oxford Brookes University},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Ek_2009_Shared_Gaussian_Process_Latent_Variables_Models.pdf;/Users/cife/Dropbox/Zotero/Ek_2009_Shared_Gaussian_Process_Latent_Variables_Models2.pdf}
}

@misc{ek2018,
  title = {Unsupervised {{Learning}} with {{Gaussian Processes}}},
  author = {Ek, Carl Henrik},
  year = {2018},
  abstract = {Unsupervised Learning with GPs Carl Henrik Ek University of Bristol http://gpss.cc/gpss17/slides/presenta... Tuesday 11am (note: audio starts 2:45 in) For the full programme and notebooks: http://gpss.cc/gpss17},
  keywords = {read}
}

@article{eklund2019,
  title = {Expected Path Length on Random Manifolds},
  author = {Eklund, David and Hauberg, S{\o}ren},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.07377 [cs, stat]},
  eprint = {1908.07377},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Manifold learning seeks a low dimensional representation that faithfully captures the essence of data. Current methods can successfully learn such representations, but do not provide a meaningful set of operations that are associated with the representation. Working towards operational representation learning, we endow the latent space of a large class of generative models with a random Riemannian metric, which provides us with elementary operators. As computational tools are unavailable for random Riemannian manifolds, we study deterministic approximations and derive tight error bounds on expected distances.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Zotero/storage/8R8TIC8B/Eklund and Hauberg - 2019 - Expected path length on random manifolds.pdf}
}

@inproceedings{eleftheriadis2016,
  title = {Variational {{Gaussian Process Auto-Encoder}} for {{Ordinal Prediction}} of {{Facial Action Units}}},
  author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc and Pantic, Maja},
  year = {2016},
  month = nov,
  doi = {10.1007/978-3-319-54184-6_10},
  abstract = {We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process (GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art.},
  isbn = {978-3-319-54183-9},
  file = {/Users/cife/Dropbox/Zotero/Eleftheriadis_2016_Variational_Gaussian_Process_Auto-Encoder_for_Ordinal_Prediction_of_Facial.pdf}
}

@article{engebretsen,
  title = {Data {{Visualization}} in {{Society}}},
  author = {Engebretsen, Martin and Kennedy, Helen},
  pages = {466},
  langid = {english},
  keywords = {grey literature,read-maybe-never},
  file = {/Users/cife/Zotero/storage/3HK2L2GJ/Engebretsen and Kennedy - Data Visualization in Society.pdf}
}

@article{errica2020,
  title = {Step-{{By-Step Derivation}} of {{SNE}} and t-{{SNE}} Gradients},
  author = {Errica, Federico},
  year = {2020},
  pages = {3},
  langid = {english},
  keywords = {read-maybe-never,tsne},
  file = {/Users/cife/Dropbox/Zotero/Errica_2020_Step-By-Step_Derivation_of_SNE_and_t-SNE_gradients.pdf}
}

@article{esteva2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  year = {2017},
  month = feb,
  journal = {Nature},
  volume = {542},
  number = {7639},
  pages = {115--118},
  issn = {1476-4687},
  doi = {10.1038/nature21056},
  abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
  langid = {english},
  pmcid = {PMC8382232},
  pmid = {28117445},
  keywords = {Automation,Cell Phone,Datasets as Topic,Dermatologists,Humans,Keratinocytes,Keratosis; Seborrheic,Melanoma,Neural Networks; Computer,Nevus,Photography,Reproducibility of Results,Skin Neoplasms},
  file = {/Users/cife/Dropbox/Zotero/Esteva_2017_Dermatologist-level_classification_of_skin_cancer_with_deep_neural_networks.pdf}
}

@article{falorsi2019,
  title = {Reparameterizing {{Distributions}} on {{Lie Groups}}},
  author = {Falorsi, Luca and {de Haan}, Pim and Davidson, Tim R. and Forr{\'e}, Patrick},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.02958 [cs, math, stat]},
  eprint = {1903.02958},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Reparameterizable densities are an important way to learn probability distributions in a deep learning setting. For many distributions it is possible to create low-variance gradient estimators by utilizing a `reparameterization trick'. Due to the absence of a general reparameterization trick, much research has recently been devoted to extend the number of reparameterizable distributional families. Unfortunately, this research has primarily focused on distributions defined in Euclidean space, ruling out the usage of one of the most influential class of spaces with non-trivial topologies: Lie groups. In this work we define a general framework to create reparameterizable densities on arbitrary Lie groups, and provide a detailed practitioners guide to further the ease of usage. We demonstrate how to create complex and multimodal distributions on the well known oriented group of 3D rotations, \$\textbackslash operatorname\{SO\}(3)\$, using normalizing flows. Our experiments on applying such distributions in a Bayesian setting for pose estimation on objects with discrete and continuous symmetries, showcase their necessity in achieving realistic uncertainty estimates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Representation Theory,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Falorsi_2019_Reparameterizing_Distributions_on_Lie_Groups.pdf;/Users/cife/Zotero/storage/WPK5SEZA/1903.html}
}

@article{fazackerley2020,
  title = {Women's Research Plummets during Lockdown - but Articles from Men Increase},
  author = {Fazackerley, Anna},
  year = {2020},
  month = may,
  journal = {The Guardian},
  issn = {0261-3077},
  abstract = {Many female academics say juggling their career with coronavirus childcare is overwhelming},
  chapter = {Education},
  langid = {british},
  keywords = {article,Coronavirus outbreak,Education,Family,grey literature,Higher education,Lecturers,Life and style,Research,Research funding,Research publishing,The gender gap,UK news,University administration},
  file = {/Users/cife/Zotero/storage/AJBTJJND/womens-research-plummets-during-lockdown-but-articles-from-men-increase.html}
}

@article{fefferman2016,
  title = {Testing the Manifold Hypothesis},
  author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  year = {2016},
  month = feb,
  journal = {Journal of the American Mathematical Society},
  volume = {29},
  number = {4},
  pages = {983--1049},
  issn = {0894-0347, 1088-6834},
  doi = {10.1090/jams/852},
  langid = {english},
  file = {/Users/cife/Zotero/storage/T5KFFEGI/Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf}
}

@misc{fehiepsi2019,
  title = {Inferences for {{Deep Gaussian Process}} Models in {{Pyro}}},
  author = {{fehiepsi}},
  year = {2019},
  month = may,
  journal = {fehiepsi's blog},
  abstract = {In this tutorial, I want to illustrate how to use Pyro's Gaussian Processes module to create and train some deep Gaussian Process models. For the background on how to use this module, readers can chec},
  howpublished = {https://fehiepsi.github.io/blog/deep-gaussian-process/},
  langid = {english},
  file = {/Users/cife/Zotero/storage/QBPI5HYE/deep-gaussian-process.html}
}

@article{feldager2017,
  title = {Deterministic Extinction by Mixing in Cyclically Competing Species},
  author = {Feldager, Cilie W. and Mitarai, Namiko and Ohta, Hiroki},
  year = {2017},
  month = mar,
  journal = {Physical Review E},
  volume = {95},
  number = {3},
  pages = {032318},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.95.032318},
  abstract = {We consider a cyclically competing species model on a ring with global mixing at finite rate, which corresponds to the well-known Lotka-Volterra equation in the limit of infinite mixing rate. Within a perturbation analysis of the model from the infinite mixing rate, we provide analytical evidence that extinction occurs deterministically at sufficiently large but finite values of the mixing rate for any species number N{$\geq$}3. Further, by focusing on the cases of rather small species numbers, we discuss numerical results concerning the trajectories toward such deterministic extinction, including global bifurcations caused by changing the mixing rate.},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Feldager_2017_Deterministic_extinction_by_mixing_in_cyclically_competing_species.pdf;/Users/cife/Zotero/storage/9DV9SS2Z/PhysRevE.95.html}
}

@misc{feldager2020b,
  title = {Unfairness from Sampling Broken Model},
  author = {Feldager, Cilie W},
  year = {2020},
  month = may,
  keywords = {in-progress,note},
  file = {/Users/cife/Zotero/storage/GTVC9SPI/learning-card.pdf;/Users/cife/Zotero/storage/HIRZFHK3/test-card.pdf}
}

@misc{feldager2020c,
  title = {Recovering Manifold in {{GPLVM}} Flow Model},
  author = {Feldager, Cilie W},
  year = {2020},
  abstract = {Recovering the Riemannian manifold used in a Riemannian flow with manifold learning.},
  keywords = {not-started,note},
  file = {/Users/cife/Zotero/storage/3NY94WFP/test-card.pdf}
}

@misc{feldager2020d,
  title = {Symmetry Breaking in {{CNN}}},
  author = {Feldager, Cilie W},
  year = {2020},
  month = may,
  abstract = {Looking for symmetry breaking the second last layer of DNN models. The output of the model is 512-dimensional for each angle of rotation in COIL and so, I can just look at distances in this 512-dimensional space using my MAD estimator.},
  keywords = {in-progress,note},
  file = {/Users/cife/Zotero/storage/RM65PD2M/test-card.pdf}
}

@inproceedings{feldager2021,
  title = {Spontaneous {{Symmetry Breaking}} in {{Data Visualization}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}}},
  author = {Feldager, Cilie W and Hauberg, S{\o}ren and Hansen, Lars Kai},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})"},
  pages = {435--446},
  publisher = {{Springer}},
  doi = {10.1007/978-3-030-86340-1_35},
  abstract = {ata visualization tools should create low-dimensional representations of data that emphasize structure and suppress noise. However, such non-linear amplifications of structural differences can have side effects like spurious clustering in t-SNE [1]. We present a more general class of spurious structure, namely broken symmetry, defined as visualizations that lack symmetry present in the underlying data. We develop a simple workflow for detection of broken symmetry and give examples of spontaneous symmetry breaking in t-SNE and other well-known algorithms such as GPLVM and kPCA. Our extensive, quantitative study shows that these algorithms frequently break symmetry, thereby highlighting new shortcomings of current visualization tools.},
  isbn = {978-3-030-86339-5}
}

@article{fellenius,
  title = {Guide for {{Writing}} a {{Thesis}}},
  author = {Fellenius, Bengt H},
  pages = {41},
  langid = {english},
  file = {/Users/cife/Zotero/storage/A292RY82/Fellenius - Guide for Writing a Thesis.pdf}
}

@inproceedings{feragen2015,
  title = {Geodesic Exponential Kernels: {{When}} Curvature and Linearity Conflict},
  shorttitle = {Geodesic Exponential Kernels},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Feragen, Aasa and Lauze, Francois and Hauberg, Soren},
  year = {2015},
  month = jun,
  pages = {3032--3042},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298922},
  abstract = {We consider kernel methods on general geodesic metric spaces and provide both negative and positive results. First we show that the common Gaussian kernel can only be generalized to a positive definite kernel on a geodesic metric space if the space is flat. As a result, for data on a Riemannian manifold, the geodesic Gaussian kernel is only positive definite if the Riemannian manifold is Euclidean. This implies that any attempt to design geodesic Gaussian kernels on curved Riemannian manifolds is futile. However, we show that for spaces with conditionally negative definite distances the geodesic Laplacian kernel can be generalized while retaining positive definiteness. This implies that geodesic Laplacian kernels can be generalized to some curved spaces, including spheres and hyperbolic spaces. Our theoretical results are verified empirically.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/feragen2015.md;/Users/cife/Dropbox/Zotero/Feragen_2015_Geodesic_exponential_kernels.pdf}
}

@article{flennerhag2019,
  title = {Transferring {{Knowledge}} across {{Learning Processes}}},
  author = {Flennerhag, Sebastian and Moreno, Pablo G. and Lawrence, Neil D. and Damianou, Andreas},
  year = {2019},
  month = mar,
  journal = {arXiv:1812.01054 [cs, stat]},
  eprint = {1812.01054},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Flennerhag_2019_Transferring_Knowledge_across_Learning_Processes.pdf;/Users/cife/Zotero/storage/DVVL45K8/1812.html}
}

@article{fong2020,
  title = {On the Marginal Likelihood and Cross-Validation},
  author = {Fong, E and Holmes, C C},
  year = {2020},
  month = jun,
  journal = {Biometrika},
  volume = {107},
  number = {2},
  pages = {489--496},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz077},
  abstract = {In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate model fit as it quantifies the joint probability of the data under the prior. In contrast, non-Bayesian models are typically compared using cross-validation on held-out data, either through \$k\$-fold partitioning or leave-\$p\$-out subsampling. We show that the marginal likelihood is formally equivalent to exhaustive leave-\$p\$-out crossvalidation averaged over all values of \$p\$ and all held-out test sets when using the log posterior predictive probability as the scoring rule. Moreover, the log posterior predictive score is the only coherent scoring rule under data exchangeability. This offers new insight into the marginal likelihood and cross-validation, and highlights the potential sensitivity of the marginal likelihood to the choice of the prior. We suggest an alternative approach using cumulative cross-validation following a preparatory training phase. Our work has connections to prequential analysis and intrinsic Bayes factors, but is motivated in a different way.},
  file = {/Users/cife/Dropbox/Zotero/Fong_2019_On_the_marginal_likelihood_and_cross-validation.pdf;/Users/cife/Dropbox/Zotero/Fong_2020_On_the_marginal_likelihood_and_cross-validation.pdf;/Users/cife/Zotero/storage/3IZZFESC/asz077_supplementary_data.pdf;/Users/cife/Zotero/storage/LKM4ZHB9/5715611.html}
}

@article{fortuin2022,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}}},
  shorttitle = {Priors in {{Bayesian Deep Learning}}},
  author = {Fortuin, Vincent},
  year = {2022},
  month = mar,
  journal = {arXiv:2105.06868 [cs, stat]},
  eprint = {2105.06868},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Fortuin_2022_Priors_in_Bayesian_Deep_Learning.pdf;/Users/cife/Zotero/storage/YSNF8L7H/2105.html}
}

@article{fortuin2022a,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}} - {{Fortuin}} - - {{International Statistical Review}} - {{Wiley Online Library}}},
  author = {Fortuin, Vincent},
  year = {2022},
  journal = {International Statistical Review},
  doi = {10.1111/insr.12502},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow,recent Bayesian deep learning models have often fallen back on vague priors, such as standardGaussians. In this review, we highlight the importance of prior choices for Bayesian deep learningand present an overview of different priors that have been proposed for (deep) Gaussian processes,variational autoencoders and Bayesian neural networks. We also outline different methods of learn-ing priors for these models from data. We hope to motivate practitioners in Bayesian deep learningto think more carefully about the prior specification for their models and to provide them with someinspiration in this regard},
  keywords = {Bayesian deep learning,Bayesian learning,Deep learning,Priors},
  file = {/Users/cife/Dropbox/Zotero/Vincent_2022_Priors_in_Bayesian_Deep_Learning.pdf}
}

@article{frazier2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.02811 [cs, math, stat]},
  eprint = {1807.02811},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/cife/Zotero/storage/M85SAGXN/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf;/Users/cife/Zotero/storage/ZQ3JCTZM/1807.html}
}

@article{freifeld2017,
  title = {Transformations {{Based}} on {{Continuous Piecewise-Affine Velocity Fields}}},
  author = {Freifeld, Oren and Hauberg, Soren and Batmanghelich, Kayhan and Fisher, Jonn W.},
  year = {2017},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {12},
  pages = {2496--2509},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2016.2646685},
  abstract = {We propose novel finite-dimensional spaces of well-behaved Rn ! Rn transformations. The latter are obtained by (fast and highly-accurate) integration of continuous piecewise-affine velocity fields. The proposed method is simple yet highly expressive, effortlessly handles optional constraints (e.g., volume preservation and/or boundary conditions), and supports convenient modeling choices such as smoothing priors and coarse-to-fine analysis. Importantly, the proposed approach, partly due to its rapid likelihood evaluations and partly due to its other properties, facilitates tractable inference over rich transformation spaces, including using Markov-Chain Monte-Carlo methods. Its applications include, but are not limited to: monotonic regression (more generally, optimization over monotonic functions); modeling cumulative distribution functions or histograms; time-warping; image warping; image registration; real-time diffeomorphic image editing; data augmentation for image classifiers. Our GPU-based code is publicly available.},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Zotero/storage/I7F93EMA/Freifeld et al. - 2017 - Transformations Based on Continuous Piecewise-Affi.pdf}
}

@article{frid-adar2018,
  title = {{{GAN-based}} Synthetic Medical Image Augmentation for Increased {{CNN}} Performance in Liver Lesion Classification},
  author = {{Frid-Adar}, Maayan and Diamant, Idit and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  year = {2018},
  month = dec,
  journal = {Neurocomputing},
  volume = {321},
  pages = {321--331},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.09.013},
  abstract = {Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6\% sensitivity and 88.4\% specificity. By adding the synthetic data augmentation the results increased to 85.7\% sensitivity and 92.4\% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists' efforts to improve diagnosis.},
  langid = {english},
  keywords = {Convolutional neural networks,Data augmentation,Deep learning,Generative adversarial network,Image synthesis,Lesion classification,Liver lesions},
  file = {/Users/cife/Dropbox/Zotero/Frid-Adar_2018_GAN-based_synthetic_medical_image_augmentation_for_increased_CNN_performance_in.pdf;/Users/cife/Zotero/storage/JI8PFPLP/S0925231218310749.html}
}

@inproceedings{frome2007,
  title = {Learning {{Globally-Consistent Local Distance Functions}} for {{Shape-Based Image Retrieval}} and {{Classification}}},
  booktitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  author = {Frome, Andrea and Singer, Yoram and Sha, Fei and Malik, Jitendra},
  year = {2007},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro, Brazil}},
  doi = {10.1109/ICCV.2007.4408839},
  isbn = {978-1-4244-1630-1},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Frome_2007_Learning_Globally-Consistent_Local_Distance_Functions_for_Shape-Based_Image.pdf;/Users/cife/Dropbox/Zotero/Frome_2007_Learning_Globally-Consistent_Local_Distance_Functions_for_Shape-Based_Image2.pdf}
}

@article{frye2012,
  title = {Spherical {{Harmonics}} in p {{Dimensions}}},
  author = {Frye, Christopher and Efthimiou, Costas J.},
  year = {2012},
  month = may,
  journal = {arXiv:1205.3548 [hep-th, physics:math-ph]},
  eprint = {1205.3548},
  eprinttype = {arxiv},
  primaryclass = {hep-th, physics:math-ph},
  abstract = {The authors prepared this booklet in order to make several useful topics from the theory of special functions, in particular the spherical harmonics and Legendre polynomials for any dimension, available to undergraduates studying physics or mathematics. With this audience in mind, nearly all details of the calculations and proofs are written out, and extensive background material is covered before beginning the main subject matter. The reader is assumed to have knowledge of multivariable calculus and linear algebra as well as some level of comfort with reading proofs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Theory,Mathematical Physics,Mathematics - Analysis of PDEs,Mathematics - Classical Analysis and ODEs,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Frye_2012_Spherical_Harmonics_in_p_Dimensions.pdf}
}

@misc{fuchs,
  title = {Fabian {{Fuchs}}},
  author = {Fuchs, Fabian},
  abstract = {\# Noether's Theorem, Symmetries, and Invariant Neural Networks \_\_\_ When I tell physicists that I am working on invariant and equivariant neural network architectures which exploit symmetries, they often mention Noether's theorem. In this post, I am trying to answer the question of how the two topics are connected. I...},
  howpublished = {https://fabianfuchsml.github.io/noether/},
  langid = {english},
  file = {/Users/cife/Zotero/storage/BAG63IGY/noether.html}
}

@article{fukushima1975,
  title = {Cognitron: {{A}} Self-Organizing Multilayered Neural Network},
  shorttitle = {Cognitron},
  author = {Fukushima, Kunihiko},
  year = {1975},
  month = sep,
  journal = {Biological Cybernetics},
  volume = {20},
  number = {3},
  pages = {121--136},
  issn = {1432-0770},
  doi = {10.1007/BF00342633},
  abstract = {A new hypothesis for the organization of synapses between neurons is proposed: ``The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y''. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named ``cognitron'', is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a ``teacher'' which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
  langid = {english},
  keywords = {Deep Layer,Final Layer,Individual Cell,Neural Network,Receptive Field}
}

@article{gadd2018,
  title = {Pseudo-Marginal {{Bayesian}} Inference for Supervised {{Gaussian}} Process Latent Variable Models},
  author = {Gadd, Charles and Wade, Sara and Shah, Akeel and Grammatopoulos, Dimitris},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.10746 [cs, stat]},
  eprint = {1803.10746},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a Bayesian framework for inference with a supervised version of the Gaussian process latent variable model. The framework overcomes the high correlations between latent variables and hyperparameters by using an unbiased pseudo estimate for the marginal likelihood that approximately integrates over the latent variables. This is used to construct a Markov Chain to explore the posterior of the hyperparameters. We demonstrate the procedure on simulated and real examples, showing its ability to capture uncertainty and multimodality of the hyperparameters and improved uncertainty quantification in predictions when compared with variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Gadd_2018_Pseudo-marginal_Bayesian_inference_for_supervised_Gaussian_process_latent.pdf;/Users/cife/Zotero/storage/9FAJVQE2/1803.html}
}

@article{gal2014,
  title = {Variational {{Inference}} in {{Sparse Gaussian Process Regression}} and {{Latent Variable Models}} - a {{Gentle Tutorial}}},
  author = {Gal, Yarin and {van der Wilk}, Mark},
  year = {2014},
  month = sep,
  journal = {arXiv:1402.1412 [stat]},
  eprint = {1402.1412},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \& Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read-someday,Statistics - Machine Learning,thesis},
  file = {/Users/cife/Dropbox/Zotero/Gal_2014_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable.pdf}
}

@article{gal2015,
  title = {Latent {{Gaussian Processes}} for {{Distribution Estimation}} of {{Multivariate Categorical Data}}},
  author = {Gal, Yarin and Chen, Yutian and Ghahramani, Zoubin},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.02182 [stat]},
  eprint = {1503.02182},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Gal_2015_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate.pdf;/Users/cife/Zotero/storage/N9KUYNLX/1503.html}
}

@article{gal2022,
  title = {Bayesian Uncertainty Quantification for Machine-Learned Models in Physics},
  author = {Gal, Yarin and Koumoutsakos, Petros and Lanusse, Francois and Louppe, Gilles and Papadimitriou, Costas},
  year = {2022},
  month = sep,
  journal = {Nature Reviews Physics},
  volume = {4},
  number = {9},
  pages = {573--577},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-022-00498-4},
  abstract = {Being able to quantify uncertainty when comparing a theoretical or computational model to observations is critical to conducting a sound scientific investigation. With the rise of data-driven modelling, understanding various sources of uncertainty and developing methods to estimate them has gained renewed attention. Five researchers discuss uncertainty quantification in machine-learned models with an emphasis on issues relevant to physics problems.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Physics},
  file = {/Users/cife/Zotero/storage/ESFH8GSU/s42254-022-00498-4.html}
}

@book{gallot2004,
  ids = {gallotRiemannianGeometry2004a},
  title = {Riemannian Geometry},
  author = {Gallot, S. and Hulin, D. and Lafontaine, J.},
  year = {2004},
  series = {Universitext},
  edition = {3rd ed},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-20493-0},
  langid = {english},
  lccn = {QA649 .G35 2004},
  keywords = {Geometry; Riemannian,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Gallot_2004_Riemannian_geometry.pdf;/Users/cife/Dropbox/Zotero/Gallot_2004_Riemannian_geometry2.pdf}
}

@article{gangstad2013,
  title = {Noisy Transcription Factor {{NF-}}\textquestiondown{{B}} Oscillations Stabilize and Sensitize Cytokine Signaling in Space},
  author = {Gangstad, S. W.},
  year = {2013},
  journal = {Physical Review E (statistical, Nonlinear, and Soft Matter Physics)},
  volume = {87},
  number = {2},
  issn = {24700045},
  doi = {10.1103/PhysRevE.87.022702},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Gangstad_2013_Noisy_transcription_factor_NF-¿B_oscillations_stabilize_and_sensitize_cytokine.pdf;/Users/cife/Zotero/storage/J7P7FC3N/2532029350.html}
}

@inproceedings{gardner2018,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  shorttitle = {{{GPyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n\^3) to O(n\^2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  file = {/Users/cife/Dropbox/Zotero/Gardner_2018_GPyTorch.pdf}
}

@article{gatys2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2015},
  month = sep,
  journal = {arXiv:1508.06576 [cs, q-bio]},
  eprint = {1508.06576},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Gatys_2015_A_Neural_Algorithm_of_Artistic_Style.pdf}
}

@article{gauss1827,
  title = {Disquisitiones Generales circa Superficies Curvas, Commentationes Societatis Regiae Scientiarum Gottingesis Recentiores, Vol},
  author = {Gauss, CF},
  year = {1827},
  journal = {VI, G\"ottingen},
  pages = {99--146}
}

@article{gelman,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models},
  author = {Gelman, Andrew},
  pages = {19},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of ``noninformative'' prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/F6EZMACW/Gelman - Prior distributions for variance parameters in hie.pdf}
}

@misc{gelman2016,
  title = {Crimes against Data, {{Professor Andrew Gelman}}},
  author = {Gelman, Andrew},
  year = {2016},
  month = sep,
  keywords = {philosophy,read-someday}
}

@article{gelman2017,
  title = {The {{Prior Can Often Only Be Understood}} in the {{Context}} of the {{Likelihood}}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  year = {2017},
  month = oct,
  journal = {Entropy},
  volume = {19},
  number = {10},
  pages = {555},
  issn = {1099-4300},
  doi = {10.3390/e19100555},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/gelman2017.md;/Users/cife/Zotero/storage/ZSLPRRWG/Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf}
}

@article{gelman2020,
  title = {Holes in {{Bayesian Statistics}}},
  author = {Gelman, Andrew and Yao, Yuling},
  year = {2020},
  pages = {11},
  abstract = {Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayes factors fail in the presence of flat or weak priors, (5) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference.},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Gelman_2020_Holes_in_Bayesian_Statistics.pdf}
}

@misc{gelman2020a,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Gelman_2020_Bayesian_workflow.pdf}
}

@article{gemici2016,
  title = {Normalizing {{Flows}} on {{Riemannian Manifolds}}},
  author = {Gemici, Mevlana C. and Rezende, Danilo and Mohamed, Shakir},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.02304 [cs, math, stat]},
  eprint = {1611.02304},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces \$\textbackslash mathbf\{R\}\^n\$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere \$\textbackslash mathbf\{S\}\^n\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Gemici_2016_Normalizing_Flows_on_Riemannian_Manifolds.pdf;/Users/cife/Zotero/storage/SK4K2Y4K/1611.html}
}

@article{ghrist2008,
  title = {Barcodes: {{The}} Persistent Topology of Data},
  shorttitle = {Barcodes},
  author = {Ghrist, Robert},
  year = {2008},
  journal = {Bulletin of the American Mathematical Society},
  volume = {45},
  number = {1},
  pages = {61--75},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0273-0979-07-01191-3},
  abstract = {This article surveys recent work of Carlsson and collaborators on applications of computational algebraic topology to problems of feature detection and shape recognition in high-dimensional data. The primary mathematical tool considered is a homology theory for point-cloud data sets--persistent homology--and a novel representation of this algebraic characterization--barcodes. We sketch an application of these techniques to the classification of natural images.},
  langid = {english},
  keywords = {read,sb},
  file = {/Users/cife/Dropbox/Zotero/Ghrist_2008_Barcodes.pdf;/Users/cife/Zotero/storage/57QPW8WC/S0273-0979-07-01191-3.html}
}

@misc{giordano2022,
  title = {On the Inability of {{Gaussian}} Process Regression to Optimally Learn Compositional Functions},
  author = {Giordano, Matteo and Ray, Kolyan and {Schmidt-Hieber}, Johannes},
  year = {2022},
  month = may,
  number = {arXiv:2205.07764},
  eprint = {2205.07764},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {We rigorously prove that deep Gaussian process priors can outperform Gaussian process priors if the target function has a compositional structure. To this end, we study information-theoretic lower bounds for posterior contraction rates for Gaussian process regression in a continuous regression model. We show that if the true function is a generalized additive function, then the posterior based on any mean-zero Gaussian process can only recover the truth at a rate that is strictly slower than the minimax rate by a factor that is polynomially suboptimal in the sample size \$n\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Giordano_2022_On_the_inability_of_Gaussian_process_regression_to_optimally_learn.pdf;/Users/cife/Zotero/storage/7EXE86GE/2205.html}
}

@article{girard,
  title = {Gaussian {{Process}} Priors with {{Uncertain Inputs}}: {{Multiple-Step-Ahead Prediction}}},
  author = {Girard, Agathe and Rasmussen, Carl Edward and {Murray-Smith}, Roderick},
  pages = {18},
  abstract = {We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time nonlinear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form yt = f (yt-1, . . . , yt-L), the prediction of y at time t + k is based on the estimates y\textasciicircum t+k-1, . . . , y\textasciicircum t+k-L of the previous outputs. We show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction. In this framework, the problem is that of predicting responses at a random input and we compare the Gaussian approximation to the Monte-Carlo numerical approximation of the predictive distribution. The approach is illustrated on a simulated non-linear dynamic example, as well as on a simple one-dimensional static example.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/TTHULFLG/Girard et al. - Gaussian Process priors with Uncertain Inputs Mul.pdf}
}

@article{girosi1995,
  title = {Regularization Theory and Neural Networks Architectures},
  author = {Girosi, Federico and Jones, Michael and Poggio, Tomaso},
  year = {1995},
  journal = {Neural computation},
  volume = {7},
  number = {2},
  pages = {219--269},
  publisher = {{MIT Press}}
}

@book{gliklikh2011,
  title = {Global and {{Stochastic Analysis}} with {{Applications}} to {{Mathematical Physics}}},
  author = {Gliklikh, Yuri E.},
  year = {2011},
  series = {Theoretical and {{Mathematical Physics}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-0-85729-163-9},
  isbn = {978-0-85729-162-2 978-0-85729-163-9},
  langid = {english},
  file = {/Users/cife/Zotero/storage/QN84NR2W/Gliklikh - 2011 - Global and Stochastic Analysis with Applications t.pdf}
}

@inproceedings{glorot2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Glorot_2011_Deep_Sparse_Rectifier_Neural_Networks.pdf}
}

@misc{gortler2019,
  title = {A {{Visual Exploration}} of {{Gaussian Processes}}},
  author = {G{\"o}rtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  year = {2019},
  howpublished = {https://distill.pub/2019/visual-exploration-gaussian-processes/},
  annotation = {\{doi:10.23915/distill.00017,affiliation:University of Konstanz\}}
}

@techreport{gunawardena2020,
  type = {Preprint},
  title = {Nonlinear {{Classification}} of {{EEG}} Recordings from Patients with {{Alzheimer}}'s {{Disease}} Using {{Gaussian Process Latent Variable Model}}},
  author = {Gunawardena, S. Rajintha. A. S. and He, Fei and Sarrigiannis, Ptolemaios and Blackburn, Daniel J.},
  year = {2020},
  month = may,
  institution = {{Neurology}},
  doi = {10.1101/2020.05.07.20093922},
  abstract = {In this work, nonlinear temporal features from multi-channel EEGs are used for the classification of Alzheimer's disease patients from healthy individuals. This was achieved by temporal manifold learning using Gaussian Process Latent Variable Models (GPLVM) as a nonlinear dimensionality reduction technique. Classification of the extracted features was undertaken using a nonlinear Support Vector Machine. Comparisons were made against the linear counterpart, Principle Component Analysis while exploring the effect of the time window or EEG epoch length used. It was demonstrated that temporal manifold learning using GPLVM is better in extracting features that attain high separability and prediction accuracy. This work aims to set the significance of using GPLVM temporal manifold learning for EEG feature extraction in the classification of Alzheimer's disease.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/C82AMAKF/Gunawardena et al. - 2020 - Nonlinear Classification of EEG recordings from pa.pdf}
}

@article{guo2021,
  title = {A Brief Note on Understanding Neural Networks as {{Gaussian}} Processes},
  author = {Guo, Mengwu},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.11892 [cs, stat]},
  eprint = {2107.11892},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {As a generalization of the work in [Lee et al., 2017], this note briefly discusses when the prior of a neural network output follows a Gaussian process, and how a neural-network-induced Gaussian process is formulated. The posterior mean functions of such a Gaussian process regression lie in the reproducing kernel Hilbert space defined by the neural-network-induced kernel. In the case of two-layer neural networks, the induced Gaussian processes provide an interpretation of the reproducing kernel Hilbert spaces whose union forms a Barron space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Guo_2021_A_brief_note_on_understanding_neural_networks_as_Gaussian_processes.pdf;/Users/cife/Zotero/storage/QFGEJVUM/2107.html}
}

@book{gupta2018,
  title = {Matrix Variate Distributions},
  author = {Gupta, Arjun K and Nagar, Daya K},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}}
}

@article{haff1979,
  title = {An Identity for the {{Wishart}} Distribution with Applications},
  author = {Haff, L. R},
  year = {1979},
  month = dec,
  journal = {Journal of Multivariate Analysis},
  volume = {9},
  number = {4},
  pages = {531--544},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(79)90056-3},
  abstract = {Let Sp\texttimes p have a Wishart distribution with unknown matrix {$\Sigma$} and k degrees of freedom. For a matrix T(S) and a scalar h(S), an identity is obtained for E{$\sum$}tr[h(S)T{$\sum-$}1]. Two applications are given. The first provides product moments and related formulae for the Wishart distribution. Higher moments involving S can be generated recursively. The second application concerns good estimators of {$\sum$} and {$\sum-$}1. In particular, identities for several risk functions are obtained, and estimators of {$\sum$} ({$\sum-$}1) are described which dominate aS(bS-1), a {$\leq$} 1k (b {$\leq$} k - p - 1). Haff [(1977) J. Multivar. Anal. 7 374\textendash 385; (1979) Ann. Statist. 7 No. 5; (1980) Ann. Statist. 8 used special cases of the identity to find unbiased risk estimators. These are unobtainable in closed form for certain natural loss functions. In this paper, we treat these case as well. The dominance results provide a unified theory for the estimation of {$\sum$} and {$\sum-$}1.},
  langid = {english},
  keywords = {estimation of covariance matrix and its inverse,general identities for the risk function,Stoke's theorem,Wishart and inverted moments},
  file = {/Users/cife/Dropbox/Zotero/Haff_1979_An_identity_for_the_Wishart_distribution_with_applications.pdf;/Users/cife/Zotero/storage/XZ2NVJWA/0047259X79900563.html}
}

@inproceedings{hagberg2008,
  title = {Exploring Network Structure, Dynamics, and Function Using {{NetworkX}}},
  booktitle = {Proceedings of the 7th {{Python}} in {{Science Conference}}},
  author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J. and Varoquaux, G{\"a}el and Vaught, Travis and Millman, Jarrod},
  year = {2008},
  month = aug,
  pages = {11--15}
}

@inproceedings{hajian2016,
  title = {Algorithmic {{Bias}}: {{From Discrimination Discovery}} to {{Fairness-aware Data Mining}}},
  shorttitle = {Algorithmic {{Bias}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Hajian, Sara and Bonchi, Francesco and Castillo, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {2125--2126},
  publisher = {{Association for Computing Machinery}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2945386},
  abstract = {Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research.},
  isbn = {978-1-4503-4232-2},
  keywords = {algorithmic bias,discrimination discovery,discrimination prevention,fairness,read-soon},
  file = {/Users/cife/Dropbox/Zotero/Hajian_2016_Algorithmic_Bias.pdf}
}

@book{hall2015,
  title = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}: {{An Elementary Introduction}}},
  shorttitle = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}},
  author = {Hall, Brian C.},
  year = {2015},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  edition = {Second},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-13467-3},
  abstract = {This textbook treats Lie groups, Lie algebras and their representations in an elementary but fully rigorous fashion requiring minimal prerequisites. In particular, the theory of matrix Lie groups and their Lie algebras is developed using only linear algebra, and more motivation and intuition for proofs is provided than in most classic texts on the subject.In addition to its accessible treatment of the basic theory of Lie groups and Lie algebras, the book is also noteworthy for including:a treatment of the Baker\textendash Campbell\textendash Hausdorff formula and its use in place of the Frobenius theorem to establish deeper results about the relationship between Lie groups and Lie algebrasmotivation for the machinery of roots, weights and the Weyl group via a concrete and detailed exposition of the representation theory of sl(3;C)an unconventional definition of semisimplicity that allows for a rapid development of the structure theory of semisimple Lie algebrasa self-contained construction of the representations of compact groups, independent of Lie-algebraic argumentsThe second edition of Lie Groups, Lie Algebras, and Representations contains many substantial improvements and additions, among them: an entirely new part devoted to the structure and representation theory of compact Lie groups; a complete derivation of the main properties of root systems; the construction of finite-dimensional representations of semisimple Lie algebras has been elaborated; a treatment of universal enveloping algebras, including a proof of the Poincar\'e\textendash Birkhoff\textendash Witt theorem and the existence of Verma modules; complete proofs of the Weyl character formula, the Weyl dimension formula and the Kostant multiplicity formula.Review of the first edition:This is an excellent book. It deserves to, and undoubtedly will, become the standard text for early graduate courses in Lie group theory ... an important addition to the textbook literature ... it is highly recommended.\textemdash{} The Mathematical Gazette},
  isbn = {978-3-319-13466-6},
  langid = {english},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/HENNY7DP/9783319134666.html}
}

@article{hamelijnck2021,
  title = {Spatio-{{Temporal Variational Gaussian Processes}}},
  author = {Hamelijnck, Oliver and Wilkinson, William J. and Loppi, Niki A. and Solin, Arno and Damoulas, Theodoros},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01732 [cs, stat]},
  eprint = {2111.01732},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a scalable approach to Gaussian process inference that combines spatio-temporal filtering with natural gradient variational inference, resulting in a non-conjugate GP method for multivariate data that scales linearly with respect to time. Our natural gradient approach enables application of parallel filtering and smoothing, further reducing the temporal span complexity to be logarithmic in the number of time steps. We derive a sparse approximation that constructs a state-space model over a reduced set of spatial inducing points, and show that for separable Markov kernels the full and sparse cases exactly recover the standard variational GP, whilst exhibiting favourable computational properties. To further improve the spatial scaling we propose a mean-field assumption of independence between spatial locations which, when coupled with sparsity and parallelisation, leads to an efficient and accurate method for large spatio-temporal problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hamelijnck_2021_Spatio-Temporal_Variational_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/NW8E8FK7/2111.html}
}

@article{hampel1974,
  title = {The {{Influence Curve}} and Its {{Role}} in {{Robust Estimation}}},
  author = {Hampel, Frank R.},
  year = {1974},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {69},
  number = {346},
  pages = {383--393},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1974.10482962},
  abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation ``near'' strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given.},
  keywords = {read-maybe-never},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1974.10482962},
  file = {/Users/cife/Zotero/storage/ARZ6DTSG/01621459.1974.html}
}

@article{hand2019,
  title = {What Is the {{Purpose}} of {{Statistical Modelling}}?},
  author = {Hand, David},
  year = {2019},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {1},
  number = {1},
  publisher = {{PubPub}},
  issn = {,},
  doi = {10.1162/99608f92.4a85af74},
  langid = {english},
  keywords = {philosophy,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Hand_2019_What_is_the_Purpose_of_Statistical_Modelling.pdf;/Users/cife/Zotero/storage/ITUCKBV6/4.html}
}

@inproceedings{harandi2012,
  title = {Kernel Analysis over {{Riemannian}} Manifolds for Visual Recognition of Actions, Pedestrians and Textures},
  booktitle = {2012 {{IEEE Workshop}} on the {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Harandi, Mehrtash T. and Sanderson, Conrad and Wiliem, Arnold and Lovell, Brian C.},
  year = {2012},
  pages = {433--439},
  publisher = {{IEEE}},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/FE9DEZDF/stamp.html;/Users/cife/Zotero/storage/LAUMHUFQ/6163005.html}
}

@misc{hartley2016,
  title = {Riemannian Manifolds, Kernels and Learning},
  author = {Hartley, Richard},
  year = {2016},
  abstract = {I will talk about recent results from a number of people in the group on Riemannian manifolds in computer vision.  In many Vision problems Riemannian manifolds come up as a natural model.  Data related to a problem can be naturally represented as a point on a Riemannian manifold. This talk will give an intuitive introduction to Riemannian manifolds, and show how they can be applied in many situations.  Examples that will be considered are the Essential manifold, relevant in structure from motion; the manifold of Positive Definite matrices and the Grassman Manifolds, which have a role in object recognition and classification, and the Kendall shape manifold, which represents the shape of 2D objects},
  keywords = {read-maybe-never,riemann}
}

@book{hatcher2002,
  title = {Algebraic Topology},
  author = {Hatcher, A.},
  year = {2002},
  publisher = {{Cambridge University Press}}
}

@misc{hauberg,
  title = {Random {{Metrics Handbook}}},
  author = {Hauberg, S{\o}ren},
  keywords = {reference},
  file = {/Users/cife/Dropbox/Zotero/Hauberg_Random_Metrics_Handbook.pdf}
}

@article{hauberg2012,
  title = {A {{Geometric}} Take on {{Metric Learning}}},
  author = {Hauberg, S{\o}ren and Freifeld, Oren and Black, Michael J},
  year = {2012},
  pages = {15},
  abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Dropbox/Zotero/Hauberg_2012_A_Geometric_take_on_Metric_Learning.pdf}
}

@inproceedings{hauberg2016,
  title = {Dreaming More Data: {{Class-dependent}} Distributions over Diffeomorphisms for Learned Data Augmentation},
  shorttitle = {Dreaming More Data},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Hauberg, S{\o}ren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher, John and Hansen, Lars},
  year = {2016},
  pages = {342--350},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Hauberg_2016_Dreaming_more_data.pdf}
}

@article{hauberg2018,
  title = {Only {{Bayes}} Should Learn a Manifold},
  author = {Hauberg, S{\o}ren},
  year = {2018},
  pages = {15},
  abstract = {We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models.},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Hauberg_2018_Only_Bayes_should_learn_a_manifold.pdf}
}

@techreport{hauberg2018a,
  type = {Technical {{Report}}},
  title = {On the {{Geometry}} of {{Latent Variable Models}}},
  author = {Hauberg, S{\o}ren},
  year = {2018},
  pages = {3},
  institution = {{Technical University of Denmark}},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Hauberg_2018_On_the_Geometry_of_Latent_Variable_Models.pdf}
}

@article{hauberg2019,
  title = {Only {{Bayes}} Should Learn a Manifold (on the Estimation of Differential Geometric Structure from Data)},
  author = {Hauberg, S{\o}ren},
  year = {2019},
  month = sep,
  journal = {arXiv:1806.04994 [cs, stat]},
  eprint = {1806.04994},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,read-very-soon,Statistics - Machine Learning},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/hauberg2019.md;/Users/cife/Dropbox/Zotero/Hauberg_2019_Only_Bayes_should_learn_a_manifold_(on_the_estimation_of_differential_geometric.pdf;/Users/cife/Zotero/storage/395T4RH5/1806.html}
}

@article{hauberga,
  title = {The Non-Central {{Nakagami}} Distribution},
  author = {Hauberg, S{\o}ren},
  pages = {2},
  abstract = {The Nakagami distribution describe the square root of a random variable drawn from a Gamma distribution. Equivalently, the Gamma distribution can be seen as a one-dimensional Wishart distribution. In this note, we consider the distribution of the square root of a random variable drawn from a non-central one-dimensional Wishart distribution. We present the probability density function of this distribution along with closed-form expressions for its moments.},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Zotero/storage/T3947JVF/Hauberg - The non-central Nakagami distribution.pdf}
}

@article{haubergb,
  title = {This Version Is Printed on {{October}} 27, 2021. {{For}} the Latest Version, Please See {{http://compute.dtu.dk/\texttildelow sohau/weekendwithbernie/}}},
  author = {Hauberg, S{\o}ren},
  pages = {88},
  langid = {english},
  file = {/Users/cife/Zotero/storage/5ASMFHPF/Hauberg - This version is printed on October 27, 2021. For t.pdf}
}

@article{haubergc,
  title = {This Version Is Printed on {{October}} 27, 2021. {{For}} the Latest Version, Please See {{http://compute.dtu.dk/\texttildelow sohau/weekendwithbernie/}}},
  author = {Hauberg, S{\o}ren},
  pages = {88},
  langid = {english},
  file = {/Users/cife/Zotero/storage/IEF99I2A/Hauberg - This version is printed on October 27, 2021. For t.pdf}
}

@article{he2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/He_2015_Deep_Residual_Learning_for_Image_Recognition.pdf;/Users/cife/Zotero/storage/S5QJD7P6/1512.html}
}

@misc{health2020,
  title = {Guidance for the {{Use}} of {{Bayesian Statistics}} in {{Medical Device Clinical Trials}}},
  author = {Health, Center for Devices {and} Radiological},
  year = {Sun, 01/19/2020 - 15:13},
  journal = {U.S. Food and Drug Administration},
  publisher = {{FDA}},
  abstract = {1601},
  howpublished = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/guidance-use-bayesian-statistics-medical-device-clinical-trials},
  langid = {english},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/TLFJGICA/guidance-use-bayesian-statistics-medical-device-clinical-trials.html}
}

@article{hegde2018,
  title = {Deep Learning with Differential {{Gaussian}} Process Flows},
  author = {Hegde, Pashupati and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri and Kaski, Samuel},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.04066 [cs, stat]},
  eprint = {1810.04066},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a novel deep learning paradigm of differential flows that learn a stochastic differential equation transformations of inputs prior to a standard classification or regression function. The key property of differential Gaussian processes is the warping of inputs through infinitely deep, but infinitesimal, differential fields, that generalise discrete layers into a dynamical system. We demonstrate state-of-the-art results that exceed the performance of deep Gaussian processes and neural networks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hegde_2018_Deep_learning_with_differential_Gaussian_process_flows.pdf;/Users/cife/Zotero/storage/MQEFDRJY/1810.html}
}

@article{henao2012,
  title = {Predictive Active Set Selection Methods for {{Gaussian}} Processes},
  author = {Henao, Ricardo and Winther, Ole},
  year = {2012},
  month = mar,
  journal = {Neurocomputing},
  series = {Special {{Issue}} on {{Machine Learning}} for {{Signal Processing}} 2010},
  volume = {80},
  pages = {10--18},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2011.09.017},
  abstract = {We propose an active set selection framework for Gaussian process classification for cases when the dataset is large enough to render its inference prohibitive. Our scheme consists of a two step alternating procedure of active set update rules and hyperparameter optimization based upon marginal likelihood maximization. The active set update rules rely on the ability of the predictive distributions of a Gaussian process classifier to estimate the relative contribution of a data point when being either included or removed from the model. This means that we can use it to include points with potentially high impact to the classifier decision process while removing those that are less relevant. We introduce two active set rules based on different criteria, the first one prefers a model with interpretable active set parameters whereas the second puts computational complexity first, thus a model with active set parameters that directly control its complexity. We also provide both theoretical and empirical support for our active set selection strategy being a good approximation of a full Gaussian process classifier. Our extensive experiments show that our approach can compete with state-of-the-art classification techniques with reasonable time complexity. Source code publicly available at http://cogsys.imm.dtu.dk/passgp.},
  langid = {english},
  keywords = {Active set selection,Expectation propagation,Gaussian process classification,Predictive distribution,scalable-gp},
  file = {/Users/cife/Dropbox/Zotero/Henao_2012_Predictive_active_set_selection_methods_for_Gaussian_processes.pdf;/Users/cife/Zotero/storage/U6ST9MQL/S0925231211006047.html}
}

@misc{hensman,
  title = {Deep {{Gaussian Processes}} for {{Large Datasets}}},
  author = {Hensman, James and Damianou, Andreas and Lawrence, Neil},
  abstract = {stitute two of the most important foci of modern machine learning research. In this preliminary work we propose a neat solution for combining the afore-mentioned domains into a single principled framework based on Gaussian processes. Speficically, we invisti-gate algorithms for training deep generative models with hidden layers connected with non-linear Gaus-sian process (GP) mappings. Building on recent devel-opments on (stochastic) variational approximations, the models are fitted on massive data and the hidden variables are marginalised out in a Bayesian manner to allow for efficient propagation of the uncertainty throughout the network of variables. Defining deep Gaussian process networks is challeng-ing even for few data. Consider n observed datapoints},
  file = {/Users/cife/Dropbox/Zotero/Hensman_Deep_Gaussian_Processes_for_Large_Datasets.pdf;/Users/cife/Zotero/storage/CTGEKS53/summary.html}
}

@inproceedings{hensman2012,
  title = {Fast {{Variational Inference}} in the {{Conjugate Exponential Family}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hensman, James and Rattray, Magnus and Lawrence, Neil},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-very-soon},
  file = {/Users/cife/Dropbox/Zotero/Hensman_2012_Fast_Variational_Inference_in_the_Conjugate_Exponential_Family.pdf}
}

@article{hensman2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  year = {2013},
  pages = {9},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/hensman2013.md;/Users/cife/Documents/thesis/sunshine-highs/from_zotero/hensman2013.md;/Users/cife/Dropbox/Zotero/Hensman_2013_Gaussian_Processes_for_Big_Data.pdf}
}

@article{hensman2014,
  title = {Scalable {{Variational Gaussian Process Classification}}},
  author = {Hensman, James and Matthews, Alex and Ghahramani, Zoubin},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.2005 [stat]},
  eprint = {1411.2005},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.},
  archiveprefix = {arXiv},
  keywords = {read,Statistics - Machine Learning},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/hensman2014.md;/Users/cife/Dropbox/Zotero/Hensman_2014_Scalable_Variational_Gaussian_Process_Classification.pdf;/Users/cife/Zotero/storage/XNRQ8GVV/1411.html}
}

@inproceedings{hensman2015,
  title = {{{MCMC}} for {{Variationally Sparse Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Hensman, James and Matthews, Alexander G and Filippone, Maurizio and Ghahramani, Zoubin},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {1648--1656},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Hensman_2015_MCMC_for_Variationally_Sparse_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/YYI7TJAC/5875-mcmc-for-variationally-sparse-gaussian-processes.html}
}

@article{hensman2015a,
  title = {{{MCMC}} for {{Variationally Sparse Gaussian Processes}}},
  author = {Hensman, James and Matthews, Alexander G. de G. and Filippone, Maurizio and Ghahramani, Zoubin},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.04000 [stat]},
  eprint = {1506.04000},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly.},
  archiveprefix = {arXiv},
  keywords = {read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hensman_2015_MCMC_for_Variationally_Sparse_Gaussian_Processes2.pdf;/Users/cife/Zotero/storage/6GWVL8U8/1506.html}
}

@article{hensman2017,
  title = {Variational {{Fourier}} Features for {{Gaussian}} Processes},
  author = {Hensman, James and Durrande, Nicolas and Solin, Arno},
  year = {2017},
  month = nov,
  journal = {arXiv:1611.06740 [stat]},
  eprint = {1611.06740},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.},
  archiveprefix = {arXiv},
  keywords = {read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hensman_2017_Variational_Fourier_features_for_Gaussian_processes.pdf;/Users/cife/Zotero/storage/F5KZF6H3/1611.html}
}

@misc{hensman2018,
  title = {{{GPSS}} 18 | {{Gaussian Processes}} with {{Non-Gaussian Likelihoods}}},
  author = {Hensman, James},
  year = {2018},
  month = sep,
  abstract = {James Hensman of PROWLER.io talks about GPs with non-Gaussian likelihoods. Full program at http://gpss.cc/gpss18/program Slides: http://gpss.cc/gpss18/slides/Hensman2...},
  keywords = {reference}
}

@article{herman,
  title = {Derivation of the {{Geodesic Equation}} and {{Defining}} the {{Christoffel Symbols}}},
  author = {Herman, Dr Russell L},
  pages = {3},
  langid = {english},
  file = {/Users/cife/Zotero/storage/BVQPJXCF/Herman - Derivation of the Geodesic Equation and Deﬁning th.pdf}
}

@inproceedings{hernandez-lobato2016,
  title = {Scalable {{Gaussian Process Classification}} via {{Expectation Propagation}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {{Hernandez-Lobato}, Daniel and {Hernandez-Lobato}, Jose Miguel},
  year = {2016},
  month = may,
  pages = {168--176},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Variational methods have been recently considered for scaling the training process of Gaussian process classifiers to large datasets. As an alternative, we describe here how to train these classifiers efficiently using expectation propagation (EP). The proposed EP method allows to train Gaussian process classifiers on very large datasets, with millions of instances, that were out of the reach of previous implementations of EP. More precisely, it can be used for (i) training in a distributed fashion where the data instances are sent to different nodes in which the required computations are carried out, and for (ii) maximizing an estimate of the marginal likelihood using a stochastic approximation of the gradient. Several experiments involving large datasets show that the method described is competitive with the variational approach.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Hernandez-Lobato_2016_Scalable_Gaussian_Process_Classification_via_Expectation_Propagation.pdf}
}

@misc{herrero-vidal2021,
  title = {Across-Animal Odor Decoding by Probabilistic Manifold Alignment},
  author = {{Herrero-Vidal}, Pedro and Rinberg, Dmitry and Savin, Cristina},
  year = {2021},
  month = jun,
  pages = {2021.06.06.447279},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.06.447279},
  abstract = {Identifying the common structure of neural dynamics across subjects is key for extracting unifying principles of brain computation and for many brain machine interface applications. Here, we propose a novel probabilistic approach for aligning stimulus-evoked responses from multiple animals in a common low dimensional manifold and use hierarchical inference to identify which stimulus drives neural activity in any given trial. Our probabilistic decoder is robust to a range of features of the neural responses and significantly outperforms existing neural alignment procedures. When applied to recordings from the mouse olfactory bulb, our approach reveals low-dimensional population dynamics that are odor specific and have consistent structure across animals. Thus, our decoder can be used for increasing the robustness and scalability of neural-based chemical detection.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Herrero-Vidal_2021_Across-animal_odor_decoding_by_probabilistic_manifold_alignment.pdf;/Users/cife/Zotero/storage/6FJMAXK4/2021.06.06.html}
}

@article{higgins2018,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.02230 [cs, stat]},
  eprint = {1812.02230},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,sb,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Higgins_2018_Towards_a_Definition_of_Disentangled_Representations.pdf;/Users/cife/Zotero/storage/ZHFIUTKL/1812.html}
}

@article{hillier,
  title = {Moments of a {{Wishart}} Matrix},
  author = {Hillier, Grant and Kan, Raymond},
  pages = {22},
  abstract = {The paper discusses the moments of Wishart matrices, in both the central and noncentral cases. The first part of the paper shows that the expectation map has certain homogeneity and equivariance properties which impose considerable structure on the moments, hitherto unrecognised. The second part of the paper explains how the moments may be computed efficiently. The two parts of the paper are completely independent, but the computations produce precisely the algebraic structure predicted in the first part, as well as reproducing all previously known formulae. A number of examples are given for the more manageable cases.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/SUGL2TGP/Hillier and Kan - Moments of a Wishart matrix.pdf}
}

@inproceedings{hinton2003,
  title = {Stochastic Neighbor Embedding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hinton, Geoffrey E. and Roweis, Sam T.},
  year = {2003},
  pages = {857--864},
  keywords = {read,sb,tsne},
  file = {/Users/cife/Dropbox/Zotero/Hinton_2003_Stochastic_neighbor_embedding.pdf}
}

@article{hoang2020,
  title = {Revisiting the {{Sample Complexity}} of {{Sparse Spectrum Approximation}} of {{Gaussian Processes}}},
  author = {Hoang, Minh and Hoang, Nghia and Pham, Hai and Woodruff, David},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Zotero/storage/JYESTKTE/Hoang et al. - 2020 - Revisiting the Sample Complexity of Sparse Spectru.pdf;/Users/cife/Zotero/storage/IYNL3ZWH/95b431e51fc53692913da5263c214162-Abstract.html}
}

@article{hoffman2013,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  month = apr,
  journal = {arXiv:1206.7051 [cs, stat]},
  eprint = {1206.7051},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,read-very-soon,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/hoffman2013.md;/Users/cife/Dropbox/Zotero/Hoffman_2013_Stochastic_Variational_Inference.pdf;/Users/cife/Zotero/storage/75B7AIR8/1206.html}
}

@article{hooker1995,
  title = {Testing Heuristics: {{We}} Have It All Wrong},
  shorttitle = {Testing Heuristics},
  author = {Hooker, J. N.},
  year = {1995},
  month = sep,
  journal = {Journal of Heuristics},
  volume = {1},
  number = {1},
  pages = {33--42},
  issn = {1381-1231, 1572-9397},
  doi = {10.1007/BF02430364},
  abstract = {The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. It is hard to make fair comparisons between algorithms and to assemble realistic test problems. Competitive testing tells us which algorithm is faster but not why. Because it requires polished code, it consumes time and energy that could be better spent doing more experiments. This article argues that a more scientific approach of controlled experimentation, similar to that used in other empirical sciences, avoids or alleviates these problems. We have confused research and development; competitive testing is suited only for the latter.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/G9JPWLC8/Hooker - 1995 - Testing heuristics We have it all wrong.pdf}
}

@article{hooker1995a,
  title = {Testing Heuristics: {{We}} Have It All Wrong},
  shorttitle = {Testing Heuristics},
  author = {Hooker, J. N.},
  year = {1995},
  month = sep,
  journal = {Journal of Heuristics},
  volume = {1},
  number = {1},
  pages = {33--42},
  issn = {1381-1231, 1572-9397},
  doi = {10.1007/BF02430364},
  abstract = {The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. It is hard to make fair comparisons between algorithms and to assemble realistic test problems. Competitive testing tells us which algorithm is faster but not why. Because it requires polished code, it consumes time and energy that could be better spent doing more experiments. This article argues that a more scientific approach of controlled experimentation, similar to that used in other empirical sciences, avoids or alleviates these problems. We have confused research and development; competitive testing is suited only for the latter.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/C5E6TJ2R/Hooker - 1995 - Testing heuristics We have it all wrong.pdf}
}

@article{hotelling1933,
  title = {Analysis of a Complex of Statistical Variables with Principal Components},
  author = {Hotelling, Harold},
  year = {1933},
  journal = {J. Educ. Psy.},
  volume = {24},
  pages = {498--520}
}

@misc{hsu,
  title = {A {{Brief Introduction}} to {{Brownian Motion}} on a {{Riemannian Manifold}}},
  author = {Hsu, Elton P},
  langid = {english},
  file = {/Users/cife/Zotero/storage/VFVYLFQC/Hsu - A Brief Introduction to Brownian Motion on a Riema.pdf}
}

@book{hsu2002,
  title = {Stochastic Analysis on Manifolds},
  author = {Hsu, Elton P},
  year = {2002},
  volume = {38},
  publisher = {{American Mathematical Soc.}},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/TEYXAKUC/Kyushu.pdf}
}

@article{huang2021,
  title = {Gaussian {{Process Regression With Maximizing}} the {{Composite Conditional Likelihood}}},
  author = {Huang, Haojie and Li, Zhongmei and Peng, Xin and Ding, Steven X. and Zhong, Weimin},
  year = {2021},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {70},
  pages = {1--11},
  issn = {1557-9662},
  doi = {10.1109/TIM.2021.3104376},
  abstract = {Gaussian process regression (GPR) has an outstanding nonlinear fitting ability, and its uncertainty predictions can deliver the confidence level of the estimations, which is well adapted to deal with complex industrial processes. However, disturbances and noises in outputs might lead to mispredictions for new samples. In this article, a method using the modified likelihood is proposed to deal with the output corrupted by noises, which aims to achieve a more stable and reliable generative model. Furthermore, the proposed method is applied to a simulation experiment and an actual hydrocracking process to model the relationship between the input variables and the light ends, and the experimental results demonstrate the efficiency of the proposed method.},
  keywords = {Bayesian method,Fitting,Gaussian process regression (GPR),Gaussian processes,hydrocracking process,Input variables,Linear programming,Stability analysis,Training,Uncertainty}
}

@book{huber2004,
  title = {Robust {{Statistics}}},
  author = {Huber, Peter J.},
  year = {2004},
  publisher = {{John Wiley \& Sons}},
  abstract = {The first systematic, book-length treatment of the subject. Begins with a general introduction and the formal mathematical background behind qualitative and quantitative robustness. Stresses concepts. Provides selected numerical algorithms for computing robust estimates, as well as convergence proofs. Tables contain quantitative robustness information for a variety of estimates.},
  googlebooks = {e62RhdqIdMkC},
  isbn = {978-0-471-65072-0},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{hullermeier2020,
  title = {Aleatoric and {{Epistemic Uncertainty}} in {{Machine Learning}}: {{An Introduction}} to {{Concepts}} and {{Methods}}},
  shorttitle = {Aleatoric and {{Epistemic Uncertainty}} in {{Machine Learning}}},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = {2020},
  month = sep,
  journal = {arXiv:1910.09457 [cs, stat]},
  eprint = {1910.09457},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,reference,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hüllermeier_2020_Aleatoric_and_Epistemic_Uncertainty_in_Machine_Learning.pdf;/Users/cife/Zotero/storage/SP6VASY9/1910.html}
}

@article{hutchinson2021,
  title = {Vector-Valued {{Gaussian Processes}} on {{Riemannian Manifolds}} via {{Gauge Independent Projected Kernels}}},
  author = {Hutchinson, Michael and Terenin, Alexander and Borovitskiy, Viacheslav and Takao, So and Teh, Yee Whye and Deisenroth, Marc Peter},
  year = {2021},
  month = nov,
  journal = {arXiv:2110.14423 [cs, stat]},
  eprint = {2110.14423},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes are machine learning models capable of learning unknown functions in a way that represents uncertainty, thereby facilitating construction of optimal decision-making systems. Motivated by a desire to deploy Gaussian processes in novel areas of science, a rapidly-growing line of research has focused on constructively extending these models to handle non-Euclidean domains, including Riemannian manifolds, such as spheres and tori. We propose techniques that generalize this class to model vector fields on Riemannian manifolds, which are important in a number of application areas in the physical sciences. To do so, we present a general recipe for constructing gauge independent kernels, which induce Gaussian vector fields, i.e. vector-valued Gaussian processes coherent with geometry, from scalar-valued Riemannian kernels. We extend standard Gaussian process training methods, such as variational inference, to this setting. This enables vector-valued Gaussian processes on Riemannian manifolds to be trained using standard methods and makes them accessible to machine learning practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Hutchinson_2021_Vector-valued_Gaussian_Processes_on_Riemannian_Manifolds_via_Gauge_Independent.pdf;/Users/cife/Zotero/storage/UEYHICB5/2110.html}
}

@misc{igorsusmelj2020,
  title = {{{IgorSusmelj}}/Pytorch-Styleguide},
  author = {IgorSusmelj},
  year = {2020},
  month = dec,
  abstract = {An unofficial styleguide and best practices summary for PyTorch},
  copyright = {GPL-3.0 License         ,                 GPL-3.0 License},
  keywords = {best-practices,pytorch,styleguide}
}

@article{initiative2020,
  title = {Data {{Readiness}}: {{Lessons}} from an {{Emergency}}},
  shorttitle = {Data {{Readiness}}},
  author = {Initiative, The DELVE},
  year = {2020},
  month = nov,
  publisher = {{The Royal Society}},
  abstract = {Responding to the COVID-19 pandemic has required rapid decision-making in changing circumstances. Those decisions and their effects on the health and wealth of the nation can be better informed wit...},
  langid = {english},
  keywords = {grey literature,reference},
  file = {/Users/cife/Zotero/storage/7EE88THS/data-readiness-lessons-from-an-emergency.html}
}

@article{ipsen2020,
  title = {Not-{{MIWAE}}: {{Deep Generative Modelling}} with {{Missing}} Not at {{Random Data}}},
  shorttitle = {Not-{{MIWAE}}},
  author = {Ipsen, Niels Bruun and Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.12871 [cs, stat]},
  eprint = {2006.12871},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {When a missing process depends on the missing values themselves, it needs to be explicitly modelled and taken into account while doing likelihood-based inference. We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data. Specifically, a deep neural network enables us to flexibly model the conditional distribution of the missingness pattern given the data. This allows for incorporating prior information about the type of missingness (e.g. self-censoring) into the model. Our inference technique, based on importance-weighted variational inference, involves maximising a lower bound of the joint likelihood. Stochastic gradients of the bound are obtained by using the reparameterisation trick both in latent space and data space. We show on various kinds of data sets and missingness patterns that explicitly modelling the missing process can be invaluable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Ipsen_2020_not-MIWAE.pdf;/Users/cife/Zotero/storage/XJ89L3WR/2006.html}
}

@article{ishizuka,
  title = {Nonparametric {{Bayesian Deep Visualization}}},
  author = {Ishizuka, Haruya and Mochihashi, Daichi},
  pages = {16},
  abstract = {Visualization methods such as t-SNE [1] have helped in knowledge discovery from high-dimensional data; however, their performance may degrade when the intrinsic structure of observations is in low-dimensional space, and they cannot estimate clusters that are often useful to understand the internal structure of a dataset. A solution is to visualize the latent coordinates and clusters estimated using a neural clustering model. However, they require a long computational time since they have numerous weights to train and must tune the layer width, the number of latent dimensions and clusters to appropriately model the latent space. Additionally, the estimated coordinates may not be suitable for visualization since such a model and visualization method are applied independently. We utilize neural network Gaussian processes (NNGP) [2] equivalent to a neural network whose weights are marginalized to eliminate the necessity to optimize weights and layer widths. Additionally, to determine latent dimensions and the number of clusters without tuning, we propose a latent variable model that combines NNGP with automatic relevance determination [3] to extract necessary dimensions of latent space and infinite Gaussian mixture model [4] to infer the number of clusters. We integrate this model and visualization method into nonparametric Bayesian deep visualization (NPDV) that learns latent and visual coordinates jointly to render latent coordinates optimal for visualization. Experimental results on images and document datasets show that NPDV shows superior accuracy to existing methods, and it requires less training time than the neural clustering model because of its lower tuning cost. Furthermore, NPDV can reveal plausible latent clusters without labels.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/MLVZKXX4/Ishizuka and Mochihashi - Nonparametric Bayesian Deep Visualization.pdf}
}

@inproceedings{izmailov2018,
  title = {Scalable {{Gaussian Processes}} with {{Billions}} of {{Inducing Inputs}} via {{Tensor Train Decomposition}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Izmailov, Pavel and Novikov, Alexander and Kropotov, Dmitry},
  year = {2018},
  month = mar,
  pages = {726--735},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) models. We build on previous scalable GP research including stochastic variational inference based on inducing inputs, kernel interpolation, and structure exploiting algebra. The key idea of our method is to use Tensor Train decomposition for variational parameters, which allows us to train GPs with billions of inducing inputs and achieve state-of-the-art results on several benchmarks. Further, our approach allows for training kernels based on deep neural networks without any modifications to the underlying GP model. A neural network learns a multidimensional embedding for the data, which is used by the GP to make the final prediction. We train GP and neural network parameters end-to-end without pretraining, through maximization of GP marginal likelihood. We show the efficiency of the proposed approach on several regression and classification benchmark datasets including MNIST, CIFAR-10, and Airline.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Izmailov_2018_Scalable_Gaussian_Processes_with_Billions_of_Inducing_Inputs_via_Tensor_Train.pdf}
}

@article{izmailov2021,
  title = {What {{Are Bayesian Neural Network Posteriors Really Like}}?},
  author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D. and Wilson, Andrew Gordon},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.14421 [cs, stat]},
  eprint = {2104.14421},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a "cold posterior" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Izmailov_2021_What_Are_Bayesian_Neural_Network_Posteriors_Really_Like.pdf;/Users/cife/Zotero/storage/CAHXJJI6/2104.html}
}

@inproceedings{jafrasteh2022,
  title = {Input {{Dependent Sparse Gaussian Processes}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Jafrasteh, Bahram and {Villacampa-Calvo}, Carlos and {Hernandez-Lobato}, Daniel},
  year = {2022},
  month = jun,
  pages = {9739--9759},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Gaussian Processes (GPs) are non-parametric models that provide accurate uncertainty estimates. Nevertheless, they have a cubic cost in the number of data instances \$N\$. To overcome this, sparse GP approximations are used, in which a set of \$M \textbackslash ll N\$ inducing points is introduced. The location of the inducing points is learned by considering them parameters of an approximate posterior distribution \$q\$. Sparse GPs, combined with stochastic variational inference for inferring \$q\$ have a cost per iteration in \$\textbackslash mathcal\{O\}(M\^3)\$. Critically, the inducing points determine the flexibility of the model and they are often located in regions where the latent function changes. A limitation is, however, that in some tasks a large number of inducing points may be required to obtain good results. To alleviate this, we propose here to amortize the computation of the inducing points locations, as well as the parameters of \$q\$. For this, we use a neural network that receives a data instance as an input and outputs the corresponding inducing points locations and the parameters of \$q\$. We evaluate our method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GPs. However, in our method the number of inducing points is reduced drastically since they depend on the input data. This makes our method scale to larger datasets and have faster training and prediction times.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Jafrasteh_2022_Input_Dependent_Sparse_Gaussian_Processes.pdf}
}

@inproceedings{jahanian2020,
  title = {On the "Steerability" of Generative Adversarial Networks},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Jahanian, Ali and Chai, Lucy and Isola, Phillip},
  year = {2020},
  month = apr,
  abstract = {An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -- these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by "steering" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan\_steerability/},
  langid = {english},
  keywords = {read,sb},
  file = {/Users/cife/Dropbox/Zotero/Jahanian_2020_On_the_steerability_of_generative_adversarial_networks.pdf;/Users/cife/Zotero/storage/ZGDR2LC8/poster_HylsTT4FvB.html}
}

@article{jakkala2021,
  title = {Deep {{Gaussian Processes}}: {{A Survey}}},
  shorttitle = {Deep {{Gaussian Processes}}},
  author = {Jakkala, Kalvik},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12135 [cs, stat]},
  eprint = {2106.12135},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes are one of the dominant approaches in Bayesian learning. Although the approach has been applied to numerous problems with great success, it has a few fundamental limitations. Multiple methods in literature have addressed these limitations. However, there has not been a comprehensive survey of the topics as of yet. Most existing surveys focus on only one particular variant of Gaussian processes and their derivatives. This survey details the core motivations for using Gaussian processes, their mathematical formulations, limitations, and research themes that have flourished over the years to address said limitations. Furthermore, one particular research area is Deep Gaussian Processes (DGPs), it has improved substantially in the past decade. The significant publications that advanced the forefront of this research area are outlined in their survey. Finally, a brief discussion on open problems and research directions for future work is presented at the end.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Jakkala_2021_Deep_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/RWXZ2HWX/2106.html}
}

@article{jakkala2021a,
  title = {Deep {{Gaussian Processes}}: {{A Survey}}},
  shorttitle = {Deep {{Gaussian Processes}}},
  author = {Jakkala, Kalvik},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12135 [cs, stat]},
  eprint = {2106.12135},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes are one of the dominant approaches in Bayesian learning. Although the approach has been applied to numerous problems with great success, it has a few fundamental limitations. Multiple methods in literature have addressed these limitations. However, there has not been a comprehensive survey of the topics as of yet. Most existing surveys focus on only one particular variant of Gaussian processes and their derivatives. This survey details the core motivations for using Gaussian processes, their mathematical formulations, limitations, and research themes that have flourished over the years to address said limitations. Furthermore, one particular research area is Deep Gaussian Processes (DGPs), it has improved substantially in the past decade. The significant publications that advanced the forefront of this research area are outlined in their survey. Finally, a brief discussion on open problems and research directions for future work is presented at the end.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Jakkala_2021_Deep_Gaussian_Processes2.pdf;/Users/cife/Zotero/storage/8RLY9I26/2106.html}
}

@article{jankowiak2019,
  title = {Parametric {{Gaussian Process Regressors}}},
  author = {Jankowiak, Martin and Pleiss, Geoff and Gardner, Jacob R.},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.07123 [cs, stat]},
  eprint = {1910.07123},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process (GP) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes GP priors attractive. In this work we propose two simple methods for scalable GP regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to FITC (Fully Independent Training Conditional; Snelson et.\textasciitilde al.\textasciitilde 2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable GP regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Jankowiak_2019_Parametric_Gaussian_Process_Regressors.pdf;/Users/cife/Zotero/storage/HEFEA2KL/1910.html}
}

@article{jankowiak2020,
  title = {Parametric {{Gaussian Process Regressors}}},
  author = {Jankowiak, Martin and Pleiss, Geoff and Gardner, Jacob R.},
  year = {2020},
  month = dec,
  journal = {arXiv:1910.07123 [cs, stat]},
  eprint = {1910.07123},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process (GP) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes GP priors attractive. In this work we propose two simple methods for scalable GP regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to FITC (Fully Independent Training Conditional; Snelson et.\textasciitilde al.\textasciitilde 2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable GP regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/jankowiak2020.md;/Users/cife/Dropbox/Zotero/Jankowiak_2020_Parametric_Gaussian_Process_Regressors.pdf;/Users/cife/Zotero/storage/Q73I584L/1910.html}
}

@article{jaquier2020,
  title = {High-{{Dimensional Bayesian Optimization}} via {{Nested Riemannian Manifolds}}},
  author = {Jaquier, No{\'e}mie and Rozo, Leonel},
  year = {2020},
  month = nov,
  journal = {arXiv:2010.10904 [cs, math]},
  eprint = {2010.10904},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Despite the recent success of Bayesian optimization (BO) in a variety of applications where sample efficiency is imperative, its performance may be seriously compromised in settings characterized by high-dimensional parameter spaces. A solution to preserve the sample efficiency of BO in such problems is to introduce domain knowledge into its formulation. In this paper, we propose to exploit the geometry of non-Euclidean search spaces, which often arise in a variety of domains, to learn structure-preserving mappings and optimize the acquisition function of BO in low-dimensional latent spaces. Our approach, built on Riemannian manifolds theory, features geometry-aware Gaussian processes that jointly learn a nested-manifold embedding and a representation of the objective function in the latent space. We test our approach in several benchmark artificial landscapes and report that it not only outperforms other high-dimensional BO approaches in several settings, but consistently optimizes the objective functions, as opposed to geometry-unaware BO methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Jaquier_2020_High-Dimensional_Bayesian_Optimization_via_Nested_Riemannian_Manifolds.pdf;/Users/cife/Zotero/storage/XD3SR2VX/2010.html}
}

@article{jaquier2021,
  title = {Geometry-Aware {{Bayesian Optimization}} in {{Robotics}} Using {{Riemannian Mat}}\textbackslash 'ern {{Kernels}}},
  author = {Jaquier, No{\'e}mie and Borovitskiy, Viacheslav and Smolensky, Andrei and Terenin, Alexander and Asfour, Tamim and Rozo, Leonel},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01460 [cs]},
  eprint = {2111.01460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. Many of these problems require optimization of functions defined on non-Euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. To do so, one must place a Gaussian process prior, or equivalently define a kernel, on the space of interest. Effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. Recent work on the Riemannian Mat\textbackslash 'ern kernels, based on stochastic partial differential equations and spectral theory of the Laplace-Beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. In this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware Bayesian optimization for a variety of robotic applications, covering orientation control, manipulability optimization, and motion planning, while showing its improved performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/cife/Dropbox/Zotero/Jaquier_2021_Geometry-aware_Bayesian_Optimization_in_Robotics_using_Riemannian_Mat-'ern.pdf;/Users/cife/Zotero/storage/UNYLDBRZ/2111.html}
}

@article{jaquier2021a,
  title = {Geometry-Aware {{Bayesian Optimization}} in {{Robotics}} Using {{Riemannian Mat}}\textbackslash 'ern {{Kernels}}},
  author = {Jaquier, No{\'e}mie and Borovitskiy, Viacheslav and Smolensky, Andrei and Terenin, Alexander and Asfour, Tamim and Rozo, Leonel},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01460 [cs]},
  eprint = {2111.01460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. Many of these problems require optimization of functions defined on non-Euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. To do so, one must place a Gaussian process prior, or equivalently define a kernel, on the space of interest. Effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. Recent work on the Riemannian Mat\textbackslash 'ern kernels, based on stochastic partial differential equations and spectral theory of the Laplace-Beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. In this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware Bayesian optimization for a variety of robotic applications, covering orientation control, manipulability optimization, and motion planning, while showing its improved performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/cife/Dropbox/Zotero/Jaquier_2021_Geometry-aware_Bayesian_Optimization_in_Robotics_using_Riemannian_Mat-'ern2.pdf;/Users/cife/Zotero/storage/4GIGNA2K/2111.html}
}

@misc{jaquier2022,
  title = {Bringing Robotics Taxonomies to Continuous Domains via {{GPLVM}} on Hyperbolic Manifolds},
  author = {Jaquier, No{\'e}mie and Rozo, Leonel and {Gonz{\'a}lez-Duque}, Miguel and Borovitskiy, Viacheslav and Asfour, Tamim},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01672},
  eprint = {2210.01672},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Robotic taxonomies have appeared as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite the efforts devoted to design their hierarchy and underlying categories, their use in application fields remains scarce. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. To do so, we formulate a Gaussian process hyperbolic latent variable model and enforce the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We test our model on the whole-body support pose taxonomy to learn hyperbolic embeddings that comply with the original graph structure. We show that our model properly encodes unseen poses from existing or new taxonomy categories, it can be used to generate trajectories between the embeddings, and it outperforms its Euclidean counterparts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/cife/Dropbox/Zotero/Jaquier_2022_Bringing_robotics_taxonomies_to_continuous_domains_via_GPLVM_on_hyperbolic.pdf;/Users/cife/Zotero/storage/GABKAMJ7/2210.html}
}

@article{jayasumana2015,
  title = {Kernel {{Methods}} on {{Riemannian Manifolds}} with {{Gaussian RBF Kernels}}},
  author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {12},
  eprint = {1412.0265},
  eprinttype = {arxiv},
  pages = {2464--2477},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2015.2414422},
  abstract = {In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-someday,riemann},
  file = {/Users/cife/Dropbox/Zotero/Jayasumana_2015_Kernel_Methods_on_Riemannian_Manifolds_with_Gaussian_RBF_Kernels.pdf;/Users/cife/Zotero/storage/B9EWKM9H/7063231.html;/Users/cife/Zotero/storage/FR8Q5QQM/1412.html}
}

@inproceedings{jensen1906,
  title = {Sur Les Fonctions Convexes et Les In\'egualit\'es                     Entre Les Valeurs {{Moyennes}}},
  author = {Jensen, Johan},
  year = {1906},
  month = nov,
  publisher = {{Zenodo}},
  doi = {10.1007/bf02418571}
}

@article{jensen2020,
  title = {Manifold {{GPLVMs}} for Discovering Non-{{Euclidean}} Latent Structure in Neural Data},
  author = {Jensen, Kristopher T. and Kao, Ta-Chu and Tripodi, Marco and Hennequin, Guillaume},
  year = {2020},
  month = oct,
  journal = {arXiv:2006.07429 [cs, q-bio, stat]},
  eprint = {2006.07429},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {A common problem in neuroscience is to elucidate the collective neural representations of behaviorally important variables such as head direction, spatial location, upcoming movements, or mental spatial transformations. Often, these latent variables are internal constructs not directly accessible to the experimenter. Here, we propose a new probabilistic latent variable model to simultaneously identify the latent state and the way each neuron contributes to its representation in an unsupervised way. In contrast to previous models which assume Euclidean latent spaces, we embrace the fact that latent states often belong to symmetric manifolds such as spheres, tori, or rotation groups of various dimensions. We therefore propose the manifold Gaussian process latent variable model (mGPLVM), where neural responses arise from (i) a shared latent variable living on a specific manifold, and (ii) a set of non-parametric tuning curves determining how each neuron contributes to the representation. Cross-validated comparisons of models with different topologies can be used to distinguish between candidate manifolds, and variational inference enables quantification of uncertainty. We demonstrate the validity of the approach on several synthetic datasets, as well as on calcium recordings from the ellipsoid body of Drosophila melanogaster and extracellular recordings from the mouse anterodorsal thalamic nucleus. These circuits are both known to encode head direction, and mGPLVM correctly recovers the ring topology expected from neural populations representing a single angular variable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Jensen_2020_Manifold_GPLVMs_for_discovering_non-Euclidean_latent_structure_in_neural_data.pdf;/Users/cife/Dropbox/Zotero/Jensen_2020_Manifold_GPLVMs_for_discovering_non-Euclidean_latent_structure_in_neural_data3.pdf;/Users/cife/Zotero/storage/UVRI7XU3/2006.html}
}

@article{jensen2020a,
  title = {Manifold {{GPLVMs}} for Discovering Non-{{Euclidean}} Latent Structure in Neural Data},
  author = {Jensen, Kristopher and Kao, Ta-Chu and Tripodi, Marco and Hennequin, Guillaume},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Zotero/storage/TL8M6YNQ/Jensen et al. - 2020 - Manifold GPLVMs for discovering non-Euclidean late.pdf;/Users/cife/Zotero/storage/IVSWE9NR/fedc604da8b0f9af74b6cfc0fab2163c-Abstract.html;/Users/cife/Zotero/storage/ZIEG3N6Q/fedc604da8b0f9af74b6cfc0fab2163c-Abstract.html}
}

@article{jiang2019,
  title = {Wasserstein {{Fair Classification}}},
  author = {Jiang, Ray and Pacchiano, Aldo and Stepleton, Tom and Jiang, Heinrich and Chiappa, Silvia},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.12059 [cs, stat]},
  eprint = {1907.12059},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose an approach to fair classification that enforces independence between the classifier outputs and sensitive information by minimizing Wasserstein-1 distances. The approach has desirable theoretical properties and is robust to specific choices of the threshold used to obtain class predictions from model outputs. We introduce different methods that enable hiding sensitive information at test time or have a simple and fast implementation. We show empirical performance against different fairness baselines on several benchmark fairness datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Jiang_2019_Wasserstein_Fair_Classification.pdf;/Users/cife/Zotero/storage/YAZKGYHF/1907.html}
}

@inproceedings{johnson2016,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Johnson, Matthew J and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  keywords = {dataset},
  file = {/Users/cife/Dropbox/Zotero/Johnson_2016_Composing_graphical_models_with_neural_networks_for_structured_representations.pdf}
}

@article{johnson2020,
  title = {Kernel {{Methods}} and Their Derivatives: {{Concept}} and Perspectives for the {{Earth}} System Sciences},
  shorttitle = {Kernel {{Methods}} and Their Derivatives},
  author = {Johnson, J. Emmanuel and Laparra, Valero and {P{\'e}rez-Suay}, Adri{\'a}n and Mahecha, Miguel D. and {Camps-Valls}, Gustau},
  year = {2020},
  month = oct,
  journal = {PLOS ONE},
  volume = {15},
  number = {10},
  eprint = {2007.14706},
  eprinttype = {arxiv},
  pages = {e0235885},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0235885},
  abstract = {Kernel methods are powerful machine learning techniques which implement generic non-linear functions to solve complex tasks in a simple way. They Have a solid mathematical background and exhibit excellent performance in practice. However, kernel machines are still considered black-box models as the feature mapping is not directly accessible and difficult to interpret.The aim of this work is to show that it is indeed possible to interpret the functions learned by various kernel methods is intuitive despite their complexity. Specifically, we show that derivatives of these functions have a simple mathematical formulation, are easy to compute, and can be applied to many different problems. We note that model function derivatives in kernel machines is proportional to the kernel function derivative. We provide the explicit analytic form of the first and second derivatives of the most common kernel functions with regard to the inputs as well as generic formulas to compute higher order derivatives. We use them to analyze the most used supervised and unsupervised kernel learning methods: Gaussian Processes for regression, Support Vector Machines for classification, Kernel Entropy Component Analysis for density estimation, and the Hilbert-Schmidt Independence Criterion for estimating the dependency between random variables. For all cases we expressed the derivative of the learned function as a linear combination of the kernel function derivative. Moreover we provide intuitive explanations through illustrative toy examples and show how to improve the interpretation of real applications in the context of spatiotemporal Earth system data cubes. This work reflects on the observation that function derivatives may play a crucial role in kernel methods analysis and understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Johnson_2020_Kernel_Methods_and_their_derivatives.pdf;/Users/cife/Zotero/storage/KFXCVS7R/2007.html}
}

@incollection{jordan1998,
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  editor = {Jordan, Michael I.},
  year = {1998},
  pages = {105--161},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5014-9_5},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  isbn = {978-94-010-6104-9 978-94-011-5014-9},
  langid = {english},
  file = {/Users/cife/Zotero/storage/GHASZ8YW/Jordan et al. - 1998 - An Introduction to Variational Methods for Graphic.pdf}
}

@article{jordy2018,
  title = {Oral {{Semaglutide Does Not Affect}} the {{Bioavailability}} of the {{Combined Oral Contraceptive Ethinylestradiol}}/{{Levonorgestrel}}},
  author = {JORDY, ANDREAS B{\^O}RSTING and BREITSCHAFT, ASTRID and CHRISTIANSEN, ERIK and GRANHALL, CHARLOTTE and HANSEN, CILIE W. and {HOUSHMAND-OREGAARD}, AZADEH and BAEKDAL, TINE A.},
  year = {2018},
  month = jul,
  journal = {Diabetes},
  volume = {67},
  number = {Supplement\_1},
  pages = {1135-P},
  issn = {0012-1797},
  doi = {10.2337/db18-1135-P},
  abstract = {Semaglutide is a glucagon-like peptide (GLP-1) analog co-formulated with the absorption enhancer, sodium N-(8-[2-hydroxybenzoyl] amino) caprylate (SNAC), to allow oral administration. The effect of oral semaglutide on the pharmacokinetics (PK) of the combined oral contraceptive (OC) ethinylestradiol (EE; 0.03 mg)/levonorgestrel (LN; 0.15 mg) was assessed in an open-label, one sequence crossover trial. Healthy post-menopausal females (n=25) received 8 days of OC alone and 8 days of OC with oral semaglutide (dose escalated to steady state at week 6: 1 week at 3 mg dose, 1 week at 7 mg dose, 4 weeks at 14 mg dose). Primary endpoints were the areas under the plasma concentration-time curve for EE and LN during a dosing interval (0-24 h) at steady state (AUC0-24h,SS). Secondary endpoints included other PK parameters, safety and tolerability. Total exposure of EE and LN were similar for OC alone vs. OC with oral semaglutide, and oral semaglutide did not affect the maximum plasma exposure (Cmax,SS) of EE or LN. AUC0-24h,SS and Cmax,SS ratios for EE and LN were within the predefined no effect interval (0.8-1.25) (Figure). Adverse events with oral semaglutide were consistent with previous trials and expected GLP-1 receptor agonist class effects. These data indicate that oral semaglutide did not affect the bioavailability of the combined OC. A. B\^orsting Jordy: Employee; Self; Novo Nordisk A/S. A. Breitschaft: None. E. Christiansen: Stock/Shareholder; Self; Novo Nordisk A/S. Employee; Self; Novo Nordisk A/S. C. Granhall: Employee; Self; Novo Nordisk A/S. C.W. Hansen: Employee; Self; Novo Nordisk A/S. A. Houshmand-Oregaard: Employee; Self; Novo Nordisk A/S. T.A. Baekdal: Stock/Shareholder; Self; Novo Nordisk A/S. Employee; Self; Novo Nordisk A/S.},
  keywords = {biostatistics,novo},
  file = {/Users/cife/Zotero/storage/LEAYE6NQ/54200.html}
}

@article{jorgensen2020,
  title = {Isometric {{Gaussian Process Latent Variable Model}} for {{Dissimilarity Data}}},
  author = {J{\o}rgensen, Martin and Hauberg, S{\o}ren},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.11741 [cs, stat]},
  eprint = {2006.11741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a fully generative model where the latent variable respects both the distances and the topology of the modeled data. The model leverages the Riemannian geometry of the generated manifold to endow the latent space with a well-defined stochastic distance measure, which is modeled as Nakagami distributions. These stochastic distances are sought to be as similar as possible to observed distances along a neighborhood graph through a censoring process. The model is inferred by variational inference and is therefore fully generative. We demonstrate how the new model can encode invariances in the learned manifolds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/jorgensen2020.md;/Users/cife/Dropbox/Zotero/Jørgensen_2020_Isometric_Gaussian_Process_Latent_Variable_Model_for_Dissimilarity_Data.pdf;/Users/cife/Zotero/storage/22ZSIBKJ/2006.html}
}

@phdthesis{jorgensen2020a,
  title = {Stochastic {{Representations}} with {{Gaussian Processes}} and {{Geometry}}},
  author = {J{\o}rgensen, Martin},
  year = {2020},
  abstract = {This thesis consists of 4 independent pieces of work and each of these have a ded- icated chapter in the manuscript. The first chapter investigates contemporary methodologies for estimating predictive variance networks in regression neural networks. The second chapter goes beyond regression task, and studies Gaus- sian processes to present a Bayesian non-parametric way of inferring stochastic differential equations for both regression and continuous-time dynamical mod- elling. The third chapter unifies theory of geometry and Gaussian processes to present a latent variable model that respects both the distances and the topol- ogy of unlabelled data. The fourth, and last, chapter shortly reviews current methodologies for bivariate causal invariance and propose an algorithm using a non-parametric estimator robust towards a causal invariant: changes in the marginal distributions.},
  school = {Technical University of Denmark},
  keywords = {read},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/jorgensen2020a.md;/Users/cife/Dropbox/Zotero/Jørgensen_2020_Stochastic_Representations_with_Gaussian_Processes_and_Geometry.pdf}
}

@article{kalatzis2020,
  title = {Variational {{Autoencoders}} with {{Riemannian Brownian Motion Priors}}},
  author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, S{\o}ren},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05227 [cs, stat]},
  eprint = {2002.05227},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kalatzis_2020_Variational_Autoencoders_with_Riemannian_Brownian_Motion_Priors.pdf;/Users/cife/Zotero/storage/5FQQB4DF/2002.html}
}

@article{kanagawa2018,
  ids = {kanagawa2018a},
  title = {Gaussian {{Processes}} and {{Kernel Methods}}: {{A Review}} on {{Connections}} and   {{Equivalences}}},
  shorttitle = {Gaussian {{Processes}} and {{Kernel Methods}}},
  author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K.},
  year = {2018},
  month = jun,
  journal = {arXiv:1807.02582v1 [stat]},
  eprint = {1807.02582v1},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kanagawa_2018_Gaussian_Processes_and_Kernel_Methods.pdf;/Users/cife/Zotero/storage/K7I6B9D2/1807.html;/Users/cife/Zotero/storage/YBJ3Z7WI/1806.html}
}

@article{karaletsos2020,
  title = {Hierarchical {{Gaussian Process Priors}} for {{Bayesian Neural Network Weights}}},
  author = {Karaletsos, Theofanis and Bui, Thang D.},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Karaletsos_2020_Hierarchical_Gaussian_Process_Priors_for_Bayesian_Neural_Network_Weights.pdf;/Users/cife/Zotero/storage/J5Y2SRQX/c70341de2c112a6b3496aec1f631dddd-Abstract.html;/Users/cife/Zotero/storage/NNPRRZGS/c70341de2c112a6b3496aec1f631dddd-Abstract.html}
}

@article{karvonen2022,
  title = {Maximum {{Likelihood Estimation}} in {{Gaussian Process Regression}} Is {{Ill-Posed}}},
  author = {Karvonen, Toni and Oates, Chris J.},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.09179 [cs, math, stat]},
  eprint = {2203.09179},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed. That is, when the predictions of the regression model are continuous (or insensitive to small perturbations) in the training data. This article presents a rigorous proof that the maximum likelihood estimator fails to be well-posed in Hellinger distance in a scenario where the data are noiseless. The failure case occurs for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is informally well-known, these theoretical results appear to be the first of their kind, and suggest that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Karvonen_2022_Maximum_Likelihood_Estimation_in_Gaussian_Process_Regression_is_Ill-Posed.pdf;/Users/cife/Zotero/storage/KYPT6TLQ/2203.html}
}

@article{kaski2003,
  title = {Trustworthiness and Metrics in Visualizing Similarity of Gene Expression},
  author = {Kaski, Samuel and Nikkil{\"a}, Janne and Oja, Merja and Venna, Jarkko and T{\"o}r{\"o}nen, Petri and Castr{\'e}n, Eero},
  year = {2003},
  month = oct,
  journal = {BMC bioinformatics},
  volume = {4},
  pages = {48},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-4-48},
  abstract = {BACKGROUND: Conventionally, the first step in analyzing the large and high-dimensional data sets measured by microarrays is visual exploration. Dendrograms of hierarchical clustering, self-organizing maps (SOMs), and multidimensional scaling have been used to visualize similarity relationships of data samples. We address two central properties of the methods: (i) Are the visualizations trustworthy, i.e., if two samples are visualized to be similar, are they really similar? (ii) The metric. The measure of similarity determines the result; we propose using a new learning metrics principle to derive a metric from interrelationships among data sets. RESULTS: The trustworthiness of hierarchical clustering, multidimensional scaling, and the self-organizing map were compared in visualizing similarity relationships among gene expression profiles. The self-organizing map was the best except that hierarchical clustering was the most trustworthy for the most similar profiles. Trustworthiness can be further increased by treating separately those genes for which the visualization is least trustworthy. We then proceed to improve the metric. The distance measure between the expression profiles is adjusted to measure differences relevant to functional classes of the genes. The genes for which the new metric is the most different from the usual correlation metric are listed and visualized with one of the visualization methods, the self-organizing map, computed in the new metric. CONCLUSIONS: The conjecture from the methodological results is that the self-organizing map can be recommended to complement the usual hierarchical clustering for visualizing and exploring gene expression data. Discarding the least trustworthy samples and improving the metric still improves it.},
  langid = {english},
  pmcid = {PMC272927},
  pmid = {14552657},
  keywords = {Animals,Cluster Analysis,Computer Graphics,Gene Expression Profiling,Gene Expression Regulation,Gene Expression Regulation; Fungal,Humans,Mice,Oligonucleotide Array Sequence Analysis,read-maybe-never,Sequence Homology; Nucleic Acid,tsne},
  file = {/Users/cife/Dropbox/Zotero/Kaski_2003_Trustworthiness_and_metrics_in_visualizing_similarity_of_gene_expression.pdf}
}

@article{kaski2003a,
  title = {Trustworthiness and Metrics in Visualizing Similarity of Gene Expression},
  author = {Kaski, Samuel and Nikkil{\"a}, Janne and Oja, Merja and Venna, Jarkko and T{\"o}r{\"o}nen, Petri and Castr{\'e}n, Eero},
  year = {2003},
  month = oct,
  journal = {BMC bioinformatics},
  volume = {4},
  pages = {48},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-4-48},
  abstract = {BACKGROUND: Conventionally, the first step in analyzing the large and high-dimensional data sets measured by microarrays is visual exploration. Dendrograms of hierarchical clustering, self-organizing maps (SOMs), and multidimensional scaling have been used to visualize similarity relationships of data samples. We address two central properties of the methods: (i) Are the visualizations trustworthy, i.e., if two samples are visualized to be similar, are they really similar? (ii) The metric. The measure of similarity determines the result; we propose using a new learning metrics principle to derive a metric from interrelationships among data sets. RESULTS: The trustworthiness of hierarchical clustering, multidimensional scaling, and the self-organizing map were compared in visualizing similarity relationships among gene expression profiles. The self-organizing map was the best except that hierarchical clustering was the most trustworthy for the most similar profiles. Trustworthiness can be further increased by treating separately those genes for which the visualization is least trustworthy. We then proceed to improve the metric. The distance measure between the expression profiles is adjusted to measure differences relevant to functional classes of the genes. The genes for which the new metric is the most different from the usual correlation metric are listed and visualized with one of the visualization methods, the self-organizing map, computed in the new metric. CONCLUSIONS: The conjecture from the methodological results is that the self-organizing map can be recommended to complement the usual hierarchical clustering for visualizing and exploring gene expression data. Discarding the least trustworthy samples and improving the metric still improves it.},
  langid = {english},
  pmcid = {PMC272927},
  pmid = {14552657},
  keywords = {Animals,Cluster Analysis,Computer Graphics,Gene Expression Profiling,Gene Expression Regulation,Gene Expression Regulation; Fungal,Humans,Mice,Oligonucleotide Array Sequence Analysis,Sequence Homology; Nucleic Acid},
  file = {/Users/cife/Dropbox/Zotero/Kaski_2003_Trustworthiness_and_metrics_in_visualizing_similarity_of_gene_expression2.pdf}
}

@misc{keng2017,
  title = {Variational {{Bayes}} and {{The Mean-Field Approximation}}},
  author = {Keng, Brian},
  year = {2017},
  month = apr,
  journal = {Bounded Rationality},
  abstract = {A brief introduction to variational Bayes and the mean-field approximation.},
  howpublished = {http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/},
  langid = {english},
  file = {/Users/cife/Zotero/storage/GX7CJWGS/variational-bayes-and-the-mean-field-approximation.html}
}

@article{khan2019,
  title = {Approximate {{Inference Turns Deep Networks}} into {{Gaussian Processes}}},
  author = {Khan, Mohammad Emtiyaz and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
  year = {2019},
  month = dec,
  journal = {arXiv:1906.01930 [cs, stat]},
  eprint = {1906.01930},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. As a result, we can obtain a GP kernel and a nonlinear feature map simply by training the DNN. Surprisingly, the resulting kernel is the neural tangent kernel which has desirable theoretical properties for infinitely-wide DNNs. We show feature maps obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Khan_2019_Approximate_Inference_Turns_Deep_Networks_into_Gaussian_Processes.pdf}
}

@article{khelif2014,
  title = {Stochastic Manifolds},
  author = {Khelif, Anatole and Tarica, Alain},
  year = {2014},
  month = jun,
  journal = {arXiv:1312.0117 [math]},
  eprint = {1312.0117},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Stochastic Manifolds\guillemotright{} Malliavin Calculus can be seen as a differential calculus on Wiener spaces. We present the notion of stochastic manifold for which the Malliavin Calculus plays the same role as the classical differential calculus for the C{$\infty$} differential manifolds. The set of the paths in a Riemmanian compact manifold is then seen as a particular case of the above structure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Khelif_2014_Stochastic_manifolds.pdf}
}

@article{kim2020,
  title = {The {{Geometry}} of {{Nonlinear Embeddings}} in {{Kernel Discriminant Analysis}}},
  author = {Kim, Jiae and Lee, Yoonkyung and Liang, Zhiyu},
  year = {2020},
  month = may,
  journal = {arXiv:2005.05546 [cs, stat]},
  eprint = {2005.05546},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Fisher's linear discriminant analysis is a classical method for classification, yet it is limited to capturing linear features only. Kernel discriminant analysis as an extension is known to successfully alleviate the limitation through a nonlinear feature mapping. We study the geometry of nonlinear embeddings in discriminant analysis with polynomial kernels and Gaussian kernel by identifying the population-level discriminant function that depends on the data distribution and the kernel. In order to obtain the discriminant function, we solve a generalized eigenvalue problem with between-class and within-class covariance operators. The polynomial discriminants are shown to capture the class difference through the population moments explicitly. For approximation of the Gaussian discriminant, we use a particular representation of the Gaussian kernel by utilizing the exponential generating function for Hermite polynomials. We also show that the Gaussian discriminant can be approximated using randomized projections of the data. Our results illuminate how the data distribution and the kernel interact in determination of the nonlinear embedding for discrimination, and provide a guideline for choice of the kernel and its parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kim_2020_The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis.pdf;/Users/cife/Zotero/storage/WT8CCP9Z/2005.html}
}

@misc{kindap2022,
  title = {Non-{{Gaussian Process Regression}}},
  author = {K{\i}ndap, Yaman and Godsill, Simon},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03117},
  eprint = {2209.03117},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Standard GPs offer a flexible modelling tool for well-behaved processes. However, deviations from Gaussianity are expected to appear in real world datasets, with structural outliers and shocks routinely observed. In these cases GPs can fail to model uncertainty adequately and may over-smooth inferences. Here we extend the GP framework into a new class of time-changed GPs that allow for straightforward modelling of heavy-tailed non-Gaussian behaviours, while retaining a tractable conditional GP structure through an infinite mixture of non-homogeneous GPs representation. The conditional GP structure is obtained by conditioning the observations on a latent transformed input space and the random evolution of the latent transformation is modelled using a L\textbackslash '\{e\}vy process which allows Bayesian inference in both the posterior predictive density and the latent transformation function. We present Markov chain Monte Carlo inference procedures for this model and demonstrate the potential benefits compared to a standard GP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kındap_2022_Non-Gaussian_Process_Regression.pdf;/Users/cife/Zotero/storage/EYM2I5JJ/2209.html}
}

@article{kingma2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kingma_2014_Auto-Encoding_Variational_Bayes.pdf;/Users/cife/Zotero/storage/EETQ3JB3/1312.html}
}

@article{kingma2015,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2015},
  journal = {International Conference on Learning Representations (ICLR)}
}

@phdthesis{kondor2008,
  title = {Group Theoretical Methods in Machine Learning},
  author = {Kondor, Risi},
  year = {2008},
  school = {Columbia University},
  keywords = {read},
  file = {/Users/cife/Zotero/storage/WT42GI79/KondorThesis.pdf}
}

@article{krizhevsky,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  pages = {60},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Krizhevsky_Learning_Multiple_Layers_of_Features_from_Tiny_Images.pdf}
}

@techreport{krizhevsky2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex},
  year = {2009}
}

@article{krizhevsky2010,
  title = {Cifar-10 and Cifar-100 Datasets},
  author = {Krizhevsky, Alex and Nair, Vinod and Geoffrey, Hinton},
  year = {2010},
  journal = {Technical Report},
  number = {Canadian Institute for Advanced Research},
  pages = {60},
  langid = {english},
  keywords = {data},
  annotation = {itemtype:dataset},
  file = {/Users/cife/Dropbox/Zotero/Krizhevsky_2010_Cifar-10_and_cifar-100_datasets.pdf}
}

@incollection{krizhevsky2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  keywords = {code},
  file = {/Users/cife/Dropbox/Zotero/Krizhevsky_2012_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.pdf;/Users/cife/Zotero/storage/5CSDICUX/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{kumar2018,
  ids = {kumar2018a},
  title = {Deep {{Gaussian Processes}} with {{Convolutional Kernels}}},
  author = {Kumar, Vinayak and Singh, Vaibhav and Srijith, P. K. and Damianou, Andreas},
  year = {2018},
  month = may,
  journal = {arXiv:1806.01655v1 [stat]},
  eprint = {1806.01655v1},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative to standard parametric deep learning models. A DGP is formed by stacking multiple GPs resulting in a well-regularized composition of functions. The Bayesian framework that equips the model with attractive properties, such as implicit capacity control and predictive uncertainty, makes it at the same time challenging to combine with a convolutional structure. This has hindered the application of DGPs in computer vision tasks, an area where deep parametric models (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such as radial basis functions (RBFs) are insufficient for handling pixel variability in raw images. In this paper, we build on the recent convolutional GP to develop Convolutional DGP (CDGP) models which effectively capture image level features through the use of convolution kernels, therefore opening up the way for applying DGPs to computer vision tasks. Our model learns local spatial influence and outperforms strong GP based baselines on multi-class image classification. We also consider various constructions of convolution kernel over the image patches, analyze the computational trade-offs and provide an efficient framework for convolutional DGP models. The experimental results on image data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate the effectiveness of the proposed approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kumar_2018_Deep_Gaussian_Processes_with_Convolutional_Kernels.pdf;/Users/cife/Zotero/storage/ACQBIDGN/1910.html;/Users/cife/Zotero/storage/S38BHEDF/1806.html}
}

@article{kumari2020,
  title = {{{ShapeVis}}: {{High-dimensional Data Visualization}} at {{Scale}}},
  shorttitle = {{{ShapeVis}}},
  author = {Kumari, Nupur and R., Siddarth and Rupela, Akash and Gupta, Piyush and Krishnamurthy, Balaji},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.05166 [cs, stat]},
  eprint = {2001.05166},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present ShapeVis, a scalable visualization technique for point cloud data inspired from topological data analysis. Our method captures the underlying geometric and topological structure of the data in a compressed graphical representation. Much success has been reported by the data visualization technique Mapper, that discreetly approximates the Reeb graph of a filter function on the data. However, when using standard dimensionality reduction algorithms as the filter function, Mapper suffers from considerable computational cost. This makes it difficult to scale to high-dimensional data. Our proposed technique relies on finding a subset of points called landmarks along the data manifold to construct a weighted witness-graph over it. This graph captures the structural characteristics of the point cloud, and its weights are determined using a Finite Markov Chain. We further compress this graph by applying induced maps from standard community detection algorithms. Using techniques borrowed from manifold tearing, we prune and reinstate edges in the induced graph based on their modularity to summarize the shape of data. We empirically demonstrate how our technique captures the structural characteristics of real and synthetic data sets. Further, we compare our approach with Mapper using various filter functions like t-SNE, UMAP, LargeVis and show that our algorithm scales to millions of data points while preserving the quality of data visualization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Kumari_2020_ShapeVis.pdf;/Users/cife/Zotero/storage/T3MXXDT9/2001.html}
}

@inproceedings{kunin2020,
  title = {Neural {{Mechanics}}: {{Symmetry}} and {{Broken Conservation Laws}} in {{Deep Learning Dynamics}}},
  shorttitle = {Neural {{Mechanics}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kunin, Daniel and {Sagastuy-Brena}, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},
  year = {2020},
  month = sep,
  abstract = {Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of...},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Kunin_2020_Neural_Mechanics.pdf;/Users/cife/Zotero/storage/GE6PPFY2/forum.html}
}

@article{kurihara2018,
  title = {Stochastic Metric Space and Quantum Mechanics},
  author = {Kurihara, Yoshimasa},
  year = {2018},
  month = mar,
  journal = {Journal of Physics Communications},
  volume = {2},
  number = {3},
  pages = {035025},
  issn = {2399-6528},
  doi = {10.1088/2399-6528/aaa851},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Kurihara_2018_Stochastic_metric_space_and_quantum_mechanics.pdf}
}

@article{kurihara2018a,
  title = {Stochastic {{Metric Space}} and {{Quantum Mechanics}}},
  author = {Kurihara, Yoshimasa},
  year = {2018},
  month = mar,
  journal = {Journal of Physics Communications},
  volume = {2},
  number = {3},
  eprint = {1612.04228},
  eprinttype = {arxiv},
  pages = {035025},
  issn = {2399-6528},
  doi = {10.1088/2399-6528/aaa851},
  abstract = {A new idea for the quantization of dynamic systems, as well as space time itself, using a stochastic metric is proposed. The quantum mechanics of a mass point is constructed on a space time manifold using a stochastic metric. A stochastic metric space is, in brief, a metric space whose metric tensor is given stochastically according to some appropriate distribution function. A mathematically consistent model of a space time manifold equipping a stochastic metric is proposed in this report. The quantum theory in the local Minkowski space can be recognized as a classical theory on the stochastic Lorentz-metric-space. A stochastic calculus on the space time manifold is performed using white noise functional analysis. A path-integral quantization is introduced as a stochastic integration of a function of the action integral, and it is shown that path-integrals on the stochastic metric space are mathematically well-defined for large variety of potential functions. The Newton--Nelson equation of motion can also be obtained from the Newtonian equation of motion on the stochastic metric space. It is also shown that the commutation relation required under the canonical quantization is consistent with the stochastic quantization introduced in this report. The quantum effects of general relativity are also analyzed through natural use of the stochastic metrics. Some example of quantum effects on the universe is discussed.},
  archiveprefix = {arXiv},
  keywords = {General Relativity and Quantum Cosmology,Quantum Physics},
  file = {/Users/cife/Zotero/storage/ABPCLCIR/Kurihara - 2018 - Stochastic Metric Space and Quantum Mechanics.pdf;/Users/cife/Zotero/storage/LNJ92TKZ/1612.html}
}

@article{lagatta2014,
  title = {Geodesics of {{Random Riemannian Metrics}}},
  author = {LaGatta, Tom and Wehr, Jan},
  year = {2014},
  month = apr,
  journal = {Communications in Mathematical Physics},
  volume = {327},
  number = {1},
  eprint = {1206.4939},
  eprinttype = {arxiv},
  pages = {181--241},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-014-1901-8},
  abstract = {We analyze the disordered Riemannian geometry resulting from random perturbations of the Euclidean metric. We focus on geodesics, the paths traced out by a particle traveling in this quenched random environment. By taking the point of the view of the particle, we show that the law of its observed environment is absolutely continuous with respect to the law of the random metric, and we provide an explicit form for its Radon-Nikodym derivative. We use this result to prove a ``local Markov property'' along an unbounded geodesic, demonstrating that it eventually encounters any type of geometric phenomenon. We also develop in this paper some general results on conditional Gaussian measures. Our Main Theorem states that a geodesic chosen with random initial conditions (chosen independently of the metric) is almost surely not minimizing. To demonstrate this, we show that a minimizing geodesic is guaranteed to eventually pass over a certain ``bump surface,'' which locally has constant positive curvature. By using Jacobi fields, we show that this is sufficient to destabilize the minimizing property.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematical Physics,Mathematics - Differential Geometry,Mathematics - Probability,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/LaGatta_2014_Geodesics_of_Random_Riemannian_Metrics.pdf}
}

@article{lagatta2014a,
  title = {Geodesics of {{Random Riemannian Metrics}}},
  author = {LaGatta, Tom and Wehr, Jan},
  year = {2014},
  month = apr,
  journal = {Communications in Mathematical Physics},
  volume = {327},
  number = {1},
  pages = {181--241},
  issn = {1432-0916},
  doi = {10.1007/s00220-014-1901-8},
  abstract = {We analyze the disordered Riemannian geometry resulting from random perturbations of the Euclidean metric. We focus on geodesics, the paths traced out by a particle traveling in this quenched random environment. By taking the point of the view of the particle, we show that the law of its observed environment is absolutely continuous with respect to the law of the random metric, and we provide an explicit form for its Radon\textendash Nikodym derivative. We use this result to prove a ``local Markov property'' along an unbounded geodesic, demonstrating that it eventually encounters any type of geometric phenomenon. We also develop in this paper some general results on conditional Gaussian measures. Our Main Theorem states that a geodesic chosen with random initial conditions (chosen independently of the metric) is almost surely not minimizing. To demonstrate this, we show that a minimizing geodesic is guaranteed to eventually pass over a certain ``bump surface,'' which locally has constant positive curvature. By using Jacobi fields, we show that this is sufficient to destabilize the minimizing property.},
  langid = {english},
  keywords = {geometry,read-soon},
  file = {/Users/cife/Dropbox/Zotero/LaGatta_2014_Geodesics_of_Random_Riemannian_Metrics2.pdf}
}

@article{lagatta2014b,
  title = {Geodesics of {{Random Riemannian Metrics}}},
  author = {LaGatta, Tom and Wehr, Jan},
  year = {2014},
  month = apr,
  journal = {Communications in Mathematical Physics},
  volume = {327},
  number = {1},
  eprint = {1206.4939},
  eprinttype = {arxiv},
  primaryclass = {math-ph},
  pages = {181--241},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-014-1901-8},
  abstract = {We analyze the disordered Riemannian geometry resulting from random perturbations of the Euclidean metric. We focus on geodesics, the paths traced out by a particle traveling in this quenched random environment. By taking the point of the view of the particle, we show that the law of its observed environment is absolutely continuous with respect to the law of the random metric, and we provide an explicit form for its Radon-Nikodym derivative. We use this result to prove a "local Markov property" along an unbounded geodesic, demonstrating that it eventually encounters any type of geometric phenomenon. We also develop in this paper some general results on conditional Gaussian measures. Our Main Theorem states that a geodesic chosen with random initial conditions (chosen independently of the metric) is almost surely not minimizing. To demonstrate this, we show that a minimizing geodesic is guaranteed to eventually pass over a certain "bump surface," which locally has constant positive curvature. By using Jacobi fields, we show that this is sufficient to destabilize the minimizing property.},
  archiveprefix = {arXiv},
  keywords = {Mathematical Physics,Mathematics - Differential Geometry,Mathematics - Probability},
  file = {/Users/cife/Dropbox/Zotero/LaGatta_2014_Geodesics_of_Random_Riemannian_Metrics3.pdf;/Users/cife/Zotero/storage/WALB8YVZ/1206.html}
}

@article{lalchand,
  title = {Latent {{Variable Models}} with {{Gaussian}} Processes and {{Normalising Flows}}},
  author = {Lalchand, V R},
  pages = {3},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/LEA79MPJ/Lalchand - Latent Variable Models with Gaussian processes and.pdf}
}

@unpublished{lalchand2020,
  type = {Draft},
  title = {Flow Based Inference in {{Bayesian GPLVM}}},
  author = {Lalchand, Vidhi},
  year = {2020},
  abstract = {Gaussian process latent variable models (GPLVM) have long been used to perform nonlinear and probabilistic dimensionality reduction extending the reach of non-parametric models like Gaussian processes (GP) to the domain of unsupervised learning [Lawrence, 2004]. The Bayesian incarnation of the GPLVM is facilitated through the variational framework where the posterior over all unknown quantities is approximated by a well-behaved variational family, frequently chosen to be a factorised Gaussian [Titsias and Lawrence, 2010]. This allows not only implicit regularisation but also mathematical convenience. In this work we narrow our focus on examining the quality of the latent representation learnt under this Gaussian assumption. We introduce non-Gaussianity in the distribution of the latent space by leveraging normalising flows. Inference is performed using Stochastic Variational Inference (SVI) with a structured variational lower bound that factorizes across data points permitting mini-batching of gradients. We call this flexible model class Gaussian process latent variable flows (GPLVF) that are compatible with several choices of the encoder (also called recognition models / back constraints). We compare this advanced framework with existing models for representation learning like VAE's and predecessors like the GPLVM, Bayesian GPLVM and the back-constrained GPLVM [Lawrence and Quin{\~ }onero-Candela, 2006]. Our experiments span synthetic constructions, real-world benchmark datasets and datasets with massively missing data.},
  keywords = {read-soon},
  file = {/Users/cife/Zotero/storage/A8T554IY/Flow_based_inference_in_Bayesian_GPLVM.pdf;/Users/cife/Zotero/storage/LE7FPRZW/Lalchand_2020_Flow_based_inference_in_Bayesian GPLVM.pdf}
}

@article{lalchand2022,
  title = {Generalised {{Gaussian Process Latent Variable Models}} ({{GPLVM}}) with {{Stochastic Variational Inference}}},
  author = {Lalchand, Vidhi and Ravuri, Aditya and Lawrence, Neil D.},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.12979 [cs, stat]},
  eprint = {2202.12979},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian process latent variable models (GPLVM) are a flexible and non-linear approach to dimensionality reduction, extending classical Gaussian processes to an unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias and Lawrence, 2010] uses a variational framework, where the posterior over latent variables is approximated by a well-behaved variational family, a factorized Gaussian yielding a tractable lower bound. However, the non-factories ability of the lower bound prevents truly scalable inference. In this work, we study the doubly stochastic formulation of the Bayesian GPLVM model amenable with minibatch training. We show how this framework is compatible with different latent variable formulations and perform experiments to compare a suite of models. Further, we demonstrate how we can train in the presence of massively missing data and obtain high-fidelity reconstructions. We demonstrate the model's performance by benchmarking against the canonical sparse GPLVM for high-dimensional data examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Lalchand_2022_Generalised_Gaussian_Process_Latent_Variable_Models_(GPLVM)_with_Stochastic.pdf;/Users/cife/Zotero/storage/HCHQD79X/2202.html}
}

@article{laloudouana2003,
  title = {Data Set Selection},
  author = {LaLoudouana, Doudou and Tarare, Mambobo Bonouliqui},
  year = {2003},
  journal = {Journal of Machine Learning Gossip},
  abstract = {We introduce the community to a new construction principle whose practical implications are very broad. Central to this research is the idea to improve the presentation of algorithms in the literature and to make them more appealing. We de ne a new notion of capacity for data sets and derive a methodology for selecting from them. The experiments show that even for not so good algorithms, you can show that they are signi cantly better than all the others. We give some experimental results, which are very promising.},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/LaLoudouana_2003_Data_set_selection.pdf}
}

@article{lawrence,
  title = {A {{Unifying Probabilistic Perspective}} for {{Spectral Dimensionality Reduction}}: {{Insights}} and {{New Models}}},
  author = {Lawrence, Neil D},
  pages = {30},
  abstract = {We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/9ACVW8BX/Lawrence - A Unifying Probabilistic Perspective for Spectral .pdf}
}

@phdthesis{lawrence2001,
  title = {Variational Inference in Probabilistic Models},
  author = {Lawrence, Neil David},
  year = {2001},
  school = {Citeseer}
}

@article{lawrence2002,
  ids = {herbricha},
  title = {Fast {{Sparse Gaussian Process Methods}}: {{The Informative Vector Machine}}},
  author = {Lawrence, Neil D and Seeger, Matthias and Herbrich, Ralf},
  year = {2002},
  pages = {8},
  abstract = {We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d\textendash sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n {$\cdot$} d2), and in large real-world classification experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be significantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (`error bars'), allows for Bayesian model selection and is less complex in implementation.},
  langid = {english},
  keywords = {read-maybe-never,scalable-gp},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2002_Fast_Sparse_Gaussian_Process_Methods.pdf}
}

@inproceedings{lawrence2004,
  title = {Learning to Learn with the Informative Vector Machine},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Lawrence, Neil D. and Platt, John C.},
  year = {2004},
  pages = {65},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015382},
  abstract = {This paper describes an e cient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An e cient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more e cient than random sub-sampling on an arti cial data-set and more e ective than the traditional IVM in a speaker dependent phoneme recognition task.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/DUQD7VFY/Lawrence and Platt - 2004 - Learning to learn with the informative vector mach.pdf}
}

@article{lawrence2005,
  ids = {lawrenceProbabilisticNonlinearPrincipal2005,lawrenceProbabilisticNonlinearPrincipala},
  title = {Probabilistic {{Non-linear Principal Component Analysis}} with {{Gaussian Process Latent Variable Models}}},
  author = {Lawrence, Neil},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  pages = {34},
  abstract = {Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artificially generated data sets.},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/lawrence2005.md;/Users/cife/Dropbox/Zotero/Lawrence_2005_Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process4.pdf;/Users/cife/Zotero/storage/3ZRTAMW3/lawrence05a.html}
}

@article{lawrence2005a,
  ids = {lawrence2004},
  title = {Gaussian {{Process Latent Variable Models}} for {{Visualisation}} of {{High Dimensional Data}}},
  author = {Lawrence, Neil D},
  year = {2005},
  pages = {8},
  abstract = {In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to `twin kernel PCA' in which a mapping between feature spaces occurs.},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2005_Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional.pdf;/Users/cife/Dropbox/Zotero/Lawrence_2005_Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional2.pdf}
}

@inproceedings{lawrence2006,
  title = {Local Distance Preservation in the {{GP-LVM}} through Back Constraints},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Lawrence, Neil D. and {Qui{\~n}onero-Candela}, Joaquin},
  year = {2006},
  month = jun,
  series = {{{ICML}} '06},
  pages = {513--520},
  publisher = {{Association for Computing Machinery}},
  address = {{Pittsburgh, Pennsylvania, USA}},
  doi = {10.1145/1143844.1143909},
  abstract = {The Gaussian process latent variable model (GP-LVM) is a generative approach to nonlinear low dimensional embedding, that provides a smooth probabilistic mapping from latent to data space. It is also a non-linear generalization of probabilistic PCA (PPCA) (Tipping \& Bishop, 1999). While most approaches to non-linear dimensionality methods focus on preserving local distances in data space, the GP-LVM focusses on exactly the opposite. Being a smooth mapping from latent to data space, it focusses on keeping things apart in latent space that are far apart in data space. In this paper we first provide an overview of dimensionality reduction techniques, placing the emphasis on the kind of distance relation preserved. We then show how the GP-LVM can be generalized, through back constraints, to additionally preserve local distances. We give illustrative experiments on common data sets.},
  isbn = {978-1-59593-383-6},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2006_Local_distance_preservation_in_the_GP-LVM_through_back_constraints.pdf}
}

@inproceedings{lawrence2007,
  ids = {lawrence2007a},
  title = {Hierarchical {{Gaussian}} Process Latent Variable Models},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning - {{ICML}} '07},
  author = {Lawrence, Neil D. and Moore, Andrew J.},
  year = {2007},
  pages = {481--488},
  publisher = {{ACM Press}},
  address = {{Corvalis, Oregon}},
  doi = {10.1145/1273496.1273557},
  abstract = {The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) allows us to express conditional independencies in the data as well as the manifold structure. We first introduce Gaussian process hierarchies through a simple dynamical model, we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets.},
  isbn = {978-1-59593-793-3},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2007_Hierarchical_Gaussian_process_latent_variable_models.pdf;/Users/cife/Dropbox/Zotero/Lawrence_2007_Hierarchical_Gaussian_process_latent_variable_models2.pdf;/Users/cife/Dropbox/Zotero/Lawrence_2007_Hierarchical_Gaussian_process_latent_variable_models3.pdf}
}

@inproceedings{lawrence2007a,
  title = {Learning for {{Larger Datasets}} with the {{Gaussian Process Latent Variable Model}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Lawrence, Neil D.},
  year = {2007},
  month = mar,
  pages = {243--250},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {In this paper we apply the latest techniques in sparse Gaussian process regression (GPR) to the Gaussian process latent variable model (GPLVM). We review three techniques and discuss how they may be implemented in the context of the GP-LVM. Each approach is then implemented on a well known benchmark data set and compared with earlier attempts to sparsify the model.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2007_Learning_for_Larger_Datasets_with_the_Gaussian_Process_Latent_Variable_Model.pdf}
}

@article{lawrence2011,
  title = {Variational {{Gaussian Process Dynamical Systems}}},
  author = {Lawrence, Neil D and Titsias, Michalis K and Damianou, Andreas},
  year = {2011},
  pages = {9},
  abstract = {High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2011_Variational_Gaussian_Process_Dynamical_Systems.pdf}
}

@article{lawrence2012,
  title = {A {{Unifying Probabilistic Perspective}} for {{Spectral Dimensionality Reduction}}: {{Insights}} and {{New Models}}},
  author = {Lawrence, Neil D},
  year = {2012},
  pages = {30},
  abstract = {We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.},
  langid = {english},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/lawrence2012.md;/Users/cife/Dropbox/Zotero/Lawrence_2012_A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction.pdf}
}

@article{lawrence2017,
  title = {Living {{Together}}: {{Mind}} and {{Machine Intelligence}}},
  shorttitle = {Living {{Together}}},
  author = {Lawrence, Neil D.},
  year = {2017},
  month = may,
  journal = {arXiv:1705.07996 [cs]},
  eprint = {1705.07996},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we consider the nature of the machine intelligences we have created in the context of our human intelligence. We suggest that the fundamental difference between human and machine intelligence comes down to embodiment factors. We define embodiment factors as the ratio between an entity's ability to communicate information vs compute information. We speculate on the role of embodiment factors in driving our own intelligence and consciousness. We briefly review dual process models of cognition and cast machine intelligence within that framework, characterising it as a dominant System Zero, which can drive behaviour through interfacing with us subconsciously. Driven by concerns about the consequence of such a system we suggest prophylactic courses of action that could be considered. Our main conclusion is that it is not sentient intelligence we should fear but non-sentient intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,grey literature,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Lawrence_2017_Living_Together.pdf}
}

@phdthesis{lebanon2005,
  title = {Riemannian {{Geometry}} and {{Statistical Machine Learning}}},
  author = {Lebanon, Guy},
  year = {2005},
  abstract = {Statistical machine learning algorithms deal with the problem of selecting an appropriate statistical model from a model space {$\Theta$} based on a training set \{xi\}Ni=1 {$\subset$} X or \{(xi,yi)\}Ni=1 {$\subset$} X \texttimes{} Y. In doing so they either implicitly or explicitly make assumptions on the geometries of the model space {$\Theta$} and the data space X. Such assumptions are crucial to the success of the algorithms as different geometries are appropriate for different models and data spaces. By studying these assumptions we are able to develop new theoretical results that enhance our understanding of several popular learning algorithms. Furthermore, using geometrical reasoning we are able to adapt existing algorithms such as radial basis kernels and linear margin classifiers to non-Euclidean geometries. Such adaptation is shown to be useful when the data space does not exhibit Euclidean geometry. In particular, we focus in our experiments on the space of text documents that is naturally associated with the Fisher information metric on corresponding multinomial models.},
  school = {Carnegie Mellon University},
  keywords = {toread},
  file = {/Users/cife/Dropbox/Zotero/Lebanon_2005_Riemannian_Geometry_and_Statistical_Machine_Learning.pdf}
}

@article{lebanon2012,
  title = {Learning {{Riemannian Metrics}}},
  author = {Lebanon, Guy},
  year = {2012},
  month = oct,
  journal = {arXiv:1212.2474 [cs, stat]},
  eprint = {1212.2474},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a solution to the problem of estimating a Riemannian metric associated with a given differentiable manifold. The metric learning problem is based on minimizing the relative volume of a given set of points. We derive the details for a family of metrics on the multinomial simplex. The resulting metric has applications in text classification and bears some similarity to TFIDF representation of text documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Lebanon_2012_Learning_Riemannian_Metrics.pdf;/Users/cife/Zotero/storage/7QWRE2Y8/1212.html}
}

@misc{lecun1999,
  title = {{{MNIST}} Handwritten Digit Database},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
  year = {1999},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  keywords = {data,dataset},
  file = {/Users/cife/Zotero/storage/UDXABHVK/mnist.html}
}

@article{lecun2015,
  ids = {lecunDeepLearning2015a},
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/LeCun_2015_Deep_learning.pdf;/Users/cife/Dropbox/Zotero/LeCun_2015_Deep_learning2.pdf}
}

@misc{lee2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  year = {2018},
  month = mar,
  number = {arXiv:1711.00165},
  eprint = {1711.00165},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.00165},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Lee_2018_Deep_Neural_Networks_as_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/AIR52GXN/1711.html}
}

@inproceedings{lee2019,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  year = {2019},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Fur- thermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Lee_2019_Wide_Neural_Networks_of_Any_Depth_Evolve_as_Linear_Models_Under_Gradient_Descent.pdf;/Users/cife/Zotero/storage/N5U8KHIB/0d1a9651497a38d8b1c3871c84528bd4-Paper.html}
}

@article{lee2020,
  title = {Predicting {{What You Already Know Helps}}: {{Provable Self-Supervised Learning}}},
  shorttitle = {Predicting {{What You Already Know Helps}}},
  author = {Lee, Jason D. and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.01064 [cs, stat]},
  eprint = {2008.01064},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words, yet predicting this \$known\textbackslash{} \$information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks. Formally, we quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Lee_2020_Predicting_What_You_Already_Know_Helps.pdf;/Users/cife/Zotero/storage/YWEBUDFY/2008.html}
}

@book{legall2016,
  title = {Brownian Motion, Martingales, and Stochastic Calculus},
  author = {Le Gall, Jean-Fran{\c c}ois and others},
  year = {2016},
  volume = {274},
  publisher = {{Springer}}
}

@misc{leibfried2021,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2021},
  month = jul,
  number = {arXiv:2012.13962},
  eprint = {2012.13962},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2012.13962},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Leibfried_2021_A_Tutorial_on_Sparse_Gaussian_Processes_and_Variational_Inference.pdf;/Users/cife/Zotero/storage/5YAW5IEV/2012.html}
}

@article{lewandowski2009,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  year = {2009},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.04.008},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276\textendash 294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177\textendash 2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
  langid = {english},
  keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation}
}

@article{leys2013,
  title = {Detecting Outliers: {{Do}} Not Use Standard Deviation around the Mean, Use Absolute Deviation around the Median},
  shorttitle = {Detecting Outliers},
  author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
  year = {2013},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {49},
  number = {4},
  pages = {764--766},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2013.03.013},
  abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.},
  langid = {english},
  keywords = {MAD,Median absolute deviation,Outlier,read-maybe-never,sb},
  file = {/Users/cife/Dropbox/Zotero/Leys_2013_Detecting_outliers.pdf;/Users/cife/Zotero/storage/YFL3SPTV/S0022103113000668.html}
}

@article{li2016,
  ids = {li2016a},
  title = {A Review on {{Gaussian Process Latent Variable Models}}},
  author = {Li, Ping and Chen, Songcan},
  year = {2016},
  month = oct,
  journal = {CAAI Transactions on Intelligence Technology},
  volume = {1},
  number = {4},
  pages = {366--376},
  issn = {24682322},
  doi = {10.1016/j.trit.2016.11.004},
  abstract = {Gaussian Process Latent Variable Model (GPLVM), as a flexible bayesian non-parametric modeling method, has been extensively studied and applied in many learning tasks such as Intrusion Detection, Image Reconstruction, Facial Expression Recognition, Human pose estimation and so on. In this paper, we give a review and analysis for GPLVM and its extensions. Firstly, we formulate basic GPLVM and discuss its relation to Kernel Principal Components Analysis. Secondly, we summarize its improvements or variants and propose a taxonomy of GPLVM related models in terms of the various strategies that be used. Thirdly, we provide the detailed formulations of the main GPLVMs that extensively developed based on the strategies described in the paper. Finally, we further give some challenges in next researches of GPLVM.},
  langid = {english},
  keywords = {bad,Gaussian process,GPLVM,Non-parametric method,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Li_2016_A_review_on_Gaussian_Process_Latent_Variable_Models2.pdf;/Users/cife/Zotero/storage/ALGNM8TR/S2468232216300828.html}
}

@article{li2020,
  title = {Random {{Lie Brackets}} That {{Induce Torsion}}: {{A Model}} for {{Noisy Vector Fields}}},
  shorttitle = {Random {{Lie Brackets}} That {{Induce Torsion}}},
  author = {Li, Didong and Mukherjee, Sayan},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.07309 [math]},
  eprint = {2007.07309},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {We define and study a random Lie bracket that induces torsion in expectation. Almost all stochastic analysis on manifolds have assumed parallel transport. Mathematically this assumption is very reasonable. However, in many applied geometry and graphics problems parallel transport is not achieved, the "change in coordinates" are not exact due to noise. We formulate a stochastic model on a manifold for which parallel transport does not hold and analyze the consequences of this model with respect to classic quantities studied in Riemannian geometry. We first define a stochastic lie bracket that induces a stochastic covariant derivative. We then study the connection implied by the stochastic covariant derivative and note that the stochastic lie bracket induces torsion. We then state the induced stochastic geodesic equations and a stochastic differential equation for parallel transport. We also derive the curvature tensors for our construction and a stochastic Laplace-Beltrami operator. We close with a discussion of the motivation and relevance of our construction.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Differential Geometry,Mathematics - Probability},
  file = {/Users/cife/Dropbox/Zotero/Li_2020_Random_Lie_Brackets_that_Induce_Torsion.pdf;/Users/cife/Zotero/storage/YJZTDUV2/2007.html}
}

@article{liao2017,
  title = {Sharpening {{Jensen}}'s {{Inequality}}},
  author = {Liao, J. G. and Berg, Arthur},
  year = {2017},
  month = oct,
  journal = {arXiv:1707.08644 [math, stat]},
  eprint = {1707.08644},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {This paper proposes a new sharpened version of the Jensen's inequality. The proposed new bound is simple and insightful, is broadly applicable by imposing minimum assumptions, and provides fairly accurate result in spite of its simple form. Applications to the moment generating function, power mean inequalities, and Rao-Blackwell estimation are presented. This presentation can be incorporated in any calculus-based statistical course.},
  archiveprefix = {arXiv},
  keywords = {60E15,Mathematics - Statistics Theory},
  file = {/Users/cife/Zotero/storage/ZSYUYF2C/Liao and Berg - 2017 - Sharpening Jensen's Inequality.pdf;/Users/cife/Zotero/storage/5PLJC6F6/1707.html}
}

@article{linderman2017,
  ids = {linderman2017a},
  title = {Clustering with T-{{SNE}}, Provably},
  author = {Linderman, George C. and Steinerberger, Stefan},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.02582 [cs, stat]},
  eprint = {1706.02582},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and visualization method proposed by van der Maaten \& Hinton in 2008, has rapidly become a standard tool in a number of natural sciences. Despite its overwhelming success, there is a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood. The purpose of this paper is to prove that t-SNE is able to recover well-separated clusters; more precisely, we prove that t-SNE in the `early exaggeration' phase, an optimization technique proposed by van der Maaten \& Hinton (2008) and van der Maaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests novel ways for setting the exaggeration parameter \$\textbackslash alpha\$ and step size \$h\$. Numerical examples illustrate the effectiveness of these rules: in particular, the quality of embedding of topological structures (e.g. the swiss roll) improves. We also discuss a connection to spectral clustering methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,sb,Statistics - Machine Learning,tsne},
  file = {/Users/cife/Dropbox/Zotero/Linderman_2017_Clustering_with_t-SNE,_provably.pdf;/Users/cife/Zotero/storage/ZZD3D89U/1706.html}
}

@article{lindner2019,
  title = {A {{Formalization}} of {{Kant}}'s {{Second Formulation}} of the {{Categorical Imperative}}},
  author = {Lindner, Felix and Bentzen, Martin Mose},
  year = {2019},
  month = jul,
  journal = {arXiv:1801.03160 [cs]},
  eprint = {1801.03160},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a formalization and computational implementation of the second formulation of Kant's categorical imperative. This ethical principle requires an agent to never treat someone merely as a means but always also as an end. Here we interpret this principle in terms of how persons are causally affected by actions. We introduce Kantian causal agency models in which moral patients, actions, goals, and causal influence are represented, and we show how to formalize several readings of Kant's categorical imperative that correspond to Kant's concept of strict and wide duties towards oneself and others. Stricter versions handle cases where an action directly causally affects oneself or others, whereas the wide version maximizes the number of persons being treated as an end. We discuss limitations of our formalization by pointing to one of Kant's cases that the machinery cannot handle in a satisfying way.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,philosophy,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Lindner_2019_A_Formalization_of_Kant's_Second_Formulation_of_the_Categorical_Imperative.pdf;/Users/cife/Zotero/storage/2FMUJQZB/1801.html}
}

@article{liu2018,
  title = {Large-Scale {{CelebFaces Attributes}} ({{CelebA}}) {{Dataset}}},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2018},
  keywords = {data},
  annotation = {itemtype:dataset},
  file = {/Users/cife/Zotero/storage/6RPIRGKP/CelebA.html}
}

@article{liu2020,
  title = {When {{Gaussian Process Meets Big Data}}: {{A Review}} of {{Scalable GPs}}},
  shorttitle = {When {{Gaussian Process Meets Big Data}}},
  author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  year = {2020},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--19},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2957109},
  abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
  keywords = {Big data,Gaussian process regression (GPR),local approximations,read-very-soon,scalability,sparse approximations.},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/liu2020.md;/Users/cife/Dropbox/Zotero/Liu_2020_When_Gaussian_Process_Meets_Big_Data3.pdf;/Users/cife/Zotero/storage/8IDNAAER/8951257.html}
}

@article{livan2018,
  ids = {livan2018a},
  title = {Introduction to {{Random Matrices}} - {{Theory}} and {{Practice}}},
  author = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
  year = {2018},
  journal = {arXiv:1712.07903 [cond-mat, physics:math-ph]},
  volume = {26},
  eprint = {1712.07903},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:math-ph},
  doi = {10.1007/978-3-319-70885-0},
  abstract = {This is a book for absolute beginners. If you have heard about random matrix theory, commonly denoted RMT, but you do not know what that is, then welcome!, this is the place for you. Our aim is to provide a truly accessible introductory account of RMT for physicists and mathematicians at the beginning of their research career. We tried to write the sort of text we would have loved to read when we were beginning Ph.D. students ourselves. Our book is structured with light and short chapters, and the style is informal. The calculations we found most instructive are spelt out in full. Particular attention is paid to the numerical verification of most analytical results. Our book covers standard material - classical ensembles, orthogonal polynomial techniques, spectral densities and spacings - but also more advanced and modern topics - replica approach and free probability - that are not normally included in elementary accounts on RMT. This book is dedicated to the fond memory of Oriol Bohigas.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,read-soon},
  file = {/Users/cife/Dropbox/Zotero/Livan_2018_Introduction_to_Random_Matrices_-_Theory_and_Practice2.pdf;/Users/cife/Zotero/storage/5FTER532/1712.html}
}

@inproceedings{locatello2019,
  title = {On the {{Fairness}} of {{Disentangled Representations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Thomas and Bauer, Stefan and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {14584--14597},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Locatello_2019_On_the_Fairness_of_Disentangled_Representations.pdf;/Users/cife/Zotero/storage/SB6GAYAH/9603-on-the-fairness-of-disentangled-representations.html}
}

@phdthesis{louis2019,
  type = {Theses},
  title = {Computational and Statistical Methods for Trajectory Analysis in a {{Riemannian}} Geometry Setting},
  author = {Louis, Maxime},
  year = {2019},
  month = oct,
  school = {Sorbonnes universit\'es},
  keywords = {Apprentissage,Géométrie riemannienne,Healthcare,Machine learning,Parallel transport,read-maybe-never,Riemannian geometry,Santé,Statistics,Transport parallèle,Vision},
  file = {/Users/cife/Dropbox/Zotero/Louis_2019_Computational_and_statistical_methods_for_trajectory_analysis_in_a_Riemannian.pdf}
}

@inproceedings{lui2018,
  title = {Dimensionality {{Reduction}} Has {{Quantifiable Imperfections}}: {{Two Geometric Bounds}}},
  shorttitle = {Dimensionality {{Reduction}} Has {{Quantifiable Imperfections}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Lui, Kry and Ding, Gavin Weiguang and Huang, Ruitong and McCann, Robert},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8453--8463},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-maybe-never,sb,tsne},
  file = {/Users/cife/Dropbox/Zotero/Lui_2018_Dimensionality_Reduction_has_Quantifiable_Imperfections.pdf;/Users/cife/Zotero/storage/7XXGD2QQ/8065-dimensionality-reduction-has-quantifiable-imperfections-two-geometric-bounds.html}
}

@article{lunnemann2018,
  title = {Gender {{Bias}} in {{Nobel Prizes}}},
  author = {Lunnemann, Per and Jensen, Mogens H. and Jauffred, Liselotte},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.07280 [physics, stat]},
  eprint = {1810.07280},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {Strikingly few Nobel laureates within medicine, natural and social sciences are women. Although it is obvious that there are fewer women researchers within these fields, does this gender ratio still fully account for the low number of female Nobel laureates? We examine whether women are awarded the Nobel Prizes less often than the gender ratio suggests. Based on historical data across four scientific fields and a Bayesian hierarchical model, we quantify any possible bias. The model reveals, with exceedingly large confidence, that indeed women are strongly under-represented among Nobel laureates across all disciplines examined.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries,grey literature,Physics - Physics and Society,read,Statistics - Applications},
  file = {/Users/cife/Dropbox/Zotero/Lunnemann_2018_Gender_Bias_in_Nobel_Prizes.pdf}
}

@article{ma2015,
  title = {The {{Chicago Face Database}}: {{A Free Stimulus Set}} of {{Faces}} and {{Norming Data}}},
  author = {Ma, Debbie S. and Correll, Joshua and Wittenbrink, Bernd},
  year = {2015},
  journal = {Behavior Research Methods},
  volume = {47},
  pages = {1122--1135},
  abstract = {The CFD is intended for use in scientific research. It provides high-resolution, standardized photographs of male and female faces of varying ethnicity between the ages of 17-65. Extensive norming data are available for each individual model. These data include both physical attributes (e.g., face size) as well as subjective ratings by independent judges (e.g., attractiveness).},
  copyright = {The CFD is a free resource for the scientific community. The database photographs and their accompanying information may be used free of charge for non-commercial research purposes only. The database materials cannot be re-distributed or published without written consent from the copyright holder, the University of Chicago, Center for Decision Research.},
  langid = {american},
  keywords = {data},
  annotation = {itemtype:dataset},
  file = {/Users/cife/Zotero/storage/DU87QHI4/download.html}
}

@inproceedings{malisiewicz2008,
  title = {Recognition by Association via Learning Per-Exemplar Distances},
  booktitle = {2008 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Malisiewicz, Tomasz and Efros, Alexei A.},
  year = {2008},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/CVPR.2008.4587462},
  abstract = {We pose the recognition problem as data association. In this setting, a novel object is explained solely in terms of a small set of exemplar objects to which it is visually similar. Inspired by the work of Frome et al., we learn separate distance functions for each exemplar; however, our distances are interpretable on an absolute scale and can be thresholded to detect the presence of an object. Our exemplars are represented as image regions and the learned distances capture the relative importance of shape, color, texture, and position features for that region. We use the distance functions to detect and segment objects in novel images by associating the bottom-up segments obtained from multiple image segmentations with the exemplar regions. We evaluate the detection and segmentation performance of our algorithm on real-world outdoor scenes from the LabelMe [15] dataset and also show some promising qualitative image parsing results.},
  isbn = {978-1-4244-2242-5},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Malisiewicz_2008_Recognition_by_association_via_learning_per-exemplar_distances.pdf;/Users/cife/Dropbox/Zotero/Malisiewicz_2008_Recognition_by_association_via_learning_per-exemplar_distances2.pdf}
}

@inproceedings{mallasto2019a,
  title = {Probabilistic {{Riemannian}} Submanifold Learning with Wrapped {{Gaussian}} Process Latent Variable Models},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Mallasto, Anton and Hauberg, S{\o}ren and Feragen, Aasa},
  year = {2019},
  month = apr,
  pages = {2368--2377},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Latent variable models (LVMs) learn probabilistic models of data manifolds lying in an ambient Euclidean space. In a number of applications, a priori known spatial constraints can shrink the ambient space into a considerably smaller manifold.  Additionally, in these applications the Euclidean geometry might induce a suboptimal similarity measure, which could be improved by choosing a different metric. Euclidean models ignore such information and assign probability mass to data points that can never appear as data, and vastly different likelihoods to points that are similar under the desired metric.We propose the wrapped Gaussian process latent variable model (WGPLVM), that extends Gaussian process latent variable models to take values strictly on a given Riemannian manifold, making the model blind to impossible data points. This allows non-linear, probabilistic inference of low-dimensional Riemannian submanifolds from data. Our evaluation on diverse datasets show that we improve performance on several tasks, including encoding, visualization and uncertainty quantification.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Mallasto_2019_Probabilistic_Riemannian_submanifold_learning_with_wrapped_Gaussian_process2.pdf;/Users/cife/Zotero/storage/NFBWWW4H/Mallasto et al. - 2019 - Probabilistic Riemannian submanifold learning with.pdf}
}

@article{mardia1979,
  title = {{{JT Kent}}. and {{J}}. {{M}}. {{Bibby}}},
  author = {Mardia, KV},
  year = {1979},
  journal = {Multivariate Analysis}
}

@book{mardia2000,
  title = {Directional Statistics},
  author = {Mardia, Kanti V and Jupp, Peter E and Mardia, KV},
  year = {2000},
  volume = {2},
  publisher = {{Wiley Online Library}}
}

@article{martens2014,
  title = {New Insights and Perspectives on the Natural Gradient Method},
  author = {Martens, James},
  year = {2014},
  month = dec,
  abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used "empirical" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature, but notably not the Hessian).},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Martens_2014_New_insights_and_perspectives_on_the_natural_gradient_method.pdf;/Users/cife/Zotero/storage/HWMDPZYD/1412.html}
}

@article{martens2019,
  title = {Decomposing Feature-Level Variation with {{Covariate Gaussian Process Latent Variable Models}}},
  author = {M{\"a}rtens, Kaspar and Campbell, Kieran R. and Yau, Christopher},
  year = {2019},
  month = jun,
  journal = {arXiv:1810.06983 [cs, stat]},
  eprint = {1810.06983},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian Process Regression (GPR) and Gaussian Process Latent Variable Models (GPLVM) offer a principled way of performing probabilistic non-linear regression and dimensionality reduction. In this paper we propose a hybrid between the two, the covariate-GPLVM (c-GPLVM), to perform dimensionality reduction in the presence of covariate information (e.g. continuous covariates, class labels, or censored survival). This construction lets us adjust for covariate effects and reveals meaningful latent structure which is not revealed when using GPLVM. Furthermore, we introduce structured decomposable kernels which will let us interpret how the fixed and latent inputs contribute to feature-level variation, e.g. identify the presence of a non-linear interaction. We demonstrate the utility of this model on applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Märtens_2019_Decomposing_feature-level_variation_with_Covariate_Gaussian_Process_Latent.pdf}
}

@inproceedings{martens2019a,
  title = {Decomposing Feature-Level Variation with {{Covariate Gaussian Process Latent Variable Models}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {M{\"a}rtens, Kaspar and Campbell, Kieran and Yau, Christopher},
  year = {2019},
  month = may,
  pages = {4372--4381},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The interpretation of complex high-dimensional data typically requires the use of dimensionality reduction techniques to extract explanatory low-dimensional representations. However, in many real-world problems these representations may not be sufficient to aid interpretation on their own, and it would be desirable to interpret the model in terms of the original features themselves. Our goal is to characterise how feature-level variation depends on latent low-dimensional representations, external covariates, and non-linear interactions between the two. In this paper, we propose to achieve this through a structured kernel decomposition in a hybrid Gaussian Process model which we call the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We demonstrate the utility of our model on simulated examples and applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes. In each setting we show how the c-GPLVM can extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not present in other commonly used dimensionality reduction approaches.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Märtens_2019_Decomposing_feature-level_variation_with_Covariate_Gaussian_Process_Latent2.pdf;/Users/cife/Zotero/storage/ESE4QDSV/Märtens et al. - 2019 - Decomposing feature-level variation with Covariate.pdf}
}

@article{martens2020,
  title = {New Insights and Perspectives on the Natural Gradient Method},
  author = {Martens, James},
  year = {2020},
  month = sep,
  journal = {arXiv:1412.1193 [cs, stat]},
  eprint = {1412.1193},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used ``empirical'' approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Zotero/storage/JX2YDNYD/Martens - 2020 - New insights and perspectives on the natural gradi.pdf}
}

@article{martino2018,
  title = {Gaussian {{Process Deep Belief Networks}}: {{A Smooth Generative Model}} of   {{Shape}} with {{Uncertainty Propagation}}},
  shorttitle = {Gaussian {{Process Deep Belief Networks}}},
  author = {Martino, Alessandro Di and Bodin, Erik and Ek, Carl Henrik and Campbell, Neill D. F.},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.05477v1 [stat]},
  eprint = {1812.05477v1},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The shape of an object is an important characteristic for many vision problems such as segmentation, detection and tracking. Being independent of appearance, it is possible to generalize to a large range of objects from only small amounts of data. However, shapes represented as silhouette images are challenging to model due to complicated likelihood functions leading to intractable posteriors. In this paper we present a generative model of shapes which provides a low dimensional latent encoding which importantly resides on a smooth manifold with respect to the silhouette images. The proposed model propagates uncertainty in a principled manner allowing it to learn from small amounts of data and providing predictions with associated uncertainty. We provide experiments that show how our proposed model provides favorable quantitative results compared with the state-of-the-art while simultaneously providing a representation that resides on a low-dimensional interpretable manifold.},
  archiveprefix = {arXiv},
  keywords = {read-someday},
  file = {/Users/cife/Zotero/storage/5UEKLN75/1806.html}
}

@article{matthews2016,
  title = {On Sparse Variational Methods and the {{Kullback-Leibler}} Divergence between Stochastic Processes},
  author = {Matthews, Alexander G. de G. and Hensman, James and Turner, Richard E and Ghahramani, Zoubin},
  year = {2016},
  journal = {Artificial Intelligence and Statistics},
  pages = {pp. 231-239},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Matthews_2016_On_sparse_variational_methods_and_the_Kullback-Leibler_divergence_between.pdf}
}

@phdthesis{matthews2017,
  type = {Thesis},
  title = {Scalable {{Gaussian}} Process Inference Using Variational Methods},
  author = {Matthews, Alexander Graeme de Garis},
  year = {2017},
  month = mar,
  doi = {10.17863/CAM.25348},
  abstract = {Gaussian processes can be used as priors on functions. The need for a flexible, principled, probabilistic model of functional relations is common in practice. Consequently, such an approach is demonstrably useful in a large variety of applications. Two challenges of Gaussian process modelling are often encountered. These are dealing with the adverse scaling with the number of data points and the lack of closed form posteriors when the likelihood is non-Gaussian. In this thesis, we study variational inference as a framework for meeting these challenges. An introductory chapter motivates the use of stochastic processes as priors, with a particular focus on Gaussian process modelling. A section on variational inference reviews the general definition of Kullback-Leibler divergence. The concept of prior conditional matching that is used throughout the thesis is contrasted to classical approaches to obtaining tractable variational approximating families. Various theoretical issues arising from the application of variational inference to the infinite dimensional Gaussian process setting are settled decisively. From this theory we are able to give a new argument for existing approaches to variational regression that settles debate about their applicability. This view on these methods justifies the principled extensions found in the rest of the work. The case of scalable Gaussian process classification is studied, both for its own merits and as a case study for non-Gaussian likelihoods in general. Using the resulting algorithms we find credible results on datasets of a scale and complexity that was not possible before our work. An extension to include Bayesian priors on model hyperparameters is studied alongside a new inference method that combines the benefits of variational sparsity and MCMC methods. The utility of such an approach is shown on a variety of example modelling tasks. We describe GPflow, a new Gaussian process software library that uses TensorFlow. Implementations of the variational algorithms discussed in the rest of the thesis are included as part of the software. We discuss the benefits of GPflow when compared to other similar software. Increased computational speed is demonstrated in relevant, timed, experimental comparisons.},
  copyright = {All rights reserved},
  langid = {english},
  school = {University of Cambridge},
  keywords = {read-sooon},
  annotation = {Accepted: 2018-07-11T11:34:22Z},
  file = {/Users/cife/Dropbox/Zotero/Matthews_2017_Scalable_Gaussian_process_inference_using_variational_methods.pdf;/Users/cife/Zotero/storage/2M4RANE2/278022.html}
}

@article{mcallester,
  title = {{{TTIC}} 31230, {{Fundamentals}} of {{Deep Learning}}},
  author = {McAllester, David},
  pages = {11},
  langid = {english},
  file = {/Users/cife/Zotero/storage/SH556CDD/McAllester - TTIC 31230, Fundamentals of Deep Learning.pdf}
}

@article{mchutchon,
  title = {Differentiating {{Gaussian Processes}}},
  author = {McHutchon, Andrew},
  pages = {8},
  langid = {english},
  file = {/Users/cife/Zotero/storage/TVMRZWP4/McHutchon - Diﬀerentiating Gaussian Processes.pdf}
}

@article{mchutchon2013,
  title = {Differentiating {{Gaussian Processes}}},
  author = {McHutchon, Andrew},
  year = {2013},
  pages = {8},
  langid = {english},
  keywords = {reference},
  file = {/Users/cife/Dropbox/Zotero/McHutchon_2013_Diﬀerentiating_Gaussian_Processes.pdf}
}

@article{mcinnes2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2018},
  month = dec,
  journal = {arXiv:1802.03426 [cs, stat]},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,read-maybe-never,sb,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/McInnes_2018_UMAP.pdf;/Users/cife/Dropbox/Zotero/McInnes_2020_UMAP.pdf;/Users/cife/Zotero/storage/V9NW65BX/1802.html}
}

@article{mcinnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  journal = {arXiv:1802.03426 [cs, stat]},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/McInnes_2020_UMAP2.pdf;/Users/cife/Zotero/storage/B4WT5JZZ/1802.html}
}

@article{meanti2020,
  title = {Kernel Methods through the Roof: Handling Billions of Points Efficiently},
  shorttitle = {Kernel Methods through the Roof},
  author = {Meanti, Giacomo and Carratino, Luigi and Rosasco, Lorenzo and Rudi, Alessandro},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.10350 [cs, stat]},
  eprint = {2006.10350},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Kernel methods provide an elegant and principled approach to nonparametric learning, but so far could hardly be used in large scale problems, since na\textbackslash "ive implementations scale poorly with data size. Recent advances have shown the benefits of a number of algorithmic ideas, for example combining optimization, numerical linear algebra and random projections. Here, we push these efforts further to develop and test a solver that takes full advantage of GPU hardware. Towards this end, we designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization. Further, we optimize the numerical precision of different operations and maximize efficiency of matrix-vector multiplications. As a result we can experimentally show dramatic speedups on datasets with billions of points, while still guaranteeing state of the art performance. Additionally, we make our software available as an easy to use library.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Meanti_2020_Kernel_methods_through_the_roof.pdf;/Users/cife/Zotero/storage/WM7DFS9N/2006.html}
}

@article{meaux,
  title = {{{COLOR IN SCIENTIFIC FIGURES}}},
  author = {Meaux, Stacie},
  pages = {5},
  langid = {english},
  file = {/Users/cife/Zotero/storage/5P8S4FBN/Meaux - COLOR IN SCIENTIFIC FIGURES.pdf}
}

@inproceedings{meier2014,
  title = {Incremental {{Local Gaussian Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Meier, Franziska and Hennig, Philipp and Schaal, Stefan},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/cife/Dropbox/Zotero/Meier_2014_Incremental_Local_Gaussian_Regression.pdf}
}

@article{meng2021,
  title = {Regularized {{Sparse Gaussian Processes}}},
  author = {Meng, Rui and Lee, Herbert and Braden, Soper and Ray, Priyadip},
  year = {2021},
  month = may,
  journal = {arXiv:1910.05843 [cs, stat]},
  eprint = {1910.05843},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes are a flexible Bayesian nonparametric modelling approach that has been widely applied but poses computational challenges. To address the poor scaling of exact inference methods, approximation methods based on sparse Gaussian processes (SGP) are attractive. An issue faced by SGP, especially in latent variable models, is the inefficient learning of the inducing inputs, which leads to poor model prediction. We propose a regularization approach by balancing the reconstruction performance of data and the approximation performance of the model itself. This regularization improves both inference and prediction performance. We extend this regularization approach into latent variable models with SGPs and show that performing variational inference (VI) on those models is equivalent to performing VI on a related empirical Bayes model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Meng_2021_Regularized_Sparse_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/EXC626LF/1910.html}
}

@article{mohamed,
  title = {Planting the {{Seeds}} of {{Probabilistic Thinking}}},
  author = {Mohamed, Shakir},
  pages = {118},
  langid = {english},
  file = {/Users/cife/Zotero/storage/WIEC9XK2/Mohamed - Planting the Seeds of Probabilistic Thinking.pdf}
}

@article{mohameda,
  title = {Planting the {{Seeds}} of {{Probabilistic Thinking}}},
  author = {Mohamed, Shakir},
  pages = {118},
  langid = {english},
  file = {/Users/cife/Zotero/storage/N2JLBUEY/Mohamed - Planting the Seeds of Probabilistic Thinking.pdf}
}

@inproceedings{moll2004,
  title = {Path Planning for Minimal Energy Curves of Constant Length},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}, 2004. {{Proceedings}}. {{ICRA}} '04. 2004},
  author = {Moll, M. and Kavraki, L.E.},
  year = {2004},
  month = apr,
  volume = {3},
  pages = {2826-2831 Vol.3},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2004.1307489},
  abstract = {In this paper we present a new path planning technique for a flexible wire. We first introduce a new parametrization designed to represent low-energy configurations. Based on this parametrization we can find curves that satisfy endpoint constraints. Next, we present three different techniques for minimizing energy within the self-motion manifold of the curve. We introduce a local planner to find smooth minimal energy deformations for these curves that can be used by a general path planning algorithm. Using a simplified model for obstacles, we can find minimal energy curves of fixed length that pass through specified tangents at given control points. Finally, we show that the parametrization introduced in this paper is a good approximation of true minimal energy curves. Our work has applications in surgical suturing and snake-like robots.},
  keywords = {Cables,Capacitive sensors,Computer science,flexible wire,low-energy configurations,minimal energy curves,Orbital robotics,path planning,Path planning,read-someday,Robot sensing systems,robots,self-motion manifold,Shape,Surgery,Wire},
  file = {/Users/cife/Dropbox/Zotero/Moll_2004_Path_planning_for_minimal_energy_curves_of_constant_length.pdf;/Users/cife/Zotero/storage/CQ4YU4VK/1307489.html}
}

@misc{moreno-munoz2020,
  title = {Notes for {{Geometry}} in {{Gaussian Processes}}.Pdf},
  author = {{Moreno-Mu{\~n}oz}, Pablo},
  year = {2020},
  file = {/Users/cife/Zotero/storage/K8BXUD7L/notes_geometryGP.pdf}
}

@misc{moreno-munoz2022,
  title = {Revisiting {{Active Sets}} for {{Gaussian Process Decoders}}},
  author = {{Moreno-Mu{\~n}oz}, Pablo and Feldager, Cilie W. and Hauberg, S{\o}ren},
  year = {2022},
  month = sep,
  number = {arXiv:2209.04636},
  eprint = {2209.04636},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.04636},
  abstract = {Decoders built on Gaussian processes (GPs) are enticing due to the marginalisation over the non-linear function space. Such models (also known as GP-LVMs) are often expensive and notoriously difficult to train in practice, but can be scaled using variational inference and inducing points. In this paper, we revisit active set approximations. We develop a new stochastic estimate of the log-marginal likelihood based on recently discovered links to cross-validation, and propose a computationally efficient approximation thereof. We demonstrate that the resulting stochastic active sets (SAS) approximation significantly improves the robustness of GP decoder training while reducing computational cost. The SAS-GP obtains more structure in the latent space, scales to many datapoints and learns better representations than variational autoencoders, which is rarely the case for GP decoders.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Moreno-Muñoz_2022_Revisiting_Active_Sets_for_Gaussian_Process_Decoders.pdf;/Users/cife/Zotero/storage/66RLVWLT/2209.html}
}

@book{muirhead1982,
  title = {Aspects of {{Multivariate Statistical Theory}}},
  author = {Muirhead, Robb J},
  year = {1982},
  publisher = {{John Wiley \& Sons, Inc.}},
  isbn = {978-0-471-09442-5},
  langid = {english},
  keywords = {read-soon,reference,wishart},
  annotation = {\{DOI:10.1002/9780470316559\}},
  file = {/Users/cife/Dropbox/Zotero/Muirhead_Aspects_of_Multivariate_Statistical_Theory.pdf}
}

@book{munzner2014,
  title = {Visualization {{Analysis}} and {{Design}}},
  author = {Munzner, Tamara},
  year = {2014},
  month = dec,
  publisher = {{CRC Press}},
  abstract = {Learn How to Design Effective Visualization SystemsVisualization Analysis and Design provides a systematic, comprehensive framework for thinking about visualization in terms of principles and design choices. The book features a unified approach encompassing information visualization techniques for abstract data, scientific visualization techniques},
  googlebooks = {NfkYCwAAQBAJ},
  isbn = {978-1-4987-5971-7},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Computers / General,Computers / Human-Computer Interaction (HCI),Computers / Programming / Games,Computers / Software Development \& Engineering / Computer Graphics,Technology \& Engineering / Imaging Systems}
}

@inproceedings{murphy1999,
  title = {Loopy {{Belief Propagation}} for {{Approximate Inference}}: {{An Empirical Study}}},
  booktitle = {{{UAI}}},
  author = {Murphy, Kevin P. and Weiss, Yair and Jordan, Michael I.},
  year = {1999},
  file = {/Users/cife/Zotero/storage/9F2JA5RZ/loopy_uai99.pdf}
}

@incollection{murray2009,
  title = {The {{Gaussian Process Density Sampler}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  author = {Murray, Iain and MacKay, David and Adams, Ryan P},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  pages = {9--16},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Murray_2009_The_Gaussian_Process_Density_Sampler.pdf;/Users/cife/Zotero/storage/JVFWWC52/3410-the-gaussian-process-density-sampler.html}
}

@article{murray2011,
  title = {Skillful Writing of an Awful Research Paper},
  author = {Murray, Royce},
  editor = {Murray, Royce},
  year = {2011},
  month = feb,
  journal = {Analytical Chemistry},
  volume = {83},
  number = {3},
  pages = {633--633},
  issn = {0003-2700, 1520-6882},
  doi = {10.1021/ac2000169},
  langid = {english},
  keywords = {grey literature,read},
  file = {/Users/cife/Dropbox/Zotero/Murray_2011_Skillful_writing_of_an_awful_research_paper.pdf}
}

@article{murray2018,
  title = {Mixed {{Likelihood Gaussian Process Latent Variable Model}}},
  author = {Murray, Samuel and Kjellstr{\"o}m, Hedvig},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.07627 [cs, stat]},
  eprint = {1811.07627},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the Mixed Likelihood Gaussian process latent variable model (GP-LVM), capable of modeling data with attributes of different types. The standard formulation of GP-LVM assumes that each observation is drawn from a Gaussian distribution, which makes the model unsuited for data with e.g. categorical or nominal attributes. Our model, for which we use a sampling based variational inference, instead assumes a separate likelihood for each observed dimension. This formulation results in more meaningful latent representations, and give better predictive performance for real world data with dimensions of different types.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Murray_2018_Mixed_Likelihood_Gaussian_Process_Latent_Variable_Model.pdf}
}

@article{murray2018a,
  title = {Mixed {{Likelihood Gaussian Process Latent Variable Model}}},
  author = {Murray, Samuel and Kjellstr{\"o}m, Hedvig},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.07627 [cs, stat]},
  eprint = {1811.07627},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the Mixed Likelihood Gaussian process latent variable model (GP-LVM), capable of modeling data with attributes of different types. The standard formulation of GP-LVM assumes that each observation is drawn from a Gaussian distribution, which makes the model unsuited for data with e.g. categorical or nominal attributes. Our model, for which we use a sampling based variational inference, instead assumes a separate likelihood for each observed dimension. This formulation results in more meaningful latent representations, and give better predictive performance for real world data with dimensions of different types.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Murray_2018_Mixed_Likelihood_Gaussian_Process_Latent_Variable_Model2.pdf;/Users/cife/Zotero/storage/JEM9PXV2/1811.html}
}

@inproceedings{nair2010,
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  booktitle = {{{ICML}}},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  year = {2010},
  pages = {8},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ``Stepped Sigmoid Units'' are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/2RP8ZVCI/Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf}
}

@inproceedings{naish-guzman2008,
  title = {The {{Generalized FITC Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Naish-guzman}, Andrew and Holden, Sean},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-someday,scalable-gp},
  file = {/Users/cife/Dropbox/Zotero/Naish-guzman_2008_The_Generalized_FITC_Approximation.pdf}
}

@incollection{nakagami1960,
  title = {The M-Distribution\textemdash{{A}} General Formula of Intensity Distribution of Rapid Fading},
  booktitle = {Statistical Methods in Radio Wave Propagation},
  author = {Nakagami, Minoru},
  year = {1960},
  pages = {3--36},
  publisher = {{Elsevier}}
}

@article{neiswanger,
  title = {Lecture 13 : {{Variational Inference}}: {{Mean Field Approximation}}},
  author = {Neiswanger, Willie and Tong, Xupeng and Liu, Minxing},
  pages = {13},
  langid = {english},
  file = {/Users/cife/Zotero/storage/KT2WXFAR/Neiswanger et al. - Lecture 13  Variational Inference Mean Field App.pdf}
}

@misc{nene1996,
  title = {{{CAVE}} | {{Software}}: {{COIL-100}}: {{Columbia Object Image Library}}},
  shorttitle = {Technical {{Report CUCS-006-96}}},
  author = {Nene, S. A. and Nayar, S. K. and Murase, H.},
  year = {1996},
  month = feb,
  howpublished = {https://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php},
  keywords = {data},
  annotation = {itemtype:dataset},
  file = {/Users/cife/Zotero/storage/L9IL347B/coil-100.html}
}

@article{nene1996a,
  title = {Columbia {{Object Image Library}} ({{COIL-20}})},
  author = {Nene, Sameer A and Nayar, Shree K and Murase, Hiroshi},
  year = {1996},
  pages = {6},
  langid = {english},
  keywords = {data},
  file = {/Users/cife/Zotero/storage/RYKI4WMZ/Nene et al. - Columbia Object Image Library (COIL-20).pdf}
}

@book{newport2016,
  title = {Deep {{Work}}: {{Rules}} for {{Focused Success}} in a {{Distracted World}}},
  shorttitle = {Deep {{Work}}},
  author = {Newport, Cal},
  year = {2016},
  month = jan,
  publisher = {{Piatkus}},
  abstract = {One of the most valuable skills in our economy is becoming increasingly rare. If you master this skill, you'll achieve extraordinary results.Deep Work is an indispensable guide to anyone seeking focused success in a distracted world.'Deep work' is the ability to focus without distraction on a cognitively demanding task. Coined by author and professor Cal Newport on his popular blog Study Hacks, deep work will make you better at what you do, let you achieve more in less time and provide the sense of true fulfilment that comes from the mastery of a skill. In short, deep work is like a superpower in our increasingly competitive economy.And yet most people, whether knowledge workers in noisy open-plan offices or creatives struggling to sharpen their vision, have lost the ability to go deep - spending their days instead in a frantic blur of email and social media, not even realising there's a better way.A mix of cultural criticism and actionable advice, DEEP WORK takes the reader on a journey through memorable stories -- from Carl Jung building a stone tower in the woods to focus his mind, to a social media pioneer buying a round-trip business class ticket to Tokyo to write a book free from distraction in the air -- and surprising suggestions, such as the claim that most serious professionals should quit social media and that you should practice being bored. Put simply: developing and cultivating a deep work practice is one of the best decisions you can make in an increasingly distracted world and this book will point the way.},
  langid = {english},
  keywords = {book}
}

@article{nickisch2010,
  title = {Gaussian {{Mixture Modeling}} with {{Gaussian Process Latent Variable Models}}},
  author = {Nickisch, Hannes and Rasmussen, Carl Edward},
  year = {2010},
  month = jul,
  journal = {arXiv:1006.3640 [stat]},
  eprint = {1006.3640},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Density modeling is notoriously difficult for high dimensional data. One approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data. Recently, the Gaussian Process Latent Variable Model (GPLVM) has successfully been used to find low dimensional manifolds in a variety of complex data. The GPLVM consists of a set of points in a low dimensional latent space, and a stochastic map to the observed space. We show how it can be interpreted as a density model in the observed space. However, the GPLVM is not trained as a density model and therefore yields bad density estimates. We propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Nickisch_2010_Gaussian_Mixture_Modeling_with_Gaussian_Process_Latent_Variable_Models.pdf;/Users/cife/Dropbox/Zotero/Nickisch_2010_Gaussian_Mixture_Modeling_with_Gaussian_Process_Latent_Variable_Models2.pdf}
}

@incollection{nielsen2013,
  title = {Pattern {{Learning}} and {{Recognition}} on {{Statistical Manifolds}}: {{An Information-Geometric Review}}},
  shorttitle = {Pattern {{Learning}} and {{Recognition}} on {{Statistical Manifolds}}},
  booktitle = {Similarity-{{Based Pattern Recognition}}},
  author = {Nielsen, Frank},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hancock, Edwin and Pelillo, Marcello},
  year = {2013},
  volume = {7953},
  pages = {1--25},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39140-8_1},
  abstract = {We review the information-geometric framework for statistical pattern recognition: First, we explain the role of statistical similarity measures and distances in fundamental statistical pattern recognition problems. We then concisely review the main statistical distances and report a novel versatile family of divergences. Depending on their intrinsic complexity, the statistical patterns are learned by either atomic parametric distributions, semi-parametric finite mixtures, or non-parametric kernel density distributions. Those statistical patterns are interpreted and handled geometrically in statistical manifolds either as single points, weighted sparse point sets or non-weighted dense point sets. We explain the construction of the two prominent families of statistical manifolds: The Rao Riemannian manifolds with geodesic metric distances, and the Amari-Chentsov manifolds with dual asymmetric non-metric divergences. For the latter manifolds, when considering atomic distributions from the same exponential families (including the ubiquitous Gaussian and multinomial families), we end up with dually flat exponential family manifolds that play a crucial role in many applications. We compare the advantages and disadvantages of these two approaches from the algorithmic point of view. Finally, we conclude with further perspectives on how ``geometric thinking'' may spur novel pattern modeling and processing paradigms.},
  isbn = {978-3-642-39139-2 978-3-642-39140-8},
  langid = {english},
  file = {/Users/cife/Zotero/storage/ISYF2CQX/Nielsen - 2013 - Pattern Learning and Recognition on Statistical Ma.pdf}
}

@article{nielsen2022,
  title = {The {{Many Faces}} of {{Information Geometry}}},
  author = {Nielsen, Frank},
  year = {2022},
  month = jan,
  journal = {Notices of the American Mathematical Society},
  volume = {69},
  number = {01},
  pages = {1},
  issn = {0002-9920, 1088-9477},
  doi = {10.1090/noti2403},
  langid = {english},
  file = {/Users/cife/Zotero/storage/ESAPN6GG/Nielsen - 2022 - The Many Faces of Information Geometry.pdf}
}

@article{nirwan2019,
  title = {Rotation {{Invariant Householder Parameterization}} for {{Bayesian PCA}}},
  author = {Nirwan, Rajbir S. and Bertschinger, Nils},
  year = {2019},
  month = may,
  journal = {arXiv:1905.04720 [cs, stat]},
  eprint = {1905.04720},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We consider probabilistic PCA and related factor models from a Bayesian perspective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to complicated posterior distributions with continuous subspaces of equal density and thus hinders efficiency of inference as well as interpretation of obtained parameters. In particular, posterior averages over factor loadings become meaningless and only model predictions are unambiguous. Here, we propose a parameterization based on Householder transformations, which remove the rotational symmetry of the posterior. Furthermore, by relying on results from random matrix theory, we establish the parameter distribution which leaves the model unchanged compared to the original rotationally symmetric formulation. In particular, we avoid the need to compute the Jacobian determinant of the parameter transformation. This allows us to efficiently implement probabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we implemented our model in the probabilistic programming language Stan and illustrate it on several examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,to read},
  file = {/Users/cife/Dropbox/Zotero/Nirwan_2019_Rotation_Invariant_Householder_Parameterization_for_Bayesian_PCA.pdf}
}

@book{nishizeki1988,
  title = {Planar {{Graphs}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Nishizeki, {{T}}., {{N}}. {{Chiba}}},
  author = {Nishizeki, T. and Chiba, N.},
  year = {1988},
  volume = {32},
  publisher = {{Annals of Discrete Mathematics}},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.4710320616},
  file = {/Users/cife/Zotero/storage/N6YTSYJI/bimj.html}
}

@article{niu2018,
  title = {Intrinsic {{Gaussian}} Processes on Complex Constrained Domains},
  author = {Niu, Mu and Cheung, Pokman and Lin, Lizhen and Dai, Zhenwen and Lawrence, Neil and Dunson, David},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.01061 [cs, stat]},
  eprint = {1801.01061},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a class of intrinsic Gaussian processes (in-GPs) for interpolation, regression and classification on manifolds with a primary focus on complex constrained domains or irregular shaped spaces arising as subsets or submanifolds of R, R2, R3 and beyond. For example, in-GPs can accommodate spatial domains arising as complex subsets of Euclidean space. in-GPs respect the potentially complex boundary or interior conditions as well as the intrinsic geometry of the spaces. The key novelty of the proposed approach is to utilise the relationship between heat kernels and the transition density of Brownian motion on manifolds for constructing and approximating valid and computationally feasible covariance kernels. This enables in-GPs to be practically applied in great generality, while existing approaches for smoothing on constrained domains are limited to simple special cases. The broad utilities of the in-GP approach is illustrated through simulation studies and data examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Niu_2018_Intrinsic_Gaussian_processes_on_complex_constrained_domains.pdf;/Users/cife/Zotero/storage/S9RUPTBR/1801.html}
}

@article{noether1918,
  title = {Invariante {{Variationsprobleme}}},
  author = {Noether, Amalie Emmy},
  year = {1918},
  journal = {Nachrichten von der Gesellschaft der Wissenschaften zu G\"ottingen, Mathematisch-Physikalische Klasse},
  pages = {235--257},
  keywords = {read,sb}
}

@article{nydick,
  title = {The {{Wishart}} and {{Inverse Wishart Distributions}}},
  author = {Nydick, Steven W},
  pages = {19},
  langid = {english},
  file = {/Users/cife/Zotero/storage/7QI9QJVT/Nydick - The Wishart and Inverse Wishart Distributions.pdf}
}

@book{o2006metric,
  title = {Metric Spaces},
  author = {O'Searcoid, M{\'i}che{\'a}l},
  year = {2006},
  publisher = {{Springer Science \& Business Media}}
}

@article{oates2019,
  title = {A {{Modern Retrospective}} on {{Probabilistic Numerics}}},
  author = {Oates, C. J. and Sullivan, T. J.},
  year = {2019},
  month = nov,
  journal = {Statistics and Computing},
  volume = {29},
  number = {6},
  eprint = {1901.04457},
  eprinttype = {arxiv},
  pages = {1335--1351},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-019-09902-z},
  abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
  archiveprefix = {arXiv},
  keywords = {62-03; 65-03; 01A60; 01A65; 01A67,Mathematics - History and Overview,Mathematics - Numerical Analysis,Mathematics - Probability,philosophy,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Oates_2019_A_Modern_Retrospective_on_Probabilistic_Numerics.pdf;/Users/cife/Zotero/storage/HSWWINHS/1901.html}
}

@incollection{oksendal2003,
  title = {Stochastic {{Differential Equations}}},
  booktitle = {Stochastic {{Differential Equations}}: {{An Introduction}} with {{Applications}}},
  author = {{\O}ksendal, Bernt},
  editor = {{\O}ksendal, Bernt},
  year = {2003},
  series = {Universitext},
  pages = {65--84},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14394-6_5},
  abstract = {We now return to the possible solutions X t ({$\omega$}) of the stochastic differential equation.},
  isbn = {978-3-642-14394-6},
  langid = {english},
  keywords = {Brownian Bridge,Brownian Motion,Gronwall Inequality,read-maybe-never,reference,Strong Solution,Weak Solution}
}

@incollection{oller1993,
  title = {On an Intrinsic Analysis of Statistical Estimation},
  booktitle = {Multivariate Analysis: {{Future}} Directions 2},
  author = {Oller, Josep M},
  year = {1993},
  pages = {421--437},
  publisher = {{Elsevier}}
}

@article{ollivier2017,
  title = {Information-{{Geometric Optimization Algorithms}}: {{A Unifying Picture}} via {{Invariance Principles}}},
  author = {Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus},
  year = {2017},
  pages = {65},
  abstract = {We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space {$\mathsl{X}$} into a continuous-time black-box optimization method on {$\mathsl{X}$}, the information-geometric optimization (IGO) method. Invariance as a major design principle keeps the number of arbitrary choices to a minimum. The resulting IGO flow is the flow of an ordinary differential equation conducting the natural gradient ascent of an adaptive, time-dependent transformation of the objective function. It makes no particular assumptions on the objective function to be optimized.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/UQVVWCF7/Ollivier et al. - Information-Geometric Optimization Algorithms A U.pdf}
}

@inproceedings{pamfil2020,
  title = {{{DYNOTEARS}}: {{Structure Learning}} from {{Time-Series Data}}},
  shorttitle = {{{DYNOTEARS}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Pamfil, Roxana and Sriwattanaworachai, Nisara and Desai, Shaan and Pilgerstorfer, Philip and Georgatzis, Konstantinos and Beaumont, Paul and Aragam, Bryon},
  year = {2020},
  month = jun,
  pages = {1595--1605},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We revisit the structure learning problem for dynamic Bayesian networks and propose a method that simultaneously estimates contemporaneous (intra-slice) and time-lagged (inter-slice) relationships between variables in a time-series. Our approach is score-based, and revolves around minimizing a penalized loss subject to an acyclicity constraint. To solve this problem, we leverage a recent algebraic result characterizing the acyclicity constraint as a smooth equality constraint. The resulting algorithm, which we call DYNOTEARS, outperforms other methods on simulated data, especially in high-dimensions as the number of variables increases. We also apply this algorithm on real datasets from two different domains, finance and molecular biology, and analyze the resulting output. Compared to state-of-the-art methods for learning dynamic Bayesian networks, our method is both scalable and accurate on real data. The simple formulation and competitive performance of our method make it suitable for a variety of problems where one seeks to learn connections between variables across time.},
  langid = {english},
  keywords = {quantumblack},
  file = {/Users/cife/Zotero/storage/RIVP5M63/Pamfil et al. - 2020 - DYNOTEARS Structure Learning from Time-Series Dat.pdf;/Users/cife/Zotero/storage/T8X7VTLW/Pamfil et al. - 2020 - DYNOTEARS Structure Learning from Time-Series Dat.pdf}
}

@misc{pan2014,
  title = {Tensor {{Transpose}} and {{Its Properties}}},
  author = {Pan, Ran},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1503},
  eprint = {1411.1503},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  institution = {{arXiv}},
  abstract = {Tensor transpose is a higher order generalization of matrix transpose. In this paper, we use permutations and symmetry group to define? the tensor transpose. Then we discuss the classification and composition of tensor transposes. Properties of tensor transpose are studied in relation to tensor multiplication, tensor eigenvalues, tensor decompositions and tensor rank.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/Users/cife/Dropbox/Zotero/Pan_2014_Tensor_Transpose_and_Its_Properties.pdf;/Users/cife/Zotero/storage/LIKGTU8S/1411.html}
}

@article{panos2018,
  ids = {panos2018a},
  title = {Fully {{Scalable Gaussian Processes}} Using {{Subspace Inducing Inputs}}},
  author = {Panos, Aristeidis and Dellaportas, Petros and Titsias, Michalis K.},
  year = {2018},
  month = dec,
  journal = {arXiv:1807.02537v2 [stat]},
  eprint = {1807.02537v2},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We introduce fully scalable Gaussian processes, an implementation scheme that tackles the problem of treating a high number of training instances together with high dimensional input data. Our key idea is a representation trick over the inducing variables called subspace inducing inputs. This is combined with certain matrix-preconditioning based parametrizations of the variational distributions that lead to simplified and numerically stable variational lower bounds. Our illustrative applications are based on challenging extreme multi-label classification problems with the extra burden of the very large number of class labels. We demonstrate the usefulness of our approach by presenting predictive performances together with low computational times in datasets with extremely large number of instances and input dimensions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Panos_2018_Fully_Scalable_Gaussian_Processes_using_Subspace_Inducing_Inputs.pdf;/Users/cife/Zotero/storage/535EJ64E/1910.html;/Users/cife/Zotero/storage/ZPU45NYA/1807.html}
}

@book{parrish2020,
  title = {The {{Great Mental Models Volume}} 2: {{Physics}}, {{Chemistry}} and {{Biology}}},
  shorttitle = {The {{Great Mental Models Volume}} 2},
  author = {Parrish, Shane and Beaubien, Rhiannon},
  year = {2020},
  month = mar,
  publisher = {{Latticework Publishing Inc.}},
  abstract = {***This is the second book in The Great Mental Models series and the highly anticipated follow up to the Wall Street Journal bestseller, Volume 1: General Thinking Concepts.***We tend to isolate the things we know in the domain we learned it. For example:-What does the inertia of a rolling stone have to do with perseverance and being open-minded?-How can the ancient process of steel production make you a more creative and innovative thinker?-What does the replication of our skin cells have to do with being a stronger and more effective leader?On the surface, these concepts may appear to be dissimilar and unrelated. But the surprising truth is the hard sciences (physics, chemistry, and biology) offer a wealth of useful tools you can use to develop critically important skills like:- Relationship building- Leadership- Communication- Creativity- Curiosity- Problem-solving- Decision-makingThis second volume of the Great Mental Models series shows you how to make those connections. It explores the core ideas from the hard sciences and offers nearly two dozen models to add to your mental toolbox.You'll not only get a better understanding of the forces that influence the world around you, but you'll learn how to direct those forces to create outsized advantages in the areas of your life that matter most to you.},
  langid = {english},
  keywords = {book}
}

@inproceedings{paszke2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8026--8037},
  publisher = {{Curran Associates, Inc.}},
  keywords = {code},
  file = {/Users/cife/Dropbox/Zotero/Paszke_2019_PyTorch.pdf;/Users/cife/Zotero/storage/EH3FUK34/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{pautasso2013,
  title = {Ten {{Simple Rules}} for {{Writing}} a {{Literature Review}}},
  author = {Pautasso, Marco},
  year = {2013},
  month = jul,
  journal = {PLoS Computational Biology},
  volume = {9},
  number = {7},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1003149},
  pmcid = {PMC3715443},
  pmid = {23874189},
  keywords = {grey literature,reference},
  file = {/Users/cife/Dropbox/Zotero/Pautasso_2013_Ten_Simple_Rules_for_Writing_a_Literature_Review.pdf}
}

@article{pearson1901,
  title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, K.},
  year = {1901},
  pages = {559--572}
}

@article{pedregosa2011,
  title = {Scikit-Learn: {{Machine Learning}} in \{\vphantom\}{{P}}\vphantom\{\}ython},
  author = {Pedregosa, F. and Varoquaux, C. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@article{pennec2006,
  title = {Intrinsic {{Statistics}} on {{Riemannian Manifolds}}: {{Basic Tools}} for {{Geometric Measurements}}},
  author = {Pennec, Xavier},
  year = {2006},
  journal = {Journal of Mathematical Imaging and Vision},
  pages = {127--154},
  abstract = {In medical image analysis and high level computer vision, there is an intensive use of geo- metric features like orientations, lines, and geometric transformations ranging from simple ones (orientations, lines, rigid body or affine transformations, etc.) to very complex ones like curves, surfaces, or general diffeomorphic transformations. The measurement of such geometric prim- itives is generally noisy in real applications and we need to use statistics either to reduce the uncertainty (estimation), to compare observations, or to test hypotheses. Unfortunately, even simple geometric primitives often belong to manifolds that are not vector spaces. In previous works [1, 2], we investigated invariance requirements to build some statistical tools on transfor- mation groups and homogeneous manifolds that avoids paradoxes. In this paper, we consider finite dimensional manifolds with a Riemannian metric as the basic structure. Based on this metric, we develop the notions of mean value and covariance matrix of a random element, nor- mal law, Mahalanobis distance and {$\chi$}2 law. We provide a new proof of the characterization of Riemannian centers of mass and an original gradient descent algorithm to efficiently compute them. The notion of Normal law we propose is based on the maximization of the entropy know- ing the mean and covariance of the distribution. The resulting family of pdfs spans the whole range from uniform (on compact manifolds) to the point mass distribution. Moreover, we were able to provide tractable approximations (with their limits) for small variances which show that we can effectively implement and work with these definitions.},
  keywords = {geometry,reading},
  annotation = {Xavier Pennec. Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measure- ments. Journal of Mathematical Imaging and Vision, Springer Verlag, 2006, 25 (1), pp.127-154. 10.1007/s10851-006-6228-4 . inria-00614994},
  file = {/Users/cife/Dropbox/Zotero/Pennec_2006_Intrinsic_Statistics_on_Riemannian_Manifolds.pdf}
}

@book{pennec2019,
  title = {Riemannian Geometric Statistics in Medical Image Analysis},
  author = {Pennec, Xavier and Sommer, Stefan and Fletcher, Tom},
  year = {2019},
  publisher = {{Academic Press}}
}

@book{penrose2006,
  title = {The {{Road}} to {{Reality}} - {{A Complete Guide}} to the {{Laws}} of the {{Universe}}},
  author = {Penrose, Roger},
  year = {2006},
  publisher = {{Vintage}},
  isbn = {978-0-09-944068-0}
}

@article{pessach2020,
  title = {Algorithmic {{Fairness}}},
  author = {Pessach, Dana and Shmueli, Erez},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09784 [cs, stat]},
  eprint = {2001.09784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pessach_2020_Algorithmic_Fairness.pdf;/Users/cife/Zotero/storage/L8E3KFKW/2001.html}
}

@book{petersen2006,
  title = {Riemannian Geometry},
  author = {Petersen, Peter},
  year = {2006},
  series = {Graduate Texts in Mathematics},
  edition = {2nd ed},
  number = {171},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-29246-5},
  langid = {english},
  lccn = {QA649 .P386 2006},
  keywords = {Geometry; Riemannian,read-maybe-never,reference},
  annotation = {OCLC: ocm71121685},
  file = {/Users/cife/Dropbox/Zotero/Petersen_2006_Riemannian_geometry.pdf}
}

@techreport{petersen2012,
  type = {Technical {{Report}}},
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year = {2012},
  institution = {{Technical University of Denmark}},
  abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
  keywords = {read,reference},
  file = {/Users/cife/Dropbox/Zotero/Petersen_2012_The_Matrix_Cookbook.pdf;/Users/cife/Zotero/storage/IPF3P4V9/publication_details.html}
}

@article{pfau2020,
  title = {Disentangling by {{Subspace Diffusion}}},
  author = {Pfau, David and Higgins, Irina and Botev, Aleksandar and Racani{\`e}re, S{\'e}bastien},
  year = {2020},
  month = nov,
  journal = {arXiv:2006.12982 [cs, stat]},
  eprint = {2006.12982},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a novel nonparametric algorithm for symmetry-based disentangling of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER). GEOMANCER provides a partial answer to the question posed by Higgins et al. (2018): is it possible to learn how to factorize a Lie group solely from observations of the orbit of an object it acts on? We show that fully unsupervised factorization of a data manifold is possible if the true metric of the manifold is known and each factor manifold has nontrivial holonomy -- for example, rotation in 3D. Our algorithm works by estimating the subspaces that are invariant under random walk diffusion, giving an approximation to the de Rham decomposition from differential geometry. We demonstrate the efficacy of GEOMANCER on several complex synthetic manifolds. Our work reduces the question of whether unsupervised disentangling is possible to the question of whether unsupervised metric learning is possible, providing a unifying insight into the geometric nature of representation learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pfau_2020_Disentangling_by_Subspace_Diffusion.pdf;/Users/cife/Zotero/storage/G2A5LE27/2006.html}
}

@article{pinder2021,
  title = {Stein {{Variational Gaussian Processes}}},
  author = {Pinder, Thomas and Nemeth, Christopher and Leslie, David},
  year = {2021},
  month = apr,
  journal = {arXiv:2009.12141 [cs, stat]},
  eprint = {2009.12141},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We show how to use Stein variational gradient descent (SVGD) to carry out inference in Gaussian process (GP) models with non-Gaussian likelihoods and large data volumes. Markov chain Monte Carlo (MCMC) is extremely computationally intensive for these situations, but the parametric assumptions required for efficient variational inference (VI) result in incorrect inference when they encounter the multi-modal posterior distributions that are common for such models. SVGD provides a non-parametric alternative to variational inference which is substantially faster than MCMC. We prove that for GP models with Lipschitz gradients the SVGD algorithm monotonically decreases the Kullback-Leibler divergence from the sampling distribution to the true posterior. Our method is demonstrated on benchmark problems in both regression and classification, a multimodal posterior, and an air quality example with 550,134 spatiotemporal observations, showing substantial performance improvements over MCMC and VI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pinder_2021_Stein_Variational_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/JP3M7PGR/2009.html}
}

@misc{pinder2021a,
  title = {Gaussian {{Processes}} on {{Hypergraphs}}},
  author = {Pinder, Thomas and Turnbull, Kathryn and Nemeth, Christopher and Leslie, David},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01982},
  eprint = {2106.01982},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2106.01982},
  abstract = {We derive a Matern Gaussian process (GP) on the vertices of a hypergraph. This enables estimation of regression models of observed or latent values associated with the vertices, in which the correlation and uncertainty estimates are informed by the hypergraph structure. We further present a framework for embedding the vertices of a hypergraph into a latent space using the hypergraph GP. Finally, we provide a scheme for identifying a small number of representative inducing vertices that enables scalable inference through sparse GPs. We demonstrate the utility of our framework on three challenging real-world problems that concern multi-class classification for the political party affiliation of legislators on the basis of voting behaviour, probabilistic matrix factorisation of movie reviews, and embedding a hypergraph of animals into a low-dimensional latent space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pinder_2021_Gaussian_Processes_on_Hypergraphs.pdf;/Users/cife/Zotero/storage/CY4KFDAW/2106.html}
}

@article{pleiss2018,
  title = {Constant-{{Time Predictive Distributions}} for {{Gaussian Processes}}},
  author = {Pleiss, Geoff and Gardner, Jacob R. and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
  year = {2018},
  month = jun,
  journal = {arXiv:1803.06058 [cs, stat]},
  eprint = {1803.06058},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacrificing accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pleiss_2018_Constant-Time_Predictive_Distributions_for_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/KCR9JBYA/1803.html}
}

@inproceedings{pleiss2020,
  title = {Fast {{Matrix Square Roots}} with {{Applications}} to {{Gaussian Processes}} and {{Bayesian Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pleiss, Geoff and Jankowiak, Martin and Eriksson, David and Damle, Anil and Gardner, Jacob},
  year = {2020},
  volume = {33},
  pages = {22268--22281},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Matrix square roots and their inverses arise frequently in machine learning, e.g., when sampling from high-dimensional Gaussians N(0,K) or ``whitening'' a vector b against covariance matrix K. While existing methods typically require O(N\^3) computation, we introduce a highly-efficient quadratic-time algorithm for computing K\^\{1/2\}b, K\^\{-1/2\}b, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approximation and typically achieves 4 decimal places of accuracy with fewer than 100 MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method's applicability on matrices as large as 50,000 by 50,000 - well beyond traditional methods - with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy. In particular, we perform variational GP inference with up to 10,000 inducing points and perform Gibbs sampling on a 25,000-dimensional problem.},
  file = {/Users/cife/Dropbox/Zotero/Pleiss_2020_Fast_Matrix_Square_Roots_with_Applications_to_Gaussian_Processes_and_Bayesian.pdf}
}

@inproceedings{pokorny2012,
  title = {Persistent {{Homology}} for {{Learning Densities}} with {{Bounded Support}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Pokorny, Florian T. and Kjellstr{\"o}m, Hedvig and Kragic, Danica and Ek, Carl},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1817--1825},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-maybe-never,sb},
  file = {/Users/cife/Dropbox/Zotero/Pokorny_2012_Persistent_Homology_for_Learning_Densities_with_Bounded_Support.pdf;/Users/cife/Zotero/storage/Y24C3SFA/4711-persistent-homology-for-learning-densities-with-bounded-support.html}
}

@misc{pouplin2020,
  title = {Gaussian Processes},
  author = {Pouplin, Alison},
  year = {2020},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Pouplin_2020_Gaussian_processes.pdf}
}

@misc{pouplin2022,
  title = {Identifying Latent Distances with {{Finslerian}} Geometry},
  author = {Pouplin, Alison and Eklund, David and Ek, Carl Henrik and Hauberg, S{\o}ren},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10010},
  eprint = {2212.10010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Riemannian geometry provides powerful tools to explore the latent space of generative models while preserving the inherent structure of the data manifold. Lengths, energies and volume measures can be derived from a pullback metric, defined through the immersion that maps the latent space to the data space. With this in mind, most generative models are stochastic, and so is the pullback metric. Manipulating stochastic objects is strenuous in practice. In order to perform operations such as interpolations, or measuring the distance between data points, we need a deterministic approximation of the pullback metric. In this work, we are defining a new metric as the expected length derived from the stochastic pullback metric. We show this metric is Finslerian, and we compare it with the expected pullback metric. In high dimensions, we show that the metrics converge to each other at a rate of \$\textbackslash mathcal\{O\}\textbackslash left(\textbackslash frac\{1\}\{D\}\textbackslash right)\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Pouplin_2022_Identifying_latent_distances_with_Finslerian_geometry.pdf;/Users/cife/Zotero/storage/HDCD8HUC/2212.html}
}

@article{power,
  title = {Grokking: {{Generalization Beyond Overfitting On Small Algorithmic Datasets}}},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  pages = {9},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of ``grokking'' a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/KUF3ZWPR/Power et al. - Grokking Generalization Beyond Overfitting On Sma.pdf}
}

@book{pressley2010,
  title = {Elementary Differential Geometry},
  author = {Pressley, Andrew},
  year = {2010},
  publisher = {{Springer Science \textbackslash\& Business Media}},
  keywords = {read-someday,reading},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/pressley2010.md;/Users/cife/Dropbox/Zotero/Experiments/Pressley_2010_Elementary_differential_geometry.pdf}
}

@article{quinonero-candela2005,
  ids = {herbrich},
  title = {A {{Unifying View}} of {{Sparse Approximate Gaussian Process Regression}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  pages = {1939--1959},
  abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
  langid = {english},
  keywords = {read-soon,scalable-gp},
  file = {/Users/cife/Dropbox/Zotero/Quin_̃onero-Candela_2005_A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.pdf}
}

@article{rainforth2019,
  title = {Tighter {{Variational Bounds}} Are {{Not Necessarily Better}}},
  author = {Rainforth, Tom and Kosiorek, Adam R. and Le, Tuan Anh and Maddison, Chris J. and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  year = {2019},
  month = mar,
  journal = {arXiv:1802.04537 [cs, stat]},
  eprint = {1802.04537},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted auto-encoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ramchandran2021,
  title = {Latent {{Gaussian}} Process with Composite Likelihoods and Numerical Quadrature},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ramchandran, Siddharth and Koskinen, Miika and L{\"a}hdesm{\"a}ki, Harri},
  year = {2021},
  month = mar,
  pages = {3718--3726},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Clinical patient records are an example of high-dimensional data that is typically collected from disparate sources and comprises of multiple likelihoods with noisy as well as missing values. In this work, we propose an unsupervised generative model that can learn a low-dimensional representation among the observations in a latent space, while making use of all available data in a heterogeneous data setting with missing values. We improve upon the existing Gaussian process latent variable model (GPLVM) by incorporating multiple likelihoods and deep neural network parameterised back-constraints to create a non-linear dimensionality reduction technique for heterogeneous data. In addition, we develop a variational inference method for our model that uses numerical quadrature. We establish the effectiveness of our model and compare against existing GPLVM methods on a standard benchmark dataset as well as on clinical data of Parkinson's disease patients treated at the HUS Helsinki University Hospital.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Ramchandran_2021_Latent_Gaussian_process_with_composite_likelihoods_and_numerical_quadrature.pdf;/Users/cife/Zotero/storage/BACAVQKZ/Ramchandran et al. - 2021 - Latent Gaussian process with composite likelihoods.pdf}
}

@inproceedings{rasmussen2000,
  title = {Occam' s {{Razor}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmussen, Carl and Ghahramani, Zoubin},
  year = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  abstract = {The Bayesian paradigm apparently only sometimes gives rise to Occam's  Razor;  at  other times  very  large models perform well.  We  give  simple  examples of both kinds of behaviour. The two views are reconciled when  measuring complexity of functions, rather than of the machinery used to  implement them.  We analyze the complexity of functions for some linear  in the parameter models that are  equivalent to  Gaussian Processes, and  always find Occam's Razor at work.},
  file = {/Users/cife/Dropbox/Zotero/Rasmussen_2000_Occam'_s_Razor.pdf}
}

@incollection{rasmussen2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  volume = {3176},
  pages = {63--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_4},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  isbn = {978-3-540-23122-6 978-3-540-28650-9},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Rasmussen_2004_Gaussian_Processes_in_Machine_Learning.pdf}
}

@book{rasmussen2006,
  ids = {rasmussenGaussianProcessesMachine2006a},
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models,read-someday},
  annotation = {OCLC: ocm61285753},
  file = {/Users/cife/Dropbox/Zotero/Rasmussen_2006_Gaussian_processes_for_machine_learning.pdf}
}

@misc{ravanbakhsh2012,
  title = {A {{Generalized Loop Correction Method}} for {{Approximate Inference}} in {{Graphical Models}}},
  author = {Ravanbakhsh, Siamak and Yu, Chun-Nam and Greiner, Russell},
  year = {2012},
  month = jun,
  number = {arXiv:1206.4654},
  eprint = {1206.4654},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Ravanbakhsh_2012_A_Generalized_Loop_Correction_Method_for_Approximate_Inference_in_Graphical.pdf;/Users/cife/Zotero/storage/AMLLL897/1206.html}
}

@article{rendle2019,
  title = {On the {{Difficulty}} of {{Evaluating Baselines}}: {{A Study}} on {{Recommender Systems}}},
  shorttitle = {On the {{Difficulty}} of {{Evaluating Baselines}}},
  author = {Rendle, Steffen and Zhang, Li and Koren, Yehuda},
  year = {2019},
  month = may,
  journal = {arXiv:1905.01395 [cs]},
  eprint = {1905.01395},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Numerical evaluations with comparisons to baselines play a central role when judging research in recommender systems. In this paper, we show that running baselines properly is difficult. We demonstrate this issue on two extensively studied datasets. First, we show that results for baselines that have been used in numerous publications over the past five years for the Movielens 10M benchmark are suboptimal. With a careful setup of a vanilla matrix factorization baseline, we are not only able to improve upon the reported results for this baseline but even outperform the reported results of any newly proposed method. Secondly, we recap the tremendous effort that was required by the community to obtain high quality results for simple methods on the Netflix Prize. Our results indicate that empirical findings in research papers are questionable unless they were obtained on standardized benchmarks where baselines have been tuned extensively by the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Rendle_2019_On_the_Difficulty_of_Evaluating_Baselines.pdf;/Users/cife/Zotero/storage/H9SX2UW9/1905.html}
}

@misc{resnet18,
  title = {{{ResNet18}}},
  shorttitle = {{{ResNet18}}, Pytorch/Vision:V0.6.0},
  author = {ResNet18, PyTorch Team},
  abstract = {An open source deep learning platform that provides a seamless path from research prototyping to production deployment.},
  keywords = {code},
  file = {/Users/cife/Zotero/storage/429GG48M/pytorch_vision_resnet.html}
}

@article{rezende2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = may,
  journal = {arXiv:1401.4082 [cs, stat]},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe-never,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Rezende_2014_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models.pdf;/Users/cife/Zotero/storage/822449ES/1401.html}
}

@inproceedings{rezende2014a,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = jun,
  pages = {1278--1286},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Rezende_2014_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models2.pdf}
}

@article{rezende2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  journal = {arXiv:1505.05770 [cs, stat]},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-someday,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Rezende_2016_Variational_Inference_with_Normalizing_Flows.pdf}
}

@article{ribeiro2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Ribeiro_2016_Why_Should_I_Trust_You.pdf}
}

@article{rieck,
  title = {Topological {{Data Analysis}} for {{Machine Learning}} - {{Lecture}} 2: {{Computational Topology}}},
  author = {Rieck, Bastian},
  pages = {54},
  langid = {english},
  file = {/Users/cife/Zotero/storage/WHDGIAFV/Rieck - Topological Data Analysis for Machine Learning - L.pdf}
}

@article{rieger2019,
  title = {Interpretations Are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge},
  shorttitle = {Interpretations Are Useful},
  author = {Rieger, Laura and Singh, Chandan and Murdoch, W. James and Yu, Bin},
  year = {2019},
  month = oct,
  journal = {arXiv:1909.13584 [cs, stat]},
  eprint = {1909.13584},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models. In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations. Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Rieger_2019_Interpretations_are_useful.pdf;/Users/cife/Zotero/storage/FZRTICR9/1909.html}
}

@article{riesselman2017,
  title = {Deep Generative Models of Genetic Variation Capture Mutation Effects},
  author = {Riesselman, Adam J. and Ingraham, John B. and Marks, Debora S.},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.06527 [cond-mat, physics:physics, q-bio, stat]},
  eprint = {1712.06527},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics, q-bio, stat},
  abstract = {The functions of proteins and RNAs are determined by a myriad of interactions between their constituent residues, but most quantitative models of how molecular phenotype depends on genotype must approximate this by simple additive effects. While recent models have relaxed this constraint to also account for pairwise interactions, these approaches do not provide a tractable path towards modeling higher-order dependencies. Here, we show how latent variable models with nonlinear dependencies can be applied to capture beyond-pairwise constraints in biomolecules. We present a new probabilistic model for sequence families, DeepSequence, that can predict the effects of mutations across a variety of deep mutational scanning experiments significantly better than site independent or pairwise models that are based on the same evolutionary data. The model, learned in an unsupervised manner solely from sequence information, is grounded with biologically motivated priors, reveals latent organization of sequence families, and can be used to extrapolate to new parts of sequence space},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Riesselman_2017_Deep_generative_models_of_genetic_variation_capture_mutation_effects.pdf;/Users/cife/Zotero/storage/Z5UREITY/1712.html}
}

@book{riley2002,
  title = {Mathematical Methods for Physics and Engineering},
  author = {Riley, Kenneth Franklin and Hobson, Michael Paul and Bence, Stephen John},
  year = {2002},
  edition = {Second},
  publisher = {{Cambridge University Press}}
}

@misc{rodrigues2018,
  title = {Outliers Make Us Go {{MAD}}: {{Univariate Outlier Detection}}},
  shorttitle = {Outliers Make Us Go {{MAD}}},
  author = {Rodrigues, Jo{\~a}o},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {Outliers are observations that deviate markedly from other observations of the same sample.},
  howpublished = {https://medium.com/james-blogs/outliers-make-us-go-mad-univariate-outlier-detection-b3a72f1ea8c7},
  langid = {english},
  keywords = {reference,sb},
  file = {/Users/cife/Zotero/storage/ES94HK4G/outliers-make-us-go-mad-univariate-outlier-detection-b3a72f1ea8c7.html}
}

@article{roeer2013,
  title = {On the Finite Dimensional Approximation of the {{Kuratowski-embedding}} for Compact Manifolds},
  author = {Roeer, Malte},
  year = {2013},
  month = jul,
  journal = {arXiv:1305.1529 [math]},
  eprint = {1305.1529},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {In the proof of his systolic inequality, Gromov uses an isometric embedding of a Riemannian manifold M into the Banach space of bounded functions on M, the so-called Kuratowski-embedding. Subsequently, it was shown by different authors that the Kuratowski embedding can be approximated by bi-Lipschitz embeddings into finite-dimensional Banach spaces. We give a detailed proof for the existence of such finite-dimensional approximations along the lines suggested by Larry Guth and go on to discuss quantitative aspects of the problem, establishing for the dimension of the Banach space a bound which depends on curvature properties of the manifold.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Metric Geometry,read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Roeer_2013_On_the_finite_dimensional_approximation_of_the_Kuratowski-embedding_for_compact.pdf;/Users/cife/Zotero/storage/GXYGCPP4/1305.html}
}

@article{rogozhnikov2022,
  title = {{{EINOPS}}: {{CLEAR AND RELIABLE TENSOR MANIPULATIONS WITH EINSTEIN-LIKE NOTATION}}},
  author = {Rogozhnikov, Alex},
  year = {2022},
  pages = {21},
  abstract = {Tensor computations underlie modern scientific computing and deep learning. A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc. However, tensor operations in all frameworks follow the same paradigm. Recent neural network architectures demonstrate demand for higher expressiveness of tensor operations. The current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. Moreover, some commonly used operations do not provide sufficient checks and can break a tensor structure. These mistakes are elusive as no tools or tests can detect them. Independently, API discrepancies complicate code transfer between frameworks. We propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors. We implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/8NQ8K2DH/Rogozhnikov - 2022 - EINOPS CLEAR AND RELIABLE TENSOR MANIPULATIONS WI.pdf}
}

@misc{rojefferson2018,
  title = {Information Geometry (Part 1/3)},
  author = {{rojefferson}},
  year = {2018},
  month = aug,
  journal = {Ro's blog},
  abstract = {Information geometry is a rather interesting fusion of statistics and differential geometry, in which a statistical model is endowed with the structure of a Riemannian manifold. Each point on the m\ldots},
  langid = {english},
  file = {/Users/cife/Zotero/storage/TD9DDFZG/information-geometry-part-1-2.html}
}

@article{rossi2020,
  title = {Rethinking {{Sparse Gaussian Processes}}: {{Bayesian Approaches}} to {{Inducing-Variable Approximations}}},
  shorttitle = {Rethinking {{Sparse Gaussian Processes}}},
  author = {Rossi, Simone and Heinonen, Markus and Bonilla, Edwin V. and Shen, Zheyang and Filippone, Maurizio},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.03080 [cs, stat]},
  eprint = {2003.03080},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Most previous works treat the locations of the inducing variables, i.e. the inducing inputs, as variational hyperparameters, and these are then optimized together with GP covariance hyper-parameters. While some approaches point to the benefits of a Bayesian treatment of GP hyper-parameters, this has been largely overlooked for the inducing inputs. In this work, we show that treating both inducing locations and GP hyper-parameters in a Bayesian way, by inferring their full posterior, further significantly improves performance. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its competitive performance through an extensive experimental campaign across several regression and classification problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Rossi_2020_Rethinking_Sparse_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/C5FZ767C/2003.html}
}

@inproceedings{roweis1997,
  title = {{{EM Algorithms}} for {{PCA}} and {{SPCA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Roweis, Sam},
  year = {1997},
  volume = {10},
  publisher = {{MIT Press}},
  abstract = {I  present  an  expectation-maximization  (EM)  algorithm  for  principal  component analysis (PCA). The algorithm allows a few eigenvectors and  eigenvalues to  be extracted from  large collections of high dimensional  data.  It is computationally very efficient in space and time.  It also natu(cid:173) rally accommodates missing infonnation.  I also introduce a new variant  of PC A called sensible principal component analysis (SPCA) which de(cid:173) fines a proper density model in the data space. Learning for SPCA is also  done with an EM algorithm.  I report results on synthetic and real data  showing that these EM algorithms correctly and efficiently find the lead(cid:173) ing eigenvectors of the covariance of datasets in a few iterations using up  to hundreds of thousands of datapoints in thousands of dimensions.},
  file = {/Users/cife/Dropbox/Zotero/Roweis_1997_EM_Algorithms_for_PCA_and_SPCA.pdf}
}

@inproceedings{rozen2021,
  title = {Moser {{Flow}}: {{Divergence-based Generative Modeling}} on {{Manifolds}}},
  shorttitle = {Moser {{Flow}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  year = {2021},
  month = may,
  abstract = {Introducing a novel generative model on manifolds based on a classical flow by Moser.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Rozen_2021_Moser_Flow.pdf;/Users/cife/Zotero/storage/ELF8DZZ7/forum.html}
}

@book{ruppert2010,
  title = {Statistics and {{Data Analysis}} for {{Financial Engineering}}},
  author = {Ruppert, David},
  year = {2010},
  month = nov,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Financial engineers have access to enormous quantities of data but need powerful methods for extracting quantitative information, particularly about volatility and risks. Key features of this textbook are: illustration of concepts with financial markets and economic data, R Labs with real-data exercises, and integration of graphical and analytic methods for modeling and diagnosing modeling errors. Despite some overlap with the author's undergraduate textbook Statistics and Finance: An Introduction, this book differs from that earlier volume in several important aspects: it is graduate-level; computations and graphics are done in R; and many advanced topics are covered, for example, multivariate distributions, copulas, Bayesian computations, VaR and expected shortfall, and cointegration. The prerequisites are basic statistics and probability, matrices and linear algebra, and calculus.Some exposure to finance is helpful.},
  googlebooks = {i2bD50PbIikC},
  isbn = {978-1-4419-7787-8},
  langid = {english},
  keywords = {Business \& Economics / General,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General}
}

@article{saemundsson2018,
  title = {Meta {{Reinforcement Learning}} with {{Latent Variable Gaussian Processes}}},
  author = {S{\ae}mundsson, Steind{\'o}r and Hofmann, Katja and Deisenroth, Marc Peter},
  year = {2018},
  month = jul,
  journal = {arXiv:1803.07551 [cs, stat]},
  eprint = {1803.07551},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning from small data sets is critical in many practical applications where data collection is time consuming or expensive, e.g., robotics, animal experiments or drug design. Meta learning is one way to increase the data efficiency of learning algorithms by generalizing learned concepts from a set of training tasks to unseen, but related, tasks. Often, this relationship between tasks is hard coded or relies in some other way on human expertise. In this paper, we frame meta learning as a hierarchical latent variable model and infer the relationship between tasks automatically from data. We apply our framework in a model-based reinforcement learning setting and show that our meta-learning model effectively generalizes to novel tasks by identifying how new tasks relate to prior ones from minimal data. This results in up to a 60\% reduction in the average interaction time needed to solve tasks compared to strong baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Sæmundsson_2018_Meta_Reinforcement_Learning_with_Latent_Variable_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/U5DLBHDP/1803.html}
}

@article{salimbeni2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  year = {2017},
  month = nov,
  journal = {arXiv:1705.08933 [stat]},
  eprint = {1705.08933},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.},
  archiveprefix = {arXiv},
  keywords = {read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Salimbeni_2017_Doubly_Stochastic_Variational_Inference_for_Deep_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/3PIPGEUD/1705.html}
}

@article{salimbeni2018,
  title = {Natural {{Gradients}} in {{Practice}}: {{Non-Conjugate Variational Inference}} in {{Gaussian Process Models}}},
  shorttitle = {Natural {{Gradients}} in {{Practice}}},
  author = {Salimbeni, Hugh and Eleftheriadis, Stefanos and Hensman, James},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.09151 [cs, stat]},
  eprint = {1803.09151},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The natural gradient method has been used effectively in conjugate Gaussian process models, but the non-conjugate case has been largely unexplored. We examine how natural gradients can be used in non-conjugate stochastic settings, together with hyperparameter learning. We conclude that the natural gradient can significantly improve performance in terms of wall-clock time. For ill-conditioned posteriors the benefit of the natural gradient method is especially pronounced, and we demonstrate a practical setting where ordinary gradients are unusable. We show how natural gradients can be computed efficiently and automatically in any parameterization, using automatic differentiation. Our code is integrated into the GPflow package.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Salimbeni_2018_Natural_Gradients_in_Practice.pdf;/Users/cife/Zotero/storage/NYBY6XZM/1803.html}
}

@article{saul2000,
  title = {An Introduction to Locally Linear Embedding},
  author = {Saul, Lawrence K. and Roweis, Sam T.},
  year = {2000},
  journal = {unpublished. Available at: http://www. cs. toronto. edu/{$\sim$} roweis/lle/publications. html},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Saul_2000_An_introduction_to_locally_linear_embedding.pdf}
}

@misc{scannell2020,
  title = {Sparse {{Variational GP Metric Tensor}}},
  author = {Scannell, Aidan},
  year = {2020},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Scannell_2020_Sparse_Variational_GP_Metric_Tensor.pdf;/Users/cife/Dropbox/Zotero/Scannell_2020_Visualising_Our_Probabilistic_Metric_Tensor.pdf}
}

@phdthesis{scannell2022,
  title = {Bayesian {{Learning}} for {{Control}} in {{Multimodal Dynamical Systems}}},
  author = {Scannell, Aidan},
  year = {2022},
  school = {University of Bristol},
  file = {/Users/cife/Dropbox/Zotero/Scannell_2022_Bayesian_Learning_for_Control_in_Multimodal_Dynamical_Systems.pdf}
}

@article{schoenberg1946,
  title = {Contributions to the Problem of Approximation of Equidistant Data by Analytic Functions. {{Part B}}. {{On}} the Problem of Osculatory Interpolation. {{A}} Second Class of Analytic Approximation Formulae},
  author = {Schoenberg, Isaac Jacob},
  year = {1946},
  journal = {Quarterly of Applied Mathematics},
  volume = {4},
  number = {2},
  pages = {112--141}
}

@article{scholkopf1998,
  ids = {scholkopf1998a},
  title = {Nonlinear {{Component Analysis}} as a {{Kernel Eigenvalue Problem}}},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  year = {1998},
  month = jul,
  journal = {Neural Computation},
  volume = {10},
  number = {5},
  pages = {1299--1319},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976698300017467},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Schölkopf_1998_Nonlinear_Component_Analysis_as_a_Kernel_Eigenvalue_Problem.pdf;/Users/cife/Dropbox/Zotero/Schölkopf_1998_Nonlinear_Component_Analysis_as_a_Kernel_Eigenvalue_Problem2.pdf}
}

@incollection{scholkopf2007,
  title = {Image {{Retrieval}} and {{Classification Using Local Distance Functions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John and Hofmann, Thomas},
  year = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0057},
  abstract = {In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classification of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3\% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al.},
  isbn = {978-0-262-25691-9},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/2007_Image_Retrieval_and_Classification_Using_Local_Distance_Functions.pdf;/Users/cife/Dropbox/Zotero/2007_Image_Retrieval_and_Classification_Using_Local_Distance_Functions2.pdf}
}

@book{schutz2009,
  title = {A First Course in General Relativity},
  author = {Schutz, Bernard},
  year = {2009},
  publisher = {{Cambridge university press}}
}

@article{schwartz2008,
  title = {The Importance of Stupidity in Scientific Research},
  author = {Schwartz, Martin A.},
  year = {2008},
  month = jun,
  journal = {Journal of Cell Science},
  volume = {121},
  number = {11},
  pages = {1771--1771},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0021-9533, 1477-9137},
  doi = {10.1242/jcs.033340},
  abstract = {I recently saw an old friend for the first time in many years. We had been Ph.D. students at the same time, both studying science, although in different areas. She later dropped out of graduate school, went to Harvard Law School and is now a senior lawyer for a major environmental organization. At},
  chapter = {Essay},
  copyright = {\textcopyright{} The Company of Biologists Limited 2008},
  langid = {english},
  pmid = {18492790},
  keywords = {grey literature,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Schwartz_2008_The_importance_of_stupidity_in_scientific_research.pdf;/Users/cife/Zotero/storage/S49NEBU6/1771.html}
}

@article{schwobel2020,
  title = {Probabilistic {{Spatial Transformers}} for {{Bayesian Data Augmentation}}},
  author = {Schw{\"o}bel, Pola and Warburg, Frederik and J{\o}rgensen, Martin and Madsen, Kristoffer H. and Hauberg, S{\o}ren},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.03637 [cs, stat]},
  eprint = {2004.03637},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {High-capacity models require vast amounts of data, and data augmentation is a common remedy when this resource is limited. Standard augmentation techniques apply small hand-tuned transformations to existing data, which is a brittle process that realistically only allows for simple transformations. We propose a Bayesian interpretation of data augmentation where the transformations are modelled as latent variables to be marginalized, and show how these can be inferred variationally in an end-to-end fashion. This allows for significantly more complex transformations than manual tuning, and the marginalization implies a form of test-time data augmentation. The resulting model can be interpreted as a probabilistic extension of spatial transformer networks. Experimentally, we demonstrate improvements in accuracy and uncertainty quantification in image and time series classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Schwöbel_2020_Probabilistic_Spatial_Transformers_for_Bayesian_Data_Augmentation.pdf;/Users/cife/Zotero/storage/7BNPQINC/2004.html}
}

@inproceedings{sculley2015,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c c}ois and Dennison, Dan},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2503--2511},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read,style},
  file = {/Users/cife/Dropbox/Zotero/Sculley_2015_Hidden_Technical_Debt_in_Machine_Learning_Systems.pdf;/Users/cife/Zotero/storage/VI44R74A/5656-hidden-technical-debt-in-machine-learning-systems.html}
}

@misc{seeger2003,
  title = {Fast {{Forward Selection}} to {{Speed Up Sparse Gaussian Process Regression}}},
  author = {Seeger, Matthias and Williams, Christopher and Lawrence, Neil},
  year = {2003},
  journal = {Artificial Intelligence and Statistics 9},
  number = {CONF},
  abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the \&quot;support\&quot; patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods.},
  howpublished = {https://infoscience.epfl.ch/record/161318},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Seeger_2003_Fast_Forward_Selection_to_Speed_Up_Sparse_Gaussian_Process_Regression.pdf;/Users/cife/Dropbox/Zotero/Seeger_2003_Fast_Forward_Selection_to_Speed_Up_Sparse_Gaussian_Process_Regression2.pdf}
}

@article{shanmugam2018,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  shorttitle = {Elements of Causal Inference},
  author = {Shanmugam, Ramalingam},
  year = {2018},
  month = nov,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {88},
  number = {16},
  pages = {3248--3248},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2018.1505197},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Shanmugam_2018_Elements_of_causal_inference.pdf}
}

@article{shao2017a,
  title = {The {{Riemannian Geometry}} of {{Deep Generative Models}}},
  author = {Shao, Hang and Kumar, Abhishek and Fletcher, P. Thomas},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.08014 [cs, stat]},
  eprint = {1711.08014},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep generative models learn a mapping from a low dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold. However, further investigation into this phenomenon is warranted, to identify if there are other architectures or datasets where curvature plays a more prominent role. We believe that exploring the Riemannian geometry of deep generative models, using the tools developed in this paper, will be an important step in understanding the high-dimensional, nonlinear spaces these models learn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Shao_2017_The_Riemannian_Geometry_of_Deep_Generative_Models.pdf;/Users/cife/Zotero/storage/P7PJUDX3/1711.html}
}

@article{shi2020,
  ids = {shi2020a},
  title = {Sparse {{Orthogonal Variational Inference}} for {{Gaussian Processes}}},
  author = {Shi, Jiaxin and Titsias, Michalis K. and Mnih, Andriy},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.10596v3 [stat]},
  eprint = {1910.10596v3},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Shi_2020_Sparse_Orthogonal_Variational_Inference_for_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/32G9V5CZ/search.html;/Users/cife/Zotero/storage/MKLA2WPC/1910.html}
}

@inproceedings{shi2020a,
  title = {Sparse {{Orthogonal Variational Inference}} for {{Gaussian Processes}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Shi, Jiaxin and Titsias, Michalis and Mnih, Andriy},
  year = {2020},
  month = jun,
  pages = {1932--1942},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Shi_2020_Sparse_Orthogonal_Variational_Inference_for_Gaussian_Processes2.pdf;/Users/cife/Zotero/storage/CGMSE8TX/Shi et al. - 2020 - Sparse Orthogonal Variational Inference for Gaussi.pdf}
}

@article{shmueli2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  eprint = {1101.0891},
  eprinttype = {arxiv},
  pages = {289--310},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  archiveprefix = {arXiv},
  keywords = {philosophy,read-someday,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Shmueli_2010_To_Explain_or_to_Predict.pdf;/Users/cife/Zotero/storage/TXITEAC7/1101.html}
}

@inproceedings{shu2018,
  title = {Amortized {{Inference Regularization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shu, Rui and Bui, Hung H and Zhao, Shengjia and Kochenderfer, Mykel J and Ermon, Stefano},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.},
  file = {/Users/cife/Dropbox/Zotero/Shu_2018_Amortized_Inference_Regularization.pdf}
}

@misc{sit2019,
  title = {Code and Data from: {{Gaussian Process Regression}} for {{Estimating EM Ducting Within}} the {{Marine Atmospheric Boundary Layer}}},
  shorttitle = {Code and Data From},
  author = {Sit, Hilarie and Earls, Christopher J.},
  year = {2019},
  month = nov,
  doi = {10.7298/d62y-4s95},
  abstract = {We show that Gaussian process regression (GPR) can be used to infer the electromagnetic (EM) duct height within the marine atmospheric boundary layer  (MABL) from sparsely sampled propagation factors within the context of bistaticradars. These propagation factors are simulated using PETOOL, developed by  Ozgun et al. 2011, and the datasets for the three cases that correspond to the different sparse sampling techniques can be found in the data folder. We use GPR  to calculate the posterior predictive distribution on the labels (i.e. duct height) from both noise-free and noise-contaminated array of propagation factors. For  duct height inference from noise-contaminated propagation factors, we compare a naive approach, utilizing one random sample from the input distribution (i.e.  disregarding the input noise), with an inverse-variance weighted approach, utilizing a few random samples to estimate the true predictive distribution. The  resulting posterior predictive distributions from these two approaches are compared to a "ground truth" distribution, which is approximated using a large  number of Monte-Carlo samples. We use Python 3.6.4 and scikit-learn 0.20.2. The ability of GPR to yield accurate duct height predictions using few training  examples, along with its inference speed, indicates the suitability of the proposed method for real-time applications. This is the dataset and code that supports this work.},
  langid = {english},
  annotation = {Accepted: 2019-11-26T21:20:18Z},
  file = {/Users/cife/Zotero/storage/LVS5W5DH/69525.html}
}

@article{skilling2007,
  ids = {skillingProbabilityGeometrya},
  title = {Probability and {{Geometry}}},
  author = {Skilling, John},
  year = {2007},
  pages = {9},
  abstract = {Probability calculus is understood, and uniquely defined as the only rational tool for consistent inference. Yet two problems remain. One is a matter of principle: how do we assign the prior distribution that expresses the question we wish to ask? The other is a matter of practice: how do we navigate the parameter space in order to compute the posterior inference? Probability distributions have a natural geometry, which can be used to help in both these. But, like any other professional tool, geometry should be used with intelligence and care.},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Skilling_2007_Probability_and_Geometry.pdf;/Users/cife/Dropbox/Zotero/Skilling_2007_Probability_and_Geometry2.pdf}
}

@inproceedings{sklearn,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  booktitle = {{{ECML PKDD}} Workshop: {{Languages}} for Data Mining and Machine Learning},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and VanderPlas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  pages = {108--122}
}

@inproceedings{smola2001,
  title = {Sparse {{Greedy Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Smola, Alex and Bartlett, Peter},
  year = {2001},
  volume = {13},
  publisher = {{MIT Press}},
  file = {/Users/cife/Dropbox/Zotero/Smola_2001_Sparse_Greedy_Gaussian_Process_Regression.pdf}
}

@inproceedings{snelson2006,
  title = {Sparse {{Gaussian Processes}} Using {{Pseudo-inputs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 18},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  editor = {Weiss, Y. and Sch{\"o}lkopf, B. and Platt, J. C.},
  year = {2006},
  pages = {1257--1264},
  publisher = {{MIT Press}},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/Snelson_2006_Sparse_Gaussian_Processes_using_Pseudo-inputs.pdf;/Users/cife/Zotero/storage/ARWDTCIH/2857-sparse-gaussian-processes-using-pseudo-inputs.html}
}

@misc{sokol2022,
  title = {Interpretable {{Representations}} in {{Explainable AI}}: {{From Theory}} to {{Practice}}},
  shorttitle = {Interpretable {{Representations}} in {{Explainable AI}}},
  author = {Sokol, Kacper and Flach, Peter},
  year = {2022},
  month = sep,
  number = {arXiv:2008.07007},
  eprint = {2008.07007},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Interpretable representations are the backbone of many explainers designed for black-box predictive systems based on artificial intelligence and machine learning algorithms. They translate the low-level data representation necessary for good predictive performance into high-level human-intelligible concepts used to convey the explanatory insights. Notably, the explanation type and its cognitive complexity are directly controlled by the interpretable representation, allowing to target a particular audience and use case. However, many explainers built upon interpretable representations overlook their merit and fall back on default solutions that often carry implicit assumptions, thereby degrading the explanatory power and reliability of such techniques. To address this problem, we study properties of interpretable representations that encode presence and absence of human-comprehensible concepts. We show how they are operationalised for tabular, image and text data; discuss their assumptions, strengths and weaknesses; identify their core building blocks; and scrutinise their parameterisation. In particular, this in-depth analysis allows us to pinpoint their explanatory properties, desiderata and scope for (malicious) manipulation in the context of tabular data, where a linear model is used to quantify the influence of interpretable concepts on a black-box prediction. Our findings support a range of recommendations for designing trustworthy interpretable representations; specifically, the benefits of class-aware (supervised) discretisation of tabular data, e.g., with decision trees, and sensitivity of image interpretable representations to segmentation granularity and occlusion colour.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Sokol_2022_Interpretable_Representations_in_Explainable_AI.pdf;/Users/cife/Zotero/storage/VAT8RIHE/2008.html}
}

@misc{soni2019,
  title = {Penrose's {{Graphical Notation}}},
  author = {Soni, Aamir},
  year = {2019},
  month = nov,
  journal = {Analytics Vidhya},
  abstract = {In this article, we will learn how to represent Tensors graphically using Penrose's graphical notation.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/TGWSR6H8/penroses-graphical-notation-fe4c2f24cf3b.html}
}

@misc{stochman,
  title = {{{StochMan}}},
  author = {Detlefsen, Nicki S. and Pouplin, Alison and Feldager, Cilie W. and Geng, Cong and Kalatzis, Dimitris and Hauschultz, Helene and Duque, Miguel Gonz{\'a}lez and Warburg, Frederik and Hauberg, S{\o}ren},
  year = {2021},
  abstract = {StochMan (Stochastic Manifolds) is a collection of elementary algorithms for computations on random manifolds learned from finite noisy data. Each algorithm assume that the considered manifold model implement a specific set of interfaces.}
}

@article{straub,
  title = {A {{Dirichlet Process Mixture Model}} for {{Spherical Data}}},
  author = {Straub, Julian and Chang, Jason and Freifeld, Oren and Iii, John W Fisher},
  pages = {9},
  langid = {english},
  file = {/Users/cife/Zotero/storage/9ZW66MIQ/Straub et al. - A Dirichlet Process Mixture Model for Spherical Da.pdf}
}

@article{straub2015,
  title = {A {{Dirichlet Process Mixture Model}} for {{Spherical Data}}},
  author = {Straub, Julian and Chang, Jason and Freifeld, Oren and Iii, John W Fisher},
  year = {2015},
  pages = {9},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/DSJABYRX/Straub et al. - A Dirichlet Process Mixture Model for Spherical Da.pdf}
}

@article{szubert2019,
  title = {Structure-Preserving Visualisation of High Dimensional Single-Cell Datasets},
  author = {Szubert, Benjamin and Cole, Jennifer E. and Monaco, Claudia and Drozdov, Ignat},
  year = {2019},
  journal = {Scientific reports},
  volume = {9},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  keywords = {read,tsne},
  file = {/Users/cife/Zotero/storage/E9U6W6GU/s41598-019-45301-0.html}
}

@article{tanaka2021,
  title = {Noether's {{Learning Dynamics}}: {{Role}} of {{Symmetry Breaking}} in {{Neural Networks}}},
  shorttitle = {Noether's {{Learning Dynamics}}},
  author = {Tanaka, Hidenori and Kunin, Daniel},
  year = {2021},
  month = nov,
  journal = {arXiv:2105.02716 [cond-mat, q-bio, stat]},
  eprint = {2105.02716},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  abstract = {In nature, symmetry governs regularities, while symmetry breaking brings texture. In artificial neural networks, symmetry has been a central design principle to efficiently capture regularities in the world, but the role of symmetry breaking is not well understood. Here, we develop a theoretical framework to study the "geometry of learning dynamics" in neural networks, and reveal a key mechanism of explicit symmetry breaking behind the efficiency and stability of modern neural networks. To build this understanding, we model the discrete learning dynamics of gradient descent using a continuous-time Lagrangian formulation, in which the learning rule corresponds to the kinetic energy and the loss function corresponds to the potential energy. Then, we identify "kinetic symmetry breaking" (KSB), the condition when the kinetic energy explicitly breaks the symmetry of the potential function. We generalize Noether's theorem known in physics to take into account KSB and derive the resulting motion of the Noether charge: "Noether's Learning Dynamics" (NLD). Finally, we apply NLD to neural networks with normalization layers and reveal how KSB introduces a mechanism of "implicit adaptive optimization", establishing an analogy between learning dynamics induced by normalization layers and RMSProp. Overall, through the lens of Lagrangian mechanics, we have established a theoretical foundation to discover geometric design principles for the learning dynamics of neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Tanaka_2021_Noether's_Learning_Dynamics.pdf;/Users/cife/Zotero/storage/R3CN35PN/2105.html}
}

@article{tenenbaum2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, J. B.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  issn = {00368075, 10959203},
  doi = {10.1126/science.290.5500.2319},
  langid = {english},
  keywords = {read-soon,sb},
  file = {/Users/cife/Dropbox/Zotero/Tenenbaum_2000_A_Global_Geometric_Framework_for_Nonlinear_Dimensionality_Reduction.pdf}
}

@phdthesis{terenin2022,
  title = {Gaussian {{Processes}} and {{Statistical Decision-making}} in {{Non-Euclidean Spaces}}},
  author = {Terenin, Alexander},
  year = {2022},
  month = apr,
  eprint = {2202.10613},
  eprinttype = {arxiv},
  abstract = {Bayesian learning using Gaussian processes provides a foundational framework for making decisions in a manner that balances what is known with what could be learned by gathering data. In this dissertation, we develop techniques for broadening the applicability of Gaussian processes. This is done in two ways. Firstly, we develop pathwise conditioning techniques for Gaussian processes, which allow one to express posterior random functions as prior random functions plus a dependent update term. We introduce a wide class of efficient approximations built from this viewpoint, which can be randomly sampled once in advance, and evaluated at arbitrary locations without any subsequent stochasticity. This key property improves efficiency and makes it simpler to deploy Gaussian process models in decision-making settings. Secondly, we develop a collection of Gaussian process models over non-Euclidean spaces, including Riemannian manifolds and graphs. We derive fully constructive expressions for the covariance kernels of scalar-valued Gaussian processes on Riemannian manifolds and graphs. Building on these ideas, we describe a formalism for defining vector-valued Gaussian processes on Riemannian manifolds. The introduced techniques allow all of these models to be trained using standard computational methods. In total, these contributions make Gaussian processes easier to work with and allow them to be used within a wider class of domains in an effective and principled manner. This, in turn, makes it possible to potentially apply Gaussian processes to novel decision-making settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-very-soon,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Terenin_2022_Gaussian_Processes_and_Statistical_Decision-making_in_Non-Euclidean_Spaces.pdf;/Users/cife/Zotero/storage/P5YKP6ID/2202.html}
}

@book{thelen2022,
  title = {A {{Comprehensive Review}} of {{Digital Twin}} -- {{Part}} 2: {{Roles}} of {{Uncertainty Quantification}} and {{Optimization}}, a {{Battery Digital Twin}}, and {{Perspectives}}},
  author = {Thelen, Adam and Xiaoge, Zhang and Fink, Olga and Lu, Yan and Ghosh, Sayan and Youn, Byeng Dong and Todd, Michael and Mahadevan, Sankaran and Hu, Zhen},
  year = {2022},
  month = aug,
  abstract = {As an emerging technology in the era of Industry 4.0, digital twin is gaining unprecedented attention because of its promise to further optimize process design, quality control, health monitoring, decision and policy making, and more, by comprehensively modeling the physical world as a group of interconnected digital models. In a two-part series of papers, we examine the fundamental role of different modeling techniques, twinning enabling technologies, and uncertainty quantification and optimization methods commonly used in digital twins. This second paper presents a literature review of key enabling technologies of digital twins, with an emphasis on uncertainty quantification, optimization methods, open source datasets and tools, major findings, challenges, and future directions. Discussions focus on current methods of uncertainty quantification and optimization and how they are applied in different dimensions of a digital twin. Additionally, this paper presents a case study where a battery digital twin is constructed and tested to illustrate some of the modeling and twinning methods reviewed in this two-part review. Code and preprocessed data for generating all the results and figures presented in the case study are available on GitHub.}
}

@article{thierry-mieg2019,
  title = {Connections between Physics, Mathematics and Deep Learning},
  author = {{Thierry-Mieg}, Jean},
  year = {2019},
  month = aug,
  journal = {arXiv:1811.00576 [hep-th, stat]},
  eprint = {1811.00576},
  eprinttype = {arxiv},
  primaryclass = {hep-th, stat},
  abstract = {Starting from the Fermat's principle of least action, which governs classical and quantum mechanics and from the theory of exterior differential forms, which governs the geometry of curved manifolds, we show how to derive the equations governing neural networks in an intrinsic, coordinate invariant way, where the loss function plays the role of the Hamiltonian. To be covariant, these equations imply a layer metric which is instrumental in pretraining and explains the role of conjugation when using complex numbers. The differential formalism also clarifies the relation of the gradient descent optimizer with Aristotelian and Newtonian mechanics and why large learning steps break the logic of the linearization procedure. We hope that this formal presentation of the differential geometry of neural networks will encourage some physicists to dive into deep learning, and reciprocally, that the specialists of deep learning will better appreciate the close interconnection of their subject with the foundations of classical and quantum field theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Theory,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Thierry-Mieg_2019_Connections_between_physics,_mathematics_and_deep_learning.pdf;/Users/cife/Zotero/storage/3NYPAH9V/1811.html}
}

@book{thompson2008,
  title = {Angular Momentum: An Illustrated Guide to Rotational Symmetries for Physical Systems},
  author = {Thompson, William J},
  year = {2008},
  publisher = {{John Wiley \& Sons}}
}

@article{tian2020,
  title = {Ivis {{Dimensionality Reduction Framework}} for {{Biomacromolecular Simulations}}},
  author = {Tian, Hao and Tao, Peng},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.10718 [q-bio]},
  eprint = {2004.10718},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Molecular dynamics (MD) simulations have been widely applied to study macromolecules including proteins. However, high-dimensionality of the datasets produced by simulations makes it difficult for thorough analysis, and further hinders a deeper understanding of the biological system. To gain more insights into the protein structure-function relations, appropriate dimensionality reduction methods are needed to project simulations to low-dimensional spaces. Linear dimensionality reduction methods, such as principal component analysis (PCA) and time-structure based independent component analysis (t-ICA), fail to preserve enough structural information. Though better than linear methods, nonlinear methods, such as t-distributed stochastic neighbor embedding (t-SNE), still suffer from the limitations in avoiding system noise and keeping inter-cluster relations. Here, we applied the ivis framework as a novel machine learning based dimensionality reduction method originally developed for single-cell datasets for analysis of macromolecular simulations. Compared with other methods, ivis is superior in constructing Markov state model (MSM), preserving global distance and maintaining similarity between high dimension and low dimension with the least information loss. Moreover, the neuron weights in the hidden layer of supervised ivis framework provide new prospective for deciphering the allosteric process of proteins. Overall, ivis is a promising member in the analysis toolbox for proteins.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Quantitative Methods,read-maybe-never,sb,tsne},
  file = {/Users/cife/Dropbox/Zotero/Tian_2020_ivis_Dimensionality_Reduction_Framework_for_Biomacromolecular_Simulations.pdf;/Users/cife/Zotero/storage/PFH3QRC2/2004.html}
}

@article{timonen2021,
  title = {Scalable Mixed-Domain {{Gaussian}} Processes},
  author = {Timonen, Juho and L{\"a}hdesm{\"a}ki, Harri},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.02019 [cs, stat]},
  eprint = {2111.02019},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian process (GP) models that combine both categorical and continuous input variables have found use e.g. in longitudinal data analysis and computer experiments. However, standard inference for these models has the typical cubic scaling, and common scalable approximation schemes for GPs cannot be applied since the covariance function is non-continuous. In this work, we derive a basis function approximation scheme for mixed-domain covariance functions, which scales linearly with respect to the number of observations and total number of basis functions. The proposed approach is naturally applicable to Bayesian GP regression with arbitrary observation models. We demonstrate the approach in a longitudinal data modelling context and show that it approximates the exact GP model accurately, requiring only a fraction of the runtime compared to fitting the corresponding exact model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  file = {/Users/cife/Dropbox/Zotero/Timonen_2021_Scalable_mixed-domain_Gaussian_processes.pdf;/Users/cife/Zotero/storage/II32K39R/2111.html}
}

@article{tipping1999,
  title = {Probabilistic {{Principal Component Analysis}}},
  author = {Tipping, Michael E. and Bishop, Christopher M.},
  year = {1999},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {61},
  number = {3},
  pages = {611--622},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00196},
  abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
  copyright = {1999 Royal Statistical Society},
  langid = {english},
  keywords = {Density estimation,EM algorithm,Gaussian mixtures,Maximum likelihood,Principal component analysis,Probability model,read,thesis},
  file = {/Users/cife/Dropbox/Zotero/Tipping_1999_Probabilistic_Principal_Component_Analysis.pdf;/Users/cife/Zotero/storage/FYZR2QXA/1467-9868.html}
}

@article{titsias2009,
  ids = {titsiasVariationalLearningInducinga},
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  author = {Titsias, Michalis K},
  year = {2009},
  journal = {Artificial Intelligence and Statistics},
  pages = {567--574},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  langid = {english},
  keywords = {read,read-very-soon,thesis},
  file = {/Users/cife/Documents/thesis/sunshine-highs/citations/titsias2009.md;/Users/cife/Dropbox/Zotero/Titsias_2009_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes.pdf}
}

@misc{titsias2009a,
  title = {Variational Model Selection for Sparse {{Gaussian}} Process Regression},
  author = {Titsias, Michalis K.},
  year = {2009},
  publisher = {{Report, University of Manchester, UK}},
  file = {/Users/cife/Dropbox/Zotero/Titsias_2009_Variational_Model_Selection_for_Sparse_Gaussian_Process_Regression.pdf}
}

@article{titsias2010,
  title = {Bayesian {{Gaussian Process Latent Variable Model}}},
  author = {Titsias, Michalis K and Lawrence, Neil D},
  year = {2010},
  journal = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = {8},
  abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
  langid = {english},
  keywords = {read,thesis},
  file = {/Users/cife/Dropbox/Zotero/Titsias_2010_Bayesian_Gaussian_Process_Latent_Variable_Model.pdf}
}

@inproceedings{titsias2013,
  title = {Variational {{Inference}} for {{Mahalanobis Distance Metrics}} in {{Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Titsias, Michalis and {Lazaro-Gredilla}, Miguel},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.},
  file = {/Users/cife/Dropbox/Zotero/Titsias_RC_AUEB_2013_Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process.pdf}
}

@article{titsias2020,
  ids = {titsias2020a},
  title = {Functional {{Regularisation}} for {{Continual Learning}} with {{Gaussian Processes}}},
  author = {Titsias, Michalis K. and Schwarz, Jonathan and Matthews, Alexander G. de G. and Pascanu, Razvan and Teh, Yee Whye},
  year = {2020},
  month = nov,
  journal = {arXiv:1901.11356v4 [stat]},
  eprint = {1901.11356v4},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs -- a fixed-size subset of the task inputs selected such that it optimally represents the task -- and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Titsias_2020_Functional_Regularisation_for_Continual_Learning_with_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/7NYVJ23Z/1901.html;/Users/cife/Zotero/storage/VNIHN5HK/search.html}
}

@article{tokuda,
  title = {Visualizing {{Distributions}} of {{Covariance Matrices}}},
  author = {Tokuda, Tomoki and Goodrich, Ben and Mechelen, Iven Van and Gelman, Andrew and Tuerlinckx, Francis},
  pages = {30},
  abstract = {We present some methods for graphing distributions of covariance matrices and demonstrate them on several models, including the Wishart, inverse-Wishart, and scaled inverse-Wishart families in different dimensions. Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Our visualization methods are available through the R package VisCov.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/IHIJPQEN/Tokuda et al. - Visualizing Distributions of Covariance Matrices.pdf}
}

@techreport{tokuda2011,
  type = {Technical {{Report}}},
  title = {Visualizing {{Distributions}} of {{Covariance Matrices}}},
  author = {Tokuda, Tomoki and Goodrich, Ben and Mechelen, Iven Van and Gelman, Andrew and Tuerlinckx, Francis},
  year = {2011},
  pages = {30},
  address = {{Columbia Univ., New York, USA}},
  abstract = {We present some methods for graphing distributions of covariance matrices and demonstrate them on several models, including the Wishart, inverse-Wishart, and scaled inverse-Wishart families in different dimensions. Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Our visualization methods are available through the R package VisCov.},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Tokuda_2011_Visualizing_Distributions_of_Covariance_Matrices.pdf}
}

@article{tomczak,
  title = {Fisher Information Matrix for {{Gaussian}} and Categorical Distributions},
  author = {Tomczak, Jakub M},
  pages = {5},
  langid = {english},
  file = {/Users/cife/Zotero/storage/3AFPXL8F/Tomczak - Fisher information matrix for Gaussian and categor.pdf}
}

@inproceedings{tomczak2018,
  title = {{{VAE}} with a {{VampPrior}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Tomczak, Jakub and Welling, Max},
  year = {2018},
  month = mar,
  pages = {1214--1223},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Tomczak_2018_VAE_with_a_VampPrior.pdf;/Users/cife/Zotero/storage/YCC5EEW5/Tomczak and Welling - 2018 - VAE with a VampPrior.pdf}
}

@phdthesis{tosi,
  title = {Visualization and {{Interpretability}} in {{Probabilistic Dimensionality Reduction Models}}},
  author = {Tosi, Alessandra},
  langid = {english},
  school = {Universitat Polit\`ecnica de Catalunya},
  file = {/Users/cife/Zotero/storage/TYQGSR65/Tosi - Visualization and Interpretability in Probabilisti.pdf}
}

@article{tosi2014,
  title = {Metrics for {{Probabilistic Geometries}}},
  author = {Tosi, Alessandra and Hauberg, S{\o}ren and Vellido, Alfredo and Lawrence, Neil D},
  year = {2014},
  journal = {Uncertainty in Artificial Intelligence},
  pages = {9},
  abstract = {We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data.},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/tosi2014.md;/Users/cife/Dropbox/Zotero/Tosi_2014_Metrics_for_Probabilistic_Geometries.pdf}
}

@inproceedings{tosi2014a,
  title = {Local Metric and Graph Based Distance for Probabilistic Dimensionality Reduction},
  booktitle = {Proceedings of the {{Workshop}} on {{Features}} and {{Structures}} ({{FEAST}} 2014) {{International Conference}} on {{Pattern Recogni-tion}} ({{ICPR}} 2014), {{Stockholm}}, {{Sweden}}},
  author = {Tosi, Alessandra and Vellido, Alfredo},
  year = {2014},
  publisher = {{Citeseer}}
}

@article{tr,
  title = {Dimensionality {{Reduction}}: {{A Comparative Review}}},
  author = {Tr, TiCC},
  pages = {36},
  abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/6WRG6DWJ/Tr - Dimensionality Reduction A Comparative Review.pdf}
}

@misc{tran2016,
  title = {The {{Variational Gaussian Process}}},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
  year = {2016},
  month = apr,
  number = {arXiv:1511.06499},
  eprint = {1511.06499},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1511.06499},
  abstract = {Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Tran_2016_The_Variational_Gaussian_Process.pdf;/Users/cife/Zotero/storage/I44VU73V/1511.html}
}

@inproceedings{tran2021,
  title = {Sparse within {{Sparse Gaussian Processes}} Using {{Neighbor Information}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Tran, Gia-Lac and Milios, Dimitrios and Michiardi, Pietro and Filippone, Maurizio},
  year = {2021},
  month = jul,
  pages = {10369--10378},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Approximations to Gaussian processes (GPs) based on inducing variables, combined with variational inference techniques, enable state-of-the-art sparse approaches to infer GPs at scale through mini-batch based learning. In this work, we further push the limits of scalability of sparse GPs by allowing large number of inducing variables without imposing a special structure on the inducing inputs. In particular, we introduce a novel hierarchical prior, which imposes sparsity on the set of inducing variables. We treat our model variationally, and we experimentally show considerable computational gains compared to standard sparse GPs when sparsity on the inducing variables is realized considering the nearest inducing inputs of a random mini-batch of the data. We perform an extensive experimental validation that demonstrates the effectiveness of our approach compared to the state-of-the-art. Our approach enables the possibility to use sparse GPs using a large number of inducing points without incurring a prohibitive computational cost.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Tran_2021_Sparse_within_Sparse_Gaussian_Processes_using_Neighbor_Information.pdf;/Users/cife/Zotero/storage/XTFPUN2Z/Tran et al. - 2021 - Sparse within Sparse Gaussian Processes using Neig.pdf}
}

@misc{tropp2015,
  title = {An {{Introduction}} to {{Matrix Concentration Inequalities}}},
  author = {Tropp, Joel A.},
  year = {2015},
  month = jan,
  number = {arXiv:1501.01571},
  eprint = {1501.01571},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {In recent years, random matrices have come to play a major role in computational mathematics, but most of the classical areas of random matrix theory remain the province of experts. Over the last decade, with the advent of matrix concentration inequalities, research has advanced to the point where we can conquer many (formerly) challenging problems with a page or two of arithmetic. The aim of this monograph is to describe the most successful methods from this area along with some interesting examples that these techniques can illuminate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,Mathematics - Numerical Analysis,Mathematics - Probability,Primary: 60B20. Secondary: 60F10; 60G50; 60G42,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Tropp_2015_An_Introduction_to_Matrix_Concentration_Inequalities.pdf;/Users/cife/Zotero/storage/ETGEXMR2/1501.html}
}

@incollection{tulupyev2005,
  title = {Directed {{Cycles}} in {{Bayesian Belief Networks}}: {{Probabilistic Semantics}} and {{Consistency Checking Complexity}}},
  shorttitle = {Directed {{Cycles}} in {{Bayesian Belief Networks}}},
  booktitle = {{{MICAI}} 2005: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tulupyev, Alexander L. and Nikolenko, Sergey I.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Gelbukh, Alexander and {de Albornoz}, {\'A}lvaro and {Terashima-Mar{\'i}n}, Hugo},
  year = {2005},
  volume = {3789},
  pages = {214--223},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11579427_22},
  abstract = {Although undirected cycles in directed graphs of Bayesian belief networks have been thoroughly studied, little attention has so far been given to a systematic analysis of directed (feedback) cycles. In this paper we propose a way of looking at those cycles; namely, we suggest that a feedback cycle represents a family of probabilistic distributions rather than a single distribution (as a regular Bayesian belief network does). A non-empty family of distributions can be explicitly represented by an ideal of conjunctions with interval estimates on the probabilities of its elements. This ideal can serve as a probabilistic model of an experts uncertain knowledge pattern; such models are studied in the theory of algebraic Bayesian networks. The family of probabilistic distributions may also be empty; in this case, the probabilistic assignment over cycle nodes is inconsistent. We propose a simple way of explicating the probabilistic relationships an isolated directed cycle contains, give an algorithm (based on linear programming) of its consistency checking, and establish a lower bound of the complexity of this checking.},
  isbn = {978-3-540-29896-0 978-3-540-31653-4},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/G6ZS6JY6/Tulupyev and Nikolenko - 2005 - Directed Cycles in Bayesian Belief Networks Proba.pdf}
}

@incollection{turner2011,
  title = {Two Problems with Variational Expectation Maximisation for Time Series Models},
  booktitle = {Bayesian {{Time Series Models}}},
  author = {Turner, Richard Eric and Sahani, Maneesh},
  editor = {Barber, David and Cemgil, A. Taylan and Chiappa, Silvia},
  year = {2011},
  pages = {104--124},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511984679.006},
  isbn = {978-0-511-98467-9},
  langid = {english},
  file = {/Users/cife/Zotero/storage/DWGKNHPZ/Turner and Sahani - 2011 - Two problems with variational expectation maximisa.pdf}
}

@incollection{turner2011a,
  title = {Two Problems with Variational Expectation Maximisation for Time Series Models},
  booktitle = {Bayesian {{Time Series Models}}},
  author = {Turner, Richard Eric and Sahani, Maneesh},
  editor = {Barber, David and Cemgil, A. Taylan and Chiappa, Silvia},
  year = {2011},
  pages = {104--124},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511984679.006},
  isbn = {978-0-511-98467-9},
  langid = {english},
  file = {/Users/cife/Zotero/storage/XRRKL9JT/Turner and Sahani - 2011 - Two problems with variational expectation maximisa.pdf}
}

@article{uhrenholt2021,
  title = {Probabilistic Selection of Inducing Points in Sparse {{Gaussian}} Processes},
  author = {Uhrenholt, Anders Kirk and Charvet, Valentin and Jensen, Bj{\o}rn Sand},
  year = {2021},
  month = jul,
  journal = {arXiv:2010.09370 [cs, stat]},
  eprint = {2010.09370},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Sparse Gaussian processes and various extensions thereof are enabled through inducing points, that simultaneously bottleneck the predictive capacity and act as the main contributor towards model complexity. However, the number of inducing points is generally not associated with uncertainty which prevents us from applying the apparatus of Bayesian reasoning for identifying an appropriate trade-off. In this work we place a point process prior on the inducing points and approximate the associated posterior through stochastic variational inference. By letting the prior encourage a moderate number of inducing points, we enable the model to learn which and how many points to utilise. We experimentally show that fewer inducing points are preferred by the model as the points become less informative, and further demonstrate how the method can be employed in deep Gaussian processes and latent variable modelling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Uhrenholt_2021_Probabilistic_selection_of_inducing_points_in_sparse_Gaussian_processes.pdf;/Users/cife/Zotero/storage/B4FAFKUY/2010.html}
}

@inproceedings{urtasun2006,
  title = {{{3D People Tracking}} with {{Gaussian Process Dynamical Models}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  author = {Urtasun, R. and Fleet, D.J. and Fua, P.},
  year = {2006},
  month = jun,
  volume = {1},
  pages = {238--245},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2006.15},
  abstract = {We advocate the use of Gaussian Process Dynamical Models (GPDMs) for learning human pose and motion priors for 3D people tracking. A GPDM provides a lowdimensional embedding of human motion data, with a density function that gives higher probability to poses and motions close to the training data. With Bayesian model averaging a GPDM can be learned from relatively small amounts of data, and it generalizes gracefully to motions outside the training set. Here we modify the GPDM to permit learning from motions with significant stylistic variation. The resulting priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and significant occlusions.},
  keywords = {Bayesian methods,Computer science,Computer vision,Density functional theory,Gaussian processes,Humans,Laboratories,Legged locomotion,read,Tracking,Training data},
  file = {/Users/cife/Dropbox/Zotero/Urtasun_2006_3D_People_Tracking_with_Gaussian_Process_Dynamical_Models.pdf;/Users/cife/Zotero/storage/VL7HGZM9/1640765.html}
}

@inproceedings{urtasun2007,
  title = {Discriminative {{Gaussian}} Process Latent Variable Model for Classification},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning - {{ICML}} '07},
  author = {Urtasun, Raquel and Darrell, Trevor},
  year = {2007},
  pages = {927--934},
  publisher = {{ACM Press}},
  address = {{Corvalis, Oregon}},
  doi = {10.1145/1273496.1273613},
  abstract = {Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.},
  isbn = {978-1-59593-793-3},
  langid = {english},
  file = {/Users/cife/Zotero/storage/MUSCG77P/Urtasun and Darrell - 2007 - Discriminative Gaussian process latent variable mo.pdf}
}

@inproceedings{urtasun2007a,
  title = {Discriminative {{Gaussian}} Process Latent Variable Model for Classification},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning - {{ICML}} '07},
  author = {Urtasun, Raquel and Darrell, Trevor},
  year = {2007},
  pages = {927--934},
  publisher = {{ACM Press}},
  address = {{Corvalis, Oregon}},
  doi = {10.1145/1273496.1273613},
  abstract = {Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.},
  isbn = {978-1-59593-793-3},
  langid = {english},
  file = {/Users/cife/Zotero/storage/4WLQ4K49/Urtasun and Darrell - 2007 - Discriminative Gaussian process latent variable mo.pdf}
}

@inproceedings{urtasun2008,
  title = {Topologically-Constrained Latent Variable Models},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Urtasun, Raquel and Fleet, David J. and Geiger, Andreas and Popovi{\'c}, Jovan and Darrell, Trevor J. and Lawrence, Neil D.},
  year = {2008},
  month = jul,
  series = {{{ICML}} '08},
  pages = {1080--1087},
  publisher = {{Association for Computing Machinery}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390292},
  abstract = {In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.},
  isbn = {978-1-60558-205-4},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Urtasun_2008_Topologically-constrained_latent_variable_models.pdf}
}

@article{vandermaaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {{van der Maaten}, Lauren and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of machine learning research},
  pages = {2579--2605},
  abstract = {We present a new technique called ``t-SNE'' that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,read,visualization},
  file = {/Users/cife/Documents/thesis/sunshine-highs/notes/vandermaaten2008.md;/Users/cife/Dropbox/Zotero/van_der_Maaten_2008_Visualizing_Data_using_t-SNE.pdf}
}

@article{vandermaaten2009,
  title = {Dimensionality {{Reduction}}: {{A Comparative Review}}},
  author = {{van der Maaten}, Lauren and Postma, Eric and {van den Herik}, Jaap},
  year = {2009},
  pages = {36},
  abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
  langid = {english},
  keywords = {read-maybe-never,tsne},
  file = {/Users/cife/Dropbox/Zotero/van_der_Maaten_2009_Dimensionality_Reduction.pdf}
}

@article{vandermaaten2013,
  title = {Barnes-{{Hut-SNE}}},
  author = {{van der Maaten}, Laurens},
  year = {2013},
  month = mar,
  journal = {arXiv:1301.3342 [cs, stat]},
  eprint = {1301.3342},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N\^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning,tsne},
  file = {/Users/cife/Dropbox/Zotero/van_der_Maaten_2013_Barnes-Hut-SNE.pdf;/Users/cife/Zotero/storage/JEBQWVTN/1301.html}
}

@article{vandermaaten2014,
  title = {Accelerating T-{{SNE}} Using Tree-Based Algorithms},
  author = {Van Der Maaten, Laurens},
  year = {2014},
  journal = {The Journal of Machine Learning Research},
  volume = {15},
  number = {1},
  pages = {3221--3245},
  publisher = {{JMLR. org}},
  keywords = {read-maybe-never,sb,tsne}
}

@inproceedings{vanderwilk2017,
  title = {Convolutional {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {{van der Wilk}, Mark and Rasmussen, Carl Edward and Hensman, James},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {2849--2858},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/van_der_Wilk_2017_Convolutional_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/QT5PI8B9/6877-convolutional-gaussian-processes.html}
}

@incollection{vanderwilk2018,
  title = {Learning {{Invariances}} Using the {{Marginal Likelihood}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {{van der Wilk}, Mark and Bauer, Matthias and John, ST and Hensman, James},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9938--9948},
  publisher = {{Curran Associates, Inc.}},
  keywords = {read-someday},
  file = {/Users/cife/Dropbox/Zotero/van_der_Wilk_2018_Learning_Invariances_using_the_Marginal_Likelihood.pdf;/Users/cife/Zotero/storage/MBU6U5Y7/8199-learning-invariances-using-the-marginal-likelihood.html}
}

@article{venna2010,
  title = {Information {{Retrieval Perspective}} to {{Nonlinear Dimensionality Reduction}} for {{Data Visualization}}},
  author = {Venna, Jarkko and Peltonen, Jaakko and Nybo, Kristian and Aidos, Helena and Kaski, Samuel},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {13},
  pages = {451--490},
  issn = {1533-7928},
  keywords = {read-maybe-never,tsne},
  file = {/Users/cife/Dropbox/Zotero/Venna_2010_Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for.pdf;/Users/cife/Zotero/storage/G423UC9D/venna10a.html}
}

@article{verma2020,
  title = {A Robust Nonlinear Low-Dimensional Manifold for Single Cell {{RNA-seq}} Data},
  author = {Verma, Archit and Engelhardt, Barbara E.},
  year = {2020},
  month = jul,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {324},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03625-z},
  abstract = {Modern developments in single-cell sequencing technologies enable broad insights into cellular state. Single-cell RNA sequencing (scRNA-seq) can be used to explore cell types, states, and developmental trajectories to broaden our understanding of cellular heterogeneity in tissues and organs. Analysis of these sparse, high-dimensional experimental results requires dimension reduction. Several methods have been developed to estimate low-dimensional embeddings for filtered and normalized single-cell data. However, methods have yet to be developed for unfiltered and unnormalized count data that estimate uncertainty in the low-dimensional space. We present a nonlinear latent variable model with robust, heavy-tailed error and adaptive kernel learning to estimate low-dimensional nonlinear structure in scRNA-seq data.},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Verma_2020_A_robust_nonlinear_low-dimensional_manifold_for_single_cell_RNA-seq_data.pdf;/Users/cife/Zotero/storage/7RGWMCXY/s12859-020-03625-z.html}
}

@article{vershik2004,
  ids = {vershikRandomMetricSpaces2004a},
  title = {Random {{Metric Spaces}} and {{Universality}}},
  author = {Vershik, A. M.},
  year = {2004},
  month = apr,
  journal = {Russian Mathematical Surveys},
  volume = {59},
  number = {2},
  eprint = {math/0402263},
  eprinttype = {arxiv},
  pages = {259--295},
  issn = {0036-0279, 1468-4829},
  doi = {10.1070/RM2004v059n02ABEH000718},
  abstract = {We define the notion of a random metric space and prove that with probability one such a space is isometric to the Urysohn universal metric space. The main technique is the study of universal and random distance matrices; we relate the properties of metric (in particular, universal) spaces to the properties of distance matrices. We give examples of other categories in which the randomness and universality coincide (graphs, etc.).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Geometric Topology,Mathematics - Metric Geometry,Mathematics - Probability,Mathematics - Representation Theory,read-someday},
  file = {/Users/cife/Dropbox/Zotero/Vershik_2004_Random_Metric_Spaces_and_Universality.pdf;/Users/cife/Dropbox/Zotero/Vershik_2004_Random_Metric_Spaces_and_Universality2.pdf}
}

@inproceedings{wang2006,
  title = {Gaussian Process Dynamical Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Jack and Hertzmann, Aaron and Fleet, David J.},
  year = {2006},
  pages = {1441--1448},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Wang_2006_Gaussian_process_dynamical_models.pdf}
}

@article{wang2007,
  title = {Gaussian Process Dynamical Models for Human Motion},
  author = {Wang, Jack M. and Fleet, David J. and Hertzmann, Aaron},
  year = {2007},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {30},
  number = {2},
  pages = {283--298},
  keywords = {read},
  file = {/Users/cife/Zotero/storage/QYFA2M4U/4359316.html}
}

@inproceedings{wang2019,
  title = {Exact {{Gaussian Processes}} on a {{Million Data Points}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Ke and Pleiss, Geoff and Gardner, Jacob and Tyree, Stephen and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/cife/Dropbox/Zotero/Wang_2019_Exact_Gaussian_Processes_on_a_Million_Data_Points.pdf}
}

@article{wang2021,
  title = {An {{Intuitive Tutorial}} to {{Gaussian Processes Regression}}},
  author = {Wang, Jie},
  year = {2021},
  month = feb,
  journal = {arXiv:2009.10862 [cs, stat]},
  eprint = {2009.10862},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This tutorial aims to provide an intuitive understanding of the Gaussian processes regression. Gaussian processes regression (GPR) models have been widely used in machine learning applications because of their representation flexibility and inherently uncertainty measures over predictions. The basic concepts that a Gaussian process is built on, including multivariate normal distribution, kernels, non-parametric models, joint and conditional probability were explained first. Next, the GPR was described concisely together with an implementation of a standard GPR algorithm. Beyond the standard GPR, packages to implement state-of-the-art Gaussian processes algorithms were reviewed. This tutorial was written in an accessible way to make sure readers without a machine learning background can obtain a good understanding of the GPR basics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wang_2021_An_Intuitive_Tutorial_to_Gaussian_Processes_Regression.pdf;/Users/cife/Zotero/storage/KQQ5RF3X/2009.html}
}

@article{weiler2019,
  title = {General \${{E}}(2)\$-{{Equivariant Steerable CNNs}}},
  author = {Weiler, Maurice and Cesa, Gabriele},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.08251 [cs, eess]},
  eprint = {1911.08251},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of E(2)-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group E(2) and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. E(2)-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,read},
  file = {/Users/cife/Dropbox/Zotero/Weiler_2019_General_$E(2)$-Equivariant_Steerable_CNNs.pdf}
}

@inproceedings{weinberger2004,
  title = {Learning a Kernel Matrix for Nonlinear Dimensionality Reduction},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
  year = {2004},
  pages = {106},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015345},
  abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that unfolds the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
  langid = {english},
  keywords = {read-someday},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/weinberger2004.md;/Users/cife/Dropbox/Zotero/Weinberger_2004_Learning_a_kernel_matrix_for_nonlinear_dimensionality_reduction.pdf}
}

@misc{wenger2022,
  title = {Posterior and {{Computational Uncertainty}} in {{Gaussian Processes}}},
  author = {Wenger, Jonathan and Pleiss, Geoff and Pf{\"o}rtner, Marvin and Hennig, Philipp and Cunningham, John P.},
  year = {2022},
  month = may,
  number = {arXiv:2205.15449},
  eprint = {2205.15449},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) that the combined variance is a tight worst-case bound for the squared error between the method's posterior mean and the latent function. Finally, we empirically demonstrate the consequences of ignoring computational uncertainty and show how implicitly modeling it improves generalization performance on benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wenger_2022_Posterior_and_Computational_Uncertainty_in_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/R5AIXJIY/2205.html}
}

@misc{wild2021,
  title = {Variational {{Gaussian Processes}}: {{A Functional Analysis View}}},
  shorttitle = {Variational {{Gaussian Processes}}},
  author = {Wild, Veit and Wynne, George},
  year = {2021},
  month = oct,
  number = {arXiv:2110.12798},
  eprint = {2110.12798},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2110.12798},
  abstract = {Variational Gaussian process (GP) approximations have become a standard tool in fast GP inference. This technique requires a user to select variational features to increase efficiency. So far the common choices in the literature are disparate and lacking generality. We propose to view the GP as lying in a Banach space which then facilitates a unified perspective. This is used to understand the relationship between existing features and to draw a connection between kernel ridge regression and variational GP approximations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wild_2021_Variational_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/BPKU2LWT/2110.html}
}

@phdthesis{wilk2018,
  title = {Sparse {{Gaussian Process Approximations}} and {{Applications}}},
  author = {van der Wilk, Mark},
  year = {2018},
  address = {{Jesus College}},
  langid = {english},
  school = {University of Cambridge},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/P9274SR2/Sparse Gaussian Process Approximations and Applica.pdf}
}

@inproceedings{wilk2020,
  title = {Variational {{Gaussian Process Models}} without {{Matrix Inverses}}},
  booktitle = {Proceedings of {{The}} 2nd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {van der Wilk, Mark and John, S. T. and Artemev, Artem and Hensman, James},
  year = {2020},
  month = feb,
  pages = {1--9},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In this work, we provide a variational lower bound that can be computed without expensive matrix operations like inversion. Our bound can be used as a drop-in replacement to the existing variational method of Hensman et al. (2013, 2015), and can therefore directly be applied in a wide variety of models, such as deep GPs (Damianou and Lawrence, 2013). We focus on the theoretical properties of this new bound, and show some initial experimental results for optimising this bound. We hope to realise the full promise in scalability that this new bound has in future work.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Wilk_2020_Variational_Gaussian_Process_Models_without_Matrix_Inverses.pdf}
}

@inproceedings{wilk2021,
  title = {Improved {{Inverse-Free Variational Bounds}} for {{Sparse Gaussian Processes}}},
  booktitle = {Fourth {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {van der Wilk, Mark and Artemev, Artem and Hensman, James},
  year = {2021},
  month = nov,
  abstract = {We provide convenient training objectives for GPs that do not require matrix inverses or other decompositions, but show that currently optimising the objective is hard.},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Wilk_2021_Improved_Inverse-Free_Variational_Bounds_for_Sparse_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/ALQLMF8H/forum.html}
}

@inproceedings{williams2001,
  title = {Using the {{Nystr\"om Method}} to {{Speed Up Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 13},
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  year = {2001},
  pages = {682--688},
  publisher = {{MIT Press}},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Williams_2001_Using_the_Nyström_Method_to_Speed_Up_Kernel_Machines.pdf;/Users/cife/Zotero/storage/4IL3YGUC/1866-using-the-nystrom-method-to-speed-up-kernel-machines.html}
}

@article{wilson2010,
  title = {Generalised {{Wishart Processes}}},
  author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2010},
  pages = {9},
  abstract = {We introduce a new stochastic process called the generalised Wishart process (GWP). It is a collection of positive semi-definite random matrices indexed by any arbitrary input variable. We use this process as a prior over dynamic (e.g. time varying) covariance matrices {$\Sigma$}(t). The GWP captures a diverse class of covariance dynamics, naturally handles missing data, scales nicely with dimension, has easily interpretable parameters, and can use input variables that include covariates other than time. We describe how to construct the GWP, introduce general procedures for inference and prediction, and show that it outperforms its main competitor, multivariate GARCH, even on financial data that especially suits GARCH.},
  langid = {english},
  keywords = {read},
  file = {/Users/cife/Dropbox/Zotero/Wilson_2010_Generalised_Wishart_Processes.pdf}
}

@inproceedings{wilson2013,
  title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  booktitle = {International Conference on Machine Learning},
  author = {Wilson, Andrew and Adams, Ryan},
  year = {2013},
  pages = {1067--1075},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Wilson_2013_Gaussian_process_kernels_for_pattern_discovery_and_extrapolation.pdf}
}

@article{wilson2015,
  title = {Thoughts on {{Massively Scalable Gaussian Processes}}},
  author = {Wilson, Andrew Gordon and Dann, Christoph and Nickisch, Hannes},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.01870 [cs, stat]},
  eprint = {1511.01870},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a framework and early results for massively scalable Gaussian processes (MSGP), significantly extending the KISS-GP approach of Wilson and Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs) on billions of datapoints, without requiring distributed inference, or severe assumptions. In particular, MSGP reduces the standard \$O(n\^3)\$ complexity of GP learning and inference to \$O(n)\$, and the standard \$O(n\^2)\$ complexity per test point prediction to \$O(1)\$. MSGP involves 1) decomposing covariance matrices as Kronecker products of Toeplitz matrices approximated by circulant matrices. This multi-level circulant approximation allows one to unify the orthogonal computational benefits of fast Kronecker and Toeplitz approaches, and is significantly faster than either approach in isolation; 2) local kernel interpolation and inducing points to allow for arbitrarily located data inputs, and \$O(1)\$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block structure (BTTB), which enables fast inference and learning when multidimensional Kronecker structure is not present; and 4) projections of the input space to flexibly model correlated inputs and high dimensional data. The ability to handle many (\$m \textbackslash approx n\$) inducing points allows for near-exact accuracy and large scale kernel learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/cife/Dropbox/Zotero/Wilson_2015_Thoughts_on_Massively_Scalable_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/ZC4R7ILH/1511.html}
}

@article{wilson2020,
  title = {Efficiently {{Sampling Functions}} from {{Gaussian Process Posteriors}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2020},
  month = jul,
  journal = {arXiv:2002.09309 [cs, stat]},
  eprint = {2002.09309},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wilson_2020_Efficiently_Sampling_Functions_from_Gaussian_Process_Posteriors.pdf;/Users/cife/Zotero/storage/G4D6V85L/2002.html}
}

@article{wilson2020a,
  title = {Pathwise {{Conditioning}} of {{Gaussian Processes}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.04026 [cs, math, stat]},
  eprint = {2011.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {As Gaussian processes are integrated into increasingly complex problem settings, analytic solutions to quantities of interest become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as vectors drawn from marginal distributions over process values at a finite number of input location. This distribution-based characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are, therefore, prohibitively expensive in cases where high-dimensional vectors - let alone continuous functions - are required. In this work, we investigate a different line of reasoning. Rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to fast sampling from Gaussian process posteriors. We analyze these methods, along with the approximation errors they introduce, from first principles. We then complement this theory, by exploring the practical ramifications of pathwise conditioning in a various applied settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wilson_2020_Pathwise_Conditioning_of_Gaussian_Processes.pdf;/Users/cife/Zotero/storage/8BS94YGW/2011.html}
}

@article{wingate2013,
  ids = {wingate2013a},
  title = {Automated {{Variational Inference}} in {{Probabilistic Programming}}},
  author = {Wingate, David and Weber, Theophane},
  year = {2013},
  month = jan,
  journal = {arXiv:1301.1299 [cs, stat]},
  eprint = {1301.1299},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe-never,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Wingate_2013_Automated_Variational_Inference_in_Probabilistic_Programming.pdf;/Users/cife/Dropbox/Zotero/Wingate_2013_Automated_Variational_Inference_in_Probabilistic_Programming2.pdf;/Users/cife/Zotero/storage/8CUNCBXE/1301.html;/Users/cife/Zotero/storage/NE7K6FSZ/1301.html}
}

@misc{xiao2017,
  title = {Fashion-{{MNIST}}: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017-08-28, 2017},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@misc{xing2014,
  title = {13 : {{Variational Inference}}: {{Loopy Belief Propagation}}},
  author = {Xing, Eric P and Das, Rajarshi and Liu, Zhengzhong and Gupta, Dishan},
  year = {2014},
  abstract = {10-708: Probabilistic Graphical Models 10-708, Spring 2014 13 : Variational Inference: Loopy Belief Propagation},
  langid = {english},
  file = {/Users/cife/Zotero/storage/BD326YZN/Xing et al. - 13  Variational Inference Loopy Belief Propagati.pdf}
}

@article{yan2007,
  title = {Graph {{Embedding}} and {{Extensions}}: {{A General Framework}} for {{Dimensionality Reduction}}},
  shorttitle = {Graph {{Embedding}} and {{Extensions}}},
  author = {Yan, Shuicheng and Xu, Dong and Zhang, Benyu and Zhang, Hong-jiang and Yang, Qiang and Lin, Stephen},
  year = {2007},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {1},
  pages = {40--51},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2007.250598},
  abstract = {A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions},
  keywords = {Algorithm design and analysis,Algorithms,Artificial Intelligence,Biometry,computer vision,Dimensionality reduction,direct graph embedding,Discriminant Analysis,Face,Face recognition,geometric property,Geometry,graph embedding framework.,graph extension,graph theory,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Information Storage and Retrieval,intraclass compactness,Kernel,kernel extension,Laplace equations,learning (artificial intelligence),Linear discriminant analysis,linear extension,manifold learning,marginal Fisher analysis,Pattern Recognition; Automated,penalty graph,Principal component analysis,read-maybe-never,Reproducibility of Results,scale normalization,Sensitivity and Specificity,statistical analysis,statistical property,Statistics,subspace learning,supervised dimensionality reduction,Tensile stress,tensor extension,tsne,Vectors},
  file = {/Users/cife/Dropbox/Zotero/Yan_2007_Graph_Embedding_and_Extensions.pdf;/Users/cife/Zotero/storage/KEQYQ492/4016549.html}
}

@article{yang2016,
  title = {Bayesian Manifold Regression},
  author = {Yang, Yun and Dunson, David B.},
  year = {2016},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {2},
  pages = {876--905},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/15-AOS1390},
  abstract = {There is increasing interest in the problem of nonparametric regression with high-dimensional predictors. When the number of predictors \$D\$ is large, one encounters a daunting problem in attempting to estimate a \$D\$-dimensional surface based on limited data. Fortunately, in many applications, the support of the data is concentrated on a \$d\$-dimensional subspace with \$d\textbackslash ll D\$. Manifold learning attempts to estimate this subspace. Our focus is on developing computationally tractable and theoretically supported Bayesian nonparametric regression methods in this context. When the subspace corresponds to a locally-Euclidean compact Riemannian manifold, we show that a Gaussian process regression approach can be applied that leads to the minimax optimal adaptive rate in estimating the regression function under some conditions. The proposed model bypasses the need to estimate the manifold, and can be implemented using standard algorithms for posterior computation in Gaussian processes. Finite sample performance is illustrated in a data analysis example.},
  keywords = {62-07,62H30,65U05,68T05,asymptotics,contraction rates,dimensionality reduction,Gaussian process,manifold learning,nonparametric Bayes,read-soon,subspace learning},
  file = {/Users/cife/Dropbox/Zotero/Yang_2016_Bayesian_manifold_regression.pdf;/Users/cife/Zotero/storage/6J5TS7FL/15-AOS1390.html}
}

@article{yang2021,
  title = {Variational Principle for Stochastic Mechanics Based on Information Measures},
  author = {Yang, Jianhao M},
  year = {2021},
  journal = {Journal of Mathematical Physics},
  volume = {62},
  number = {10},
  pages = {102104},
  doi = {10.1063/5.0056779},
  abstract = {Stochastic mechanics is regarded as a physical theory to explain quantum mechanics with classical terms such that some of the quantum mechanics paradoxes can be avoided. Here, we propose a new variational principle to uncover more insights on stochastic mechanics. According to this principle, information measures, such as relative entropy and Fisher information, are imposed as constraints on top of the least action principle. This principle not only recovers Nelson's theory and, consequently, the Schr\"odinger equation but also clears an unresolved issue in stochastic mechanics on why multiple Lagrangians can be used in the variational method and yield the same theory. The concept of forward and backward paths provides an intuitive physical picture for stochastic mechanics. Each path configuration is considered as a degree of freedom and has its own law of dynamics. Thus, the variation principle proposed here can be a new tool to derive more advanced stochastic theory by including additional degrees of freedom in the theory. The structure of Lagrangian developed here shows that some terms in the Lagrangian are originated from information constraints. This suggests that a Lagrangian may need to include both physical and informational terms in order to have a complete description of the dynamics of a physical system.},
  file = {/Users/cife/Zotero/storage/W2B3BC6W/5.html}
}

@article{yang2021a,
  title = {Variational Principle for Stochastic Mechanics Based on Information Measures},
  author = {Yang, Jianhao M.},
  year = {2021},
  month = oct,
  journal = {Journal of Mathematical Physics},
  volume = {62},
  number = {10},
  pages = {102104},
  issn = {0022-2488, 1089-7658},
  doi = {10.1063/5.0056779},
  abstract = {Stochastic mechanics is regarded as a physical theory to explain quantum mechanics with classical terms such that some of the quantum mechanics paradoxes can be avoided. Here, we propose a new variational principle to uncover more insights on stochastic mechanics. According to this principle, information measures, such as relative entropy and Fisher information, are imposed as constraints on top of the least action principle. This principle not only recovers Nelson's theory and, consequently, the Schr\"odinger equation but also clears an unresolved issue in stochastic mechanics on why multiple Lagrangians can be used in the variational method and yield the same theory. The concept of forward and backward paths provides an intuitive physical picture for stochastic mechanics. Each path configuration is considered as a degree of freedom and has its own law of dynamics. Thus, the variation principle proposed here can be a new tool to derive more advanced stochastic theory by including additional degrees of freedom in the theory. The structure of Lagrangian developed here shows that some terms in the Lagrangian are originated from information constraints. This suggests that a Lagrangian may need to include both physical and informational terms in order to have a complete description of the dynamics of a physical system.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/4XCQKNW5/Yang - 2021 - Variational principle for stochastic mechanics bas.pdf}
}

@article{yao2011,
  title = {Learning {{Probabilistic Non-Linear Latent Variable Models}} for {{Tracking Complex Activities}}},
  author = {Yao, Angela and Gall, Juergen and Gool, Luc V. and Urtasun, Raquel},
  year = {2011},
  journal = {Advances in Neural Information Processing Systems},
  volume = {24},
  langid = {english},
  file = {/Users/cife/Dropbox/Zotero/Yao_2011_Learning_Probabilistic_Non-Linear_Latent_Variable_Models_for_Tracking_Complex.pdf}
}

@article{ye2020,
  title = {Heat Kernel and Intrinsic {{Gaussian}} Processes on Manifolds},
  author = {Ye, Ke and Niu, Mu and Cheung, Pokman},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.14266 [math, stat]},
  eprint = {2006.14266},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {There is an increasing interest in the problem of nonparametric regression like Gaussian processes with predictors locating on manifold. Some recent researches developed intrinsic Gaussian processes by using the transition density of the Brownian motion on submanifolds of \$\textbackslash mathbb R\^2\$ and \$\textbackslash mathbb R\^3\$ to approximate the heat kernels. \{However\}, when the dimension of a manifold is bigger than two, the existing method struggled to get good estimation of the heat kernel. In this work, we propose an intrinsic approach of constructing the Gaussian process on \textbackslash if more \textbackslash fi general manifolds \textbackslash if \{\textbackslash color\{red\} in the matrix Lie groups\} \textbackslash fi such as orthogonal groups, unitary groups, Stiefel manifolds and Grassmannian manifolds. The heat kernel is estimated by simulating Brownian motion sample paths via the exponential map, which does not depend on the embedding of the manifold. To be more precise, this intrinsic method has the following features: (i) it is effective for high dimensional manifolds; (ii) it is applicable to arbitrary manifolds; (iii) it does not require the global parametrisation or embedding which may introduce redundant parameters; (iv) results obtained by this method do not depend on the ambient space of the manifold. Based on this method, we propose the ball algorithm for arbitrary manifolds and the strip algorithm for manifolds with extra symmetries, which is both theoretically proven and numerically tested to be much more efficient than the ball algorithm. A regression example on the projective space of dimension eight is given in this work, which demonstrates that our intrinsic method for Gaussian process is practically effective in great generality.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Ye_2020_Heat_kernel_and_intrinsic_Gaussian_processes_on_manifolds.pdf;/Users/cife/Zotero/storage/DAFXDA67/2006.html}
}

@misc{yi2020,
  title = {Sparse and {{Variational Gaussian Process}} \textemdash{} {{What To Do When Data}} Is {{Large}}},
  author = {Yi, Wei},
  year = {2020},
  month = oct,
  journal = {Medium},
  abstract = {Learn how the Sparse and Variational Gaussian Process model uses inducing variables to scale to large datasets.},
  howpublished = {https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7},
  langid = {english},
  keywords = {blog,introductory,reference},
  file = {/Users/cife/Zotero/storage/55MMB5Y7/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7.html}
}

@misc{yi2021,
  title = {Sparse and {{Variational Gaussian Process}} \textemdash{} {{What To Do When Data}} Is {{Large}}},
  author = {Yi, Wei},
  year = {2021},
  month = sep,
  journal = {Medium},
  abstract = {Learn how the Sparse and Variational Gaussian Process model uses inducing variables to scale to large datasets.},
  howpublished = {https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7},
  langid = {english},
  file = {/Users/cife/Zotero/storage/75WWNUQW/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7.html}
}

@misc{yi2021a,
  title = {Sparse and {{Variational Gaussian Process}} \textemdash{} {{What To Do When Data}} Is {{Large}}},
  author = {Yi, Wei},
  year = {2021},
  month = sep,
  journal = {Medium},
  abstract = {Learn how the Sparse and Variational Gaussian Process model uses inducing variables to scale to large datasets.},
  howpublished = {https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7},
  langid = {english},
  file = {/Users/cife/Zotero/storage/8WAIT92N/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7.html}
}

@inproceedings{yu2019,
  title = {Stochastic {{Variational Inference}} for {{Bayesian Sparse Gaussian Process Regression}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Yu, Haibin and Nghia, Trong and Hsiang Low, Bryan Kian and Jaillet, Patrick},
  year = {2019},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2019.8852481},
  abstract = {This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization. Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them. We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data. We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.},
  keywords = {Bayes methods,Computational modeling,Correlation,Optimization,Predictive models,Stochastic processes,Training},
  file = {/Users/cife/Dropbox/Zotero/Yu_2019_Stochastic_Variational_Inference_for_Bayesian_Sparse_Gaussian_Process_Regression.pdf;/Users/cife/Zotero/storage/4LXJWHBD/8852481.html}
}

@article{yue2020,
  title = {The {{Renyi Gaussian Process}}: {{Towards Improved Generalization}}},
  shorttitle = {The {{Renyi Gaussian Process}}},
  author = {Yue, Xubo and Kontar, Raed},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.06990 [cs, stat]},
  eprint = {1910.06990},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce an alternative closed form lower bound on the Gaussian process (\$\textbackslash mathcal\{GP\}\$) likelihood based on the R\textbackslash 'enyi \$\textbackslash alpha\$-divergence. This new lower bound can be viewed as a convex combination of the Nystr\textbackslash "om approximation and the exact \$\textbackslash mathcal\{GP\}\$. The key advantage of this bound, is its capability to control and tune the enforced regularization on the model and thus is a generalization of the traditional variational \$\textbackslash mathcal\{GP\}\$ regression. From a theoretical perspective, we provide the convergence rate and risk bound for inference using our proposed approach. Experiments on real data show that the proposed algorithm may be able to deliver improvement over several \$\textbackslash mathcal\{GP\}\$ inference methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Yue_2020_The_Renyi_Gaussian_Process.pdf;/Users/cife/Zotero/storage/NMLECGHN/1910.html}
}

@inproceedings{zhang2013,
  title = {Probabilistic {{Principal Geodesic Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Miaomiao and Fletcher, Tom},
  year = {2013},
  pages = {9},
  abstract = {Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is defined as a geometric fit to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images.},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Dropbox/Zotero/Zhang_2013_Probabilistic_Principal_Geodesic_Analysis.pdf}
}

@article{zhang2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2017},
  month = oct,
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  langid = {english},
  keywords = {read-maybe-never},
  file = {/Users/cife/Dropbox/Zotero/Zhang_2017_mixup.pdf;/Users/cife/Zotero/storage/E3TCZWPE/1710.html}
}

@article{zhang2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2018},
  month = apr,
  journal = {arXiv:1710.09412 [cs, stat]},
  eprint = {1710.09412},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Zhang_2018_mixup.pdf;/Users/cife/Zotero/storage/I8I8H2FF/1710.html}
}

@article{zhang2018a,
  title = {Advances in {{Variational Inference}}},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  year = {2018},
  month = oct,
  journal = {arXiv:1711.05597 [cs, stat]},
  eprint = {1711.05597},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Zhang_2018_Advances_in_Variational_Inference.pdf;/Users/cife/Zotero/storage/FHUE6UH7/1711.html}
}

@article{zhang2021,
  title = {A {{Note}} on {{Wishart}} and {{Inverse Wishart Priors}} for {{Covariance Matrix}}},
  author = {Zhang, Zhiyong},
  year = {2021},
  journal = {Journal of Behavioral Data Science},
  volume = {1},
  number = {2},
  issn = {25758306, 25741284},
  doi = {10.35566/jbds/v1n2/p2},
  abstract = {For inference involving a covariance matrix, inverse Wishart priors are often used in Bayesian analysis. To help researchers better understand the influence of inverse Wishart priors, we provide a concrete example based on the analysis of a two by two covariance matrix. Recommendations are provided on how to specify an inverse Wishart prior.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/IN3JDQYK/Zhang - 2021 - A Note on Wishart and Inverse Wishart Priors for C.pdf}
}

@article{zhe,
  title = {Regularized {{Variational Sparse Gaussian Processes}}},
  author = {Zhe, Shandian},
  pages = {5},
  abstract = {Variational sparse Gaussian processes (GPs) are important GP approximate inference approaches. The key idea is to use a small set of pseudo inputs to construct a variational model evidence lower bound (ELBO). By maximizing the ELBO, we can optimize the pseudo inputs, as free variational parameters, jointly with the model parameters. The optimization, however, is highly nonlinear, nonconvex, and is easily trapped in inferior local maximums. We argue that the learning of these parameters, could be benefited from exploiting the training input information \textemdash we regularize the pseudo input estimation toward a statistical summarization of the training inputs in kernel space. To this end, we augment GPs by placing a kernelized mixture prior over the training inputs, where the mixtures components correspond to the pseudo inputs. We then derive a tight variational lower bound, which introduces an additional regularization term of the pseudo inputs and kernel parameters. We show the effectiveness of our regularized variational sparse approximation in two real regression datasets.},
  langid = {english},
  file = {/Users/cife/Zotero/storage/MDECNHHD/Zhe - Regularized Variational Sparse Gaussian Processes.pdf}
}

@article{zhou2019,
  title = {On the {{Continuity}} of {{Rotation Representations}} in {{Neural Networks}}},
  author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  year = {2019},
  month = apr,
  journal = {arXiv:1812.07035 [cs, stat]},
  eprint = {1812.07035},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read-soon,sb,Statistics - Machine Learning},
  file = {/Users/cife/Dropbox/Zotero/Zhou_2019_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks.pdf}
}

@article{zhou2020,
  title = {Learning {{Manifold Implicitly}} via {{Explicit Heat-Kernel Learning}}},
  author = {Zhou, Yufan and Chen, Changyou and Xu, Jinhui},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Zotero/storage/BIB9T76W/Zhou et al. - 2020 - Learning Manifold Implicitly via Explicit Heat-Ker.pdf;/Users/cife/Zotero/storage/4JGP345L/05e2a0647e260c355dd2b2175edb45b8-Abstract.html;/Users/cife/Zotero/storage/TCFEV84U/05e2a0647e260c355dd2b2175edb45b8-Abstract.html}
}

@inproceedings{zhou2020a,
  title = {Learning {{Manifold Implicitly}} via {{Explicit Heat-Kernel Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Yufan and Chen, Changyou and Xu, Jinhui},
  year = {2020},
  volume = {33},
  pages = {477--487},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Manifold learning is a fundamental problem in machine learning with numerous applications. Most of the existing methods directly learn the low-dimensional embedding of the data in some high-dimensional space, and usually lack the flexibility of being directly applicable to down-stream applications. In this paper, we propose the concept of implicit manifold learning, where manifold information is implicitly obtained by learning the associated heat kernel. A heat kernel is the solution of the corresponding heat equation, which describes how ``heat'' transfers on the manifold, thus containing ample geometric information of the manifold. We provide both practical algorithm and theoretical analysis of our framework. The learned heat kernel can be applied to various kernel-based machine learning models, including deep generative models (DGM) for data generation and Stein Variational Gradient Descent for Bayesian inference. Extensive experiments show that our framework can achieve the state-of-the-art results compared to existing methods for the two tasks.},
  file = {/Users/cife/Dropbox/Zotero/Zhou_2020_Learning_Manifold_Implicitly_via_Explicit_Heat-Kernel_Learning.pdf}
}

@article{zliobaite2017,
  title = {Measuring Discrimination in Algorithmic Decision Making},
  author = {{\v Z}liobait{\.e}, Indr{\.e}},
  year = {2017},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {31},
  number = {4},
  pages = {1060--1089},
  issn = {1573-756X},
  doi = {10.1007/s10618-017-0506-1},
  abstract = {Society is increasingly relying on data-driven predictive models for automated decision making. This is not by design, but due to the nature and noisiness of observational data, such models may systematically disadvantage people belonging to certain categories or groups, instead of relying solely on individual merits. This may happen even if the computing process is fair and well-intentioned. Discrimination-aware data mining studies of how to make predictive models free from discrimination, when the historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. Discrimination-aware data mining is an emerging research discipline, and there is no firm consensus yet of how to measure the performance of algorithms. The goal of this survey is to review various discrimination measures that have been used, analytically and computationally analyze their performance, and highlight implications of using one or another measure. We also describe measures from other disciplines, which have not been used for measuring discrimination, but potentially could be suitable for this purpose. This survey is primarily intended for researchers in data mining and machine learning as a step towards producing a unifying view of performance criteria when developing new algorithms for non-discriminatory predictive modeling. In addition, practitioners and policy makers could use this study when diagnosing potential discrimination by predictive models.},
  langid = {english},
  keywords = {fairness,read-soon},
  file = {/Users/cife/Dropbox/Zotero/Žliobaitė_2017_Measuring_discrimination_in_algorithmic_decision_making.pdf}
}

@inproceedings{zomorodian2004,
  title = {Computing Persistent Homology},
  booktitle = {Proceedings of the Twentieth Annual Symposium on {{Computational}} Geometry},
  author = {Zomorodian, Afra and Carlsson, Gunnar},
  year = {2004},
  pages = {347--356}
}

@misc{zotero-1034,
  title = {Metacademy - {{Differential}} Geometry for Machine Learning},
  abstract = {Metacademy - a free open source platform for efficient, personalized learning.},
  howpublished = {https://metacademy.org/roadmaps/rgrosse/dgml},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/5PK7P3KC/dgml.html}
}

@misc{zotero-1145,
  title = {S\o ren {{Hauberg}} @ {{CogSys}}, {{DTU Compute}}},
  howpublished = {http://www2.compute.dtu.dk/\textasciitilde sohau/operational\_representations/},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/DAGXMA5H/operational_representations.html}
}

@misc{zotero-1154,
  title = {(1) {{ML Tutorial}}: {{Gaussian Processes}} ({{Richard Turner}}) - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=92-98SYOdlY\&t=3592s\&ab\_channel=MarcDeisenroth},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/3P4T6KLT/watch.html}
}

@misc{zotero-1186,
  title = {Inter-Domain {{Deep Gaussian Processes}}},
  howpublished = {https://sites.google.com/view/inter-domain-dgps},
  langid = {english},
  keywords = {read-someday}
}

@misc{zotero-1222,
  title = {Fig. 2. {{Recovering}} the Structure of a Rotated '1' from {{MNIST}}. {{The}}...},
  journal = {ResearchGate},
  abstract = {Download scientific diagram | Recovering the structure of a rotated '1' from MNIST. The learned kernel matrices (upper row) and 2D manifolds (lower row) obtained from B-GPLVM (left), VAE-DGP (middle) and the proposed VGP-AE (right), initialized from the same random instance. from publication: Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units | We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process (GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP... | Gaussian Processes | ResearchGate, the professional network for scientists.},
  howpublished = {https://www.researchgate.net/figure/Recovering-the-structure-of-a-rotated-1-from-MNIST-The-learned-kernel-matrices-upper\_fig2\_306226790},
  langid = {english},
  keywords = {read-maybe-never,sb}
}

@misc{zotero-1236,
  title = {What My Deep Model Doesn't Know... | {{Yarin Gal}} - {{Blog}} | {{Cambridge Machine Learning Group}}},
  abstract = {Trying to understand why dropout networks work so well, I was quite surprised to see that we can get principled uncertainty information from these models for free \textendash{} without changing a thing.},
  howpublished = {http://mlg.eng.cam.ac.uk/yarin/blog\_3d801aa532c1ce.html},
  keywords = {reference},
  file = {/Users/cife/Zotero/storage/34EWKIXH/blog_3d801aa532c1ce.html}
}

@misc{zotero-1238,
  title = {Natural {{Gradient Descent}} - {{Agustinus Kristiadi}}'s {{Blog}}},
  abstract = {Intuition and derivation of natural gradient descent.},
  howpublished = {http://wiseodd.github.io/techblog/2018/03/14/natural-gradient/},
  langid = {english},
  keywords = {read-soon},
  file = {/Users/cife/Zotero/storage/XHNG5DXI/natural-gradient.html}
}

@misc{zotero-1244,
  title = {(2) {{Riemann}} Geometry -- Covariant Derivative - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=BHKd6-IJgVI\&t=132s\&ab\_channel=dXoverdteqprogress},
  keywords = {read-very-soon},
  file = {/Users/cife/Documents/thesis/sunshine-highs/from_zotero/zotero-1244.md;/Users/cife/Zotero/storage/PWQTKXJY/watch.html}
}

@misc{zotero-1310,
  title = {{{NeurIPS}} 2020 : {{You Can}}'t {{Escape Hyperparameters}} and {{Latent Variables}}: {{Machine Learning}} as a {{Software Engineering Enterprise}}},
  howpublished = {https://nips.cc/virtual/2020/public/invited\_16166.html},
  keywords = {read},
  file = {/Users/cife/Zotero/storage/6JWEZGVZ/invited_16166.html}
}

@misc{zotero-1460,
  title = {From {{Probabilistic PCA}} to the {{GPLVM}}},
  howpublished = {https://gregorygundersen.com/blog/2020/07/14/pca-to-gplvm/},
  file = {/Users/cife/Zotero/storage/LQHXLB9A/pca-to-gplvm.html}
}

@misc{zotero-1538,
  title = {{Disentangling by Subspace Diffusion}},
  journal = {Deepmind},
  abstract = {We present a novel nonparametric algorithm for symmetry-based disentangling of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER)...},
  howpublished = {https://deepmind.com/research/publications/2020/Disentangling-by-Subspace-Diffusion},
  langid = {ALL},
  file = {/Users/cife/Zotero/storage/G2AZZ67I/Disentangling-by-Subspace-Diffusion.html}
}

@misc{zotero-1599,
  title = {Notion \textendash{} {{The}} All-in-One Workspace for Your Notes, Tasks, Wikis, and Databases.},
  journal = {Notion},
  abstract = {A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team},
  howpublished = {https://www.notion.so},
  file = {/Users/cife/Zotero/storage/LRMREBU7/Shopping-list-need-6053add4f5cb40cbb3725a89a6dffb8f.html}
}

@misc{zotero-1707,
  title = {Class 0.0, Class 1.0, Class 2.0, {{None}}, Class 0.0, Class 1.0, Class 2.0, {{None}}, Class 0.0, Class 1.0, Class 2.0, {{None}} | Scatter Chart Made by {{Maxz}} | Plotly},
  abstract = {Maxz's interactive graph and data of "class 0.0, class 1.0, class 2.0, None, class 0.0, class 1.0, class 2.0, None, class 0.0, class 1.0, class 2.0, None" is a scatter chart, showing class 0.0, class 1.0, class 2.0, None, class 0.0, class 1.0, class 2.0, None, class 0.0, class 1.0, class 2.0, None.},
  howpublished = {https://plotly.com/\textasciitilde maxz/665/class-00-class-10-class-20-none-class-00-class-10-class-20-none-class-00-class-1/}
}

@misc{zotero-1731,
  title = {Bayesian {{Learning}} of {{GP-LVM}}},
  journal = {Gaussian Process Summer School},
  howpublished = {https://mlatcl.github.io/gpss/lectures/10-bayesian-learning-gplvm.html},
  langid = {english},
  file = {/Users/cife/Zotero/storage/YGWCW6HG/10-bayesian-learning-gplvm.html}
}

@article{zotero-1734,
  title = {Sparse {{Gaussian Process Approximations}} and {{Applications}}},
  pages = {188},
  langid = {english},
  file = {/Users/cife/Zotero/storage/K72HB2CT/Sparse Gaussian Process Approximations and Applica.pdf}
}

@misc{zotero-1779,
  title = {Miguelgondu's Blog},
  howpublished = {https://www.miguelgondu.com/blogposts/2020-12-15/a\_translation\_table\_probability/\#fnref-1},
  file = {/Users/cife/Zotero/storage/63XTXVUA/a_translation_table_probability.html}
}

@misc{zotero-2121,
  type = {Misc}
}

@misc{zotero-2136,
  title = {Homepage},
  journal = {dtu\_mlops},
  abstract = {Exercises and supplementary material for the machine learning operations course at DTU.},
  howpublished = {https://skaftenicki.github.io/dtu\_mlops/},
  langid = {american},
  file = {/Users/cife/Zotero/storage/WRVJ37ID/dtu_mlops.html}
}

@misc{zotero-2138,
  title = {Learning Meaningful Representations of Protein Sequences | {{Nature Communications}}},
  howpublished = {https://www.nature.com/articles/s41467-022-29443-w},
  file = {/Users/cife/Zotero/storage/ITXABK6H/s41467-022-29443-w.html}
}

@misc{zotero-2198,
  title = {Martins Polterabend},
  journal = {Google Docs},
  abstract = {Tale Tips til tale: http://xn--rst-0na.dk/tips-til-din-tale/ Skoleg: https://bryllupsuniverset.dk/festen/underholdning/lege-til-brudeparret  Outline Cilie introducerer meget kort Introduc\'er Rasmus Rasmus joiner Cilie Cilie starter Killer opening: Martin var i tvivl om roller til brylluppet Derfor...},
  howpublished = {https://docs.google.com/document/d/1nMKcRm\_8d7XCk\_nzVDayqeODAPDdBBb-MOP-FA74qQQ/edit?usp=embed\_facebook},
  langid = {english},
  file = {/Users/cife/Zotero/storage/49DQ9247/edit.html}
}

@misc{zotero-2214,
  title = {Thesis - Cilie.Feldager@gmail.Com - {{Gmail}}},
  howpublished = {https://mail.google.com/mail/u/0/\#inbox/KtbxLxgKJhwZchpQCwwWmzJFPZsCBqRTcg},
  file = {/Users/cife/Zotero/storage/QPMX7CJK/0.html}
}

@inproceedings{zotero-2228,
  type = {Inproceedings}
}

@misc{zotero-2256,
  title = {Robust {{Statistics}}, 2nd {{Edition}} | {{Wiley}}},
  journal = {Wiley.com},
  abstract = {A new edition of the classic, groundbreaking book on robust statistics Over twenty-five years after the publication of its predecessor, Robust Statistics, Second Edition continues to provide an authoritative and systematic treatment of the topic. This new edition has been thoroughly updated and expanded to reflect the latest advances in the field while also outlining the established theory and applications for building a solid foundation in robust statistics for both the theoretical and the applied statistician. A comprehensive introduction and discussion on the formal mathematical background behind qualitative and quantitative robustness is provided, and subsequent chapters delve into basic types of scale estimates, asymptotic minimax theory, regression, robust covariance, and robust design. In addition to an extended treatment of robust regression, the Second Edition features four new chapters covering: Robust Tests Small Sample Asymptotics Breakdown Point Bayesian Robustness An expanded treatment of robust regression and pseudo-values is also featured, and concepts, rather than mathematical completeness, are stressed in every discussion. Selected numerical algorithms for computing robust estimates and convergence proofs are provided throughout the book, along with quantitative robustness information for a variety of estimates. A General Remarks section appears at the beginning of each chapter and provides readers with ample motivation for working with the presented methods and techniques. Robust Statistics, Second Edition is an ideal book for graduate-level courses on the topic. It also serves as a valuable reference for researchers and practitioners who wish to study the statistical research associated with robust statistics.},
  howpublished = {https://www.wiley.com/en-us/Robust+Statistics\%2C+2nd+Edition-p-9780470129906},
  langid = {american},
  file = {/Users/cife/Zotero/storage/7TNED3QI/Robust+Statistics,+2nd+Edition-p-9780470129906.html}
}

@misc{zotero-575,
  title = {The-Test-Card.Pdf},
  keywords = {exp},
  file = {/Users/cife/Zotero/storage/23RRKD3X/the-test-card.pdf}
}

@misc{zotero-602,
  title = {The-Test-Card.Pdf},
  keywords = {exp},
  file = {/Users/cife/Zotero/storage/SCMIC2ZR/the-test-card.pdf}
}

@misc{zotero-604,
  title = {The-Test-Card.Pdf},
  keywords = {exp},
  file = {/Users/cife/Zotero/storage/FL548JWC/the-test-card.pdf}
}

@misc{zotero-674,
  title = {Unfairness in Images},
  abstract = {Show unfairness in an image dataset where the symmetry break correlates with the broken part of a model.},
  keywords = {note},
  file = {/Users/cife/Zotero/storage/IISBDKSH/test-card.pdf}
}

@misc{zotero-846,
  title = {Atomic {{Habits}}: {{The}} Life-Changing Million Copy Bestseller {{eBook}}: {{Clear}}, {{James}}: {{Amazon}}.Co.Uk: {{Kindle Store}}},
  howpublished = {https://www.amazon.co.uk/Atomic-Habits-Proven-Build-Break-ebook/dp/B01N5AX61W/ref=pd\_typ\_k\_rtpb\_1\_341689031\_2/262-0275599-1301331?\_encoding=UTF8\&psc=1\&refRID=1GWJP9SN7ZD8RSBEWYFJ},
  keywords = {book},
  file = {/Users/cife/Zotero/storage/7KTEDPQI/262-0275599-1301331.html}
}

@misc{zotero-893,
  title = {Principles of {{Riemannian Geometry}} in {{Neural Networks}} | {{TDLS}}},
  abstract = {Toronto Deep Learning Series, 13 August 2018 For slides and more information, visit https://aisc.ai.science/events/2018-0... Paper Review: https://papers.nips.cc/paper/6873-pri... Speaker: https://www.linkedin.com/in/helen-ngo/ Organizer: https://www.linkedin.com/in/amirfz/ Host: Dessa Paper abstract: This paper interprets neural networks from a minority perspective with the goal to formalize a theoretical basis grounded in Riemannian geometry, framing neural networks as learning an optimal coordinate representation system of the underlying data manifold where different target classes are linearly separable by hyperplanes. We will show that residual neural networks are finite difference approximations to dynamical systems of first order differential equations. Parallels are drawn between backpropagation and the pullback metric, a linear map between the spaces of 1-forms existing on two smooth manifolds, which acts on the coordinate representation of the metric tensor between network layers.},
  keywords = {read-maybe-never}
}

@misc{zotero-894,
  title = {New Methods for Identifying Latent Manifold Structure from Neural Data | {{ASIC}}},
  abstract = {For slides and more information on the paper, visit https://ai.science/e/learning-a-laten... Speaker: Anqi Wu; Discussion Moderator: Rober Boshra Motivation Numerous studies in neuroscience posit that large-scale neural activity reflects noisy high-dimensional observations of some underlying, low-dimensional signals of interest. Discovering such low-dimensional signals or structures can help shed light on how information is encoded at the population level, and provide significant scientific insight into the brain. In this talk, Dr. Anqi Wu will present her work on developing Bayesian methods to identify such latent manifold structures . Full abstract: Numerous studies in neuroscience posit that large-scale neural activity reflects noisy high-dimensional observations of some underlying, low-dimensional signals of interest. Discovering such low-dimensional signals or structures can help shed light on how information is encoded at the population level, and provide significant scientific insight into the brain. In this talk, I will present my work on developing Bayesian methods to identify such latent manifold structures, which are referred to as latent manifold tuning models. Firstly, I will describe the latent manifold tuning model to discover low-dimensional latent dynamics in multi-neuron spike train data with an application to hippocampal place cells. Secondly, I will present a similar latent model to learn interpretable latent embeddings in calcium imaging data with an application to olfactory neurons. We show that the models are able to reveal the underlying signals of neural populations as well as uncovering interesting topography of neurons where there is a lack of knowledge and understanding about the brain.},
  keywords = {read-maybe-never}
}

@misc{zotero-895,
  title = {A {{Literature Review}} on {{Interpretability}} for {{Machine Learning}} | {{AISC}}},
  abstract = {For slides and more information on the paper, visit https://aisc.ai.science/events/2020-0... Discussion lead: Ali El Sharif Discussion facilitator: Suhas Pai},
  keywords = {read-maybe-never}
}

@misc{zotero-896,
  title = {The Research Paper Should {{NOT}} Be the Final Product | {{AISC}}},
  abstract = {For slides and more information on the paper, visit https://aisc.ai.science/events/2020-0... Discussion lead: Daniel Lemire Discussion facilitator(s): Amir Feizpour},
  keywords = {grey literature,read-maybe-never}
}

@misc{zotero-925,
  title = {Structure-Preserving Visualisation of High Dimensional Single-Cell Datasets | {{Scientific Reports}}},
  howpublished = {https://www.nature.com/articles/s41598-019-45301-0},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/C7R2FRTI/s41598-019-45301-0.html}
}

@misc{zotero-970,
  title = {Gunnar {{Carlsson}}: "{{Topological Modeling}} of {{Complex Data}}"},
  shorttitle = {Gunnar {{Carlsson}}},
  abstract = {JMM 2018: "Topological Modeling of Complex Data" by Gunnar Carlsson, Stanford University, an AMS-MAA Invited Address at the 2018 Joint Mathematics Meetings.},
  keywords = {read,sb}
}

@misc{zotero-983,
  title = {[2003.08505] {{A Metric Learning Reality Check}}},
  howpublished = {https://arxiv.org/abs/2003.08505},
  keywords = {read-maybe-never},
  file = {/Users/cife/Zotero/storage/SDTEYKVX/2003.html}
}

@misc{zotero-undefined,
  title = {Linearization ({{Taylor Expansions}}) - {{Uncertain Gaussian Processes}}},
  howpublished = {https://jejjohnson.github.io/uncertain\_gps/Notes/taylor/}
}

@misc{zotero-undefineda,
  title = {Datasets \guillemotleft{} {{Gaussian Process}}: {{Theory}} and {{Applications}}},
  howpublished = {http://gaussianprocess.com/datasets.php},
  file = {/Users/cife/Zotero/storage/6QA8QQVX/datasets.html}
}

@misc{zotero-undefinedb,
  title = {Inducing Point Methods to Speed up {{GPs}} - Posts},
  howpublished = {https://bwengals.github.io/inducing-point-methods-to-speed-up-gps.html},
  file = {/Users/cife/Zotero/storage/HHJEIU6G/inducing-point-methods-to-speed-up-gps.html}
}

@misc{zotero-undefinedc,
  title = {Papers with {{Code}} - {{Deep Neural Networks}} as {{Point Estimates}} for {{Deep Gaussian Processes}}},
  abstract = {No code available yet.},
  howpublished = {https://paperswithcode.com/paper/deep-neural-networks-as-point-estimates-for},
  langid = {english},
  file = {/Users/cife/Zotero/storage/6BJRRVV9/deep-neural-networks-as-point-estimates-for.html}
}

@misc{zotero-undefinedd,
  title = {Neural {{Mechanics}}: {{Symmetry}} and {{Broken Conservation Laws In Deep Learning Dynamics}} | {{SAIL Blog}}},
  howpublished = {http://ai.stanford.edu/blog/neural-mechanics/},
  file = {/Users/cife/Zotero/storage/DRBREM4G/neural-mechanics.html}
}

@misc{zotero-undefinede,
  title = {From {{Probabilistic PCA}} to the {{GPLVM}}},
  howpublished = {https://gregorygundersen.com/blog/2020/07/14/pca-to-gplvm/},
  file = {/Users/cife/Zotero/storage/262T5SCB/pca-to-gplvm.html}
}

@misc{zotero-undefinedf,
  title = {How {{Wittgenstein}} Might `Solve' Both Philosophy and Quantum Physics | {{Aeon Essays}}},
  journal = {Aeon},
  abstract = {Metaphysical debates in quantum physics don't get at `truth' \textendash{} they're nothing but a form of ritual, activity and culture},
  howpublished = {https://aeon.co/essays/how-wittgenstein-might-solve-both-philosophy-and-quantum-physics},
  langid = {english},
  file = {/Users/cife/Zotero/storage/8YQMXXE8/how-wittgenstein-might-solve-both-philosophy-and-quantum-physics.html}
}

@misc{zotero-undefinedg,
  title = {A {{Handbook}} for {{Sparse Variational Gaussian Processes}} | {{Louis Tiao}}},
  howpublished = {https://tiao.io/post/sparse-variational-gaussian-processes/},
  file = {/Users/cife/Zotero/storage/WFNJBBXR/sparse-variational-gaussian-processes.html}
}
